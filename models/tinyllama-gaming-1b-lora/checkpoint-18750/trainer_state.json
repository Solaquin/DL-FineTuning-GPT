{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 18750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016,
      "grad_norm": 2.018833875656128,
      "learning_rate": 1.8e-05,
      "loss": 2.5814,
      "step": 10
    },
    {
      "epoch": 0.0032,
      "grad_norm": 2.568183660507202,
      "learning_rate": 3.8e-05,
      "loss": 2.5162,
      "step": 20
    },
    {
      "epoch": 0.0048,
      "grad_norm": 2.077240467071533,
      "learning_rate": 5.8e-05,
      "loss": 2.3739,
      "step": 30
    },
    {
      "epoch": 0.0064,
      "grad_norm": 3.0512447357177734,
      "learning_rate": 7.800000000000001e-05,
      "loss": 2.1096,
      "step": 40
    },
    {
      "epoch": 0.008,
      "grad_norm": 2.2336411476135254,
      "learning_rate": 9.8e-05,
      "loss": 1.8232,
      "step": 50
    },
    {
      "epoch": 0.0096,
      "grad_norm": 1.9421135187149048,
      "learning_rate": 0.000118,
      "loss": 1.5069,
      "step": 60
    },
    {
      "epoch": 0.0112,
      "grad_norm": 1.8421499729156494,
      "learning_rate": 0.000138,
      "loss": 1.3401,
      "step": 70
    },
    {
      "epoch": 0.0128,
      "grad_norm": 1.8128323554992676,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.2725,
      "step": 80
    },
    {
      "epoch": 0.0144,
      "grad_norm": 2.3088696002960205,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.0681,
      "step": 90
    },
    {
      "epoch": 0.016,
      "grad_norm": 3.7260823249816895,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.1874,
      "step": 100
    },
    {
      "epoch": 0.0176,
      "grad_norm": 3.4207212924957275,
      "learning_rate": 0.00019990348525469168,
      "loss": 1.1297,
      "step": 110
    },
    {
      "epoch": 0.0192,
      "grad_norm": 2.4427804946899414,
      "learning_rate": 0.00019979624664879356,
      "loss": 1.0253,
      "step": 120
    },
    {
      "epoch": 0.0208,
      "grad_norm": 2.040367841720581,
      "learning_rate": 0.00019968900804289544,
      "loss": 1.1686,
      "step": 130
    },
    {
      "epoch": 0.0224,
      "grad_norm": 1.7811044454574585,
      "learning_rate": 0.00019958176943699732,
      "loss": 1.0314,
      "step": 140
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.6406593322753906,
      "learning_rate": 0.0001994745308310992,
      "loss": 0.9532,
      "step": 150
    },
    {
      "epoch": 0.0256,
      "grad_norm": 1.8835922479629517,
      "learning_rate": 0.00019936729222520109,
      "loss": 0.9419,
      "step": 160
    },
    {
      "epoch": 0.0272,
      "grad_norm": 2.2022478580474854,
      "learning_rate": 0.00019926005361930297,
      "loss": 0.8905,
      "step": 170
    },
    {
      "epoch": 0.0288,
      "grad_norm": 2.1468453407287598,
      "learning_rate": 0.00019915281501340485,
      "loss": 0.924,
      "step": 180
    },
    {
      "epoch": 0.0304,
      "grad_norm": 1.5717264413833618,
      "learning_rate": 0.00019904557640750673,
      "loss": 1.0151,
      "step": 190
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.41874361038208,
      "learning_rate": 0.0001989383378016086,
      "loss": 1.0045,
      "step": 200
    },
    {
      "epoch": 0.0336,
      "grad_norm": 1.7356369495391846,
      "learning_rate": 0.00019883109919571046,
      "loss": 1.0423,
      "step": 210
    },
    {
      "epoch": 0.0352,
      "grad_norm": 1.6461070775985718,
      "learning_rate": 0.00019872386058981235,
      "loss": 0.887,
      "step": 220
    },
    {
      "epoch": 0.0368,
      "grad_norm": 1.8689769506454468,
      "learning_rate": 0.00019861662198391423,
      "loss": 0.8818,
      "step": 230
    },
    {
      "epoch": 0.0384,
      "grad_norm": 1.4783427715301514,
      "learning_rate": 0.0001985093833780161,
      "loss": 0.9453,
      "step": 240
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.110015630722046,
      "learning_rate": 0.000198402144772118,
      "loss": 0.9511,
      "step": 250
    },
    {
      "epoch": 0.0416,
      "grad_norm": 1.2578792572021484,
      "learning_rate": 0.00019829490616621984,
      "loss": 0.9832,
      "step": 260
    },
    {
      "epoch": 0.0432,
      "grad_norm": 1.1580597162246704,
      "learning_rate": 0.00019818766756032172,
      "loss": 0.9199,
      "step": 270
    },
    {
      "epoch": 0.0448,
      "grad_norm": 1.0362507104873657,
      "learning_rate": 0.0001980804289544236,
      "loss": 0.9083,
      "step": 280
    },
    {
      "epoch": 0.0464,
      "grad_norm": 1.2262464761734009,
      "learning_rate": 0.00019797319034852549,
      "loss": 0.9298,
      "step": 290
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.362789273262024,
      "learning_rate": 0.00019786595174262737,
      "loss": 0.9266,
      "step": 300
    },
    {
      "epoch": 0.0496,
      "grad_norm": 1.1039619445800781,
      "learning_rate": 0.00019775871313672922,
      "loss": 0.9469,
      "step": 310
    },
    {
      "epoch": 0.0512,
      "grad_norm": 1.1178696155548096,
      "learning_rate": 0.0001976514745308311,
      "loss": 0.897,
      "step": 320
    },
    {
      "epoch": 0.0528,
      "grad_norm": 1.2501941919326782,
      "learning_rate": 0.00019754423592493298,
      "loss": 0.9199,
      "step": 330
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.9738243222236633,
      "learning_rate": 0.00019743699731903486,
      "loss": 0.9977,
      "step": 340
    },
    {
      "epoch": 0.056,
      "grad_norm": 1.1337049007415771,
      "learning_rate": 0.00019732975871313674,
      "loss": 0.9262,
      "step": 350
    },
    {
      "epoch": 0.0576,
      "grad_norm": 1.311880350112915,
      "learning_rate": 0.0001972225201072386,
      "loss": 0.9502,
      "step": 360
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.943278431892395,
      "learning_rate": 0.00019711528150134048,
      "loss": 0.9319,
      "step": 370
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.9749656319618225,
      "learning_rate": 0.00019700804289544236,
      "loss": 0.8729,
      "step": 380
    },
    {
      "epoch": 0.0624,
      "grad_norm": 1.0733965635299683,
      "learning_rate": 0.00019690080428954424,
      "loss": 0.9479,
      "step": 390
    },
    {
      "epoch": 0.064,
      "grad_norm": 1.3722529411315918,
      "learning_rate": 0.00019679356568364612,
      "loss": 0.9741,
      "step": 400
    },
    {
      "epoch": 0.0656,
      "grad_norm": 1.0455756187438965,
      "learning_rate": 0.00019668632707774798,
      "loss": 0.9435,
      "step": 410
    },
    {
      "epoch": 0.0672,
      "grad_norm": 1.1550333499908447,
      "learning_rate": 0.00019657908847184986,
      "loss": 0.9419,
      "step": 420
    },
    {
      "epoch": 0.0688,
      "grad_norm": 1.19549560546875,
      "learning_rate": 0.00019647184986595174,
      "loss": 0.8324,
      "step": 430
    },
    {
      "epoch": 0.0704,
      "grad_norm": 1.0167752504348755,
      "learning_rate": 0.00019636461126005362,
      "loss": 0.9119,
      "step": 440
    },
    {
      "epoch": 0.072,
      "grad_norm": 1.1047991514205933,
      "learning_rate": 0.0001962573726541555,
      "loss": 0.9484,
      "step": 450
    },
    {
      "epoch": 0.0736,
      "grad_norm": 2.405751943588257,
      "learning_rate": 0.00019615013404825738,
      "loss": 0.9657,
      "step": 460
    },
    {
      "epoch": 0.0752,
      "grad_norm": 1.176778793334961,
      "learning_rate": 0.00019604289544235926,
      "loss": 0.8607,
      "step": 470
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.9533570408821106,
      "learning_rate": 0.00019593565683646114,
      "loss": 0.8417,
      "step": 480
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.9582865238189697,
      "learning_rate": 0.00019582841823056303,
      "loss": 0.883,
      "step": 490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.9171707630157471,
      "learning_rate": 0.0001957211796246649,
      "loss": 0.9056,
      "step": 500
    },
    {
      "epoch": 0.0816,
      "grad_norm": 1.3601685762405396,
      "learning_rate": 0.00019561394101876676,
      "loss": 0.9518,
      "step": 510
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.9139277338981628,
      "learning_rate": 0.00019550670241286864,
      "loss": 1.0374,
      "step": 520
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.9662815928459167,
      "learning_rate": 0.00019539946380697052,
      "loss": 0.8569,
      "step": 530
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.8720711469650269,
      "learning_rate": 0.0001952922252010724,
      "loss": 0.9062,
      "step": 540
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.9249356985092163,
      "learning_rate": 0.00019518498659517428,
      "loss": 0.8407,
      "step": 550
    },
    {
      "epoch": 0.0896,
      "grad_norm": 1.0087916851043701,
      "learning_rate": 0.00019507774798927614,
      "loss": 0.8828,
      "step": 560
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.9795021414756775,
      "learning_rate": 0.00019497050938337802,
      "loss": 0.9506,
      "step": 570
    },
    {
      "epoch": 0.0928,
      "grad_norm": 1.0200968980789185,
      "learning_rate": 0.0001948632707774799,
      "loss": 0.9594,
      "step": 580
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.912581205368042,
      "learning_rate": 0.00019475603217158178,
      "loss": 0.9149,
      "step": 590
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.0211478471755981,
      "learning_rate": 0.00019464879356568366,
      "loss": 0.9253,
      "step": 600
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.8852025866508484,
      "learning_rate": 0.00019454155495978552,
      "loss": 0.872,
      "step": 610
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.814598798751831,
      "learning_rate": 0.0001944343163538874,
      "loss": 0.8895,
      "step": 620
    },
    {
      "epoch": 0.1008,
      "grad_norm": 1.403814673423767,
      "learning_rate": 0.00019432707774798928,
      "loss": 0.9226,
      "step": 630
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.8195483088493347,
      "learning_rate": 0.00019421983914209116,
      "loss": 0.8749,
      "step": 640
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.8700811266899109,
      "learning_rate": 0.00019411260053619304,
      "loss": 0.8999,
      "step": 650
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.787926197052002,
      "learning_rate": 0.0001940053619302949,
      "loss": 0.9449,
      "step": 660
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.8920451402664185,
      "learning_rate": 0.00019389812332439678,
      "loss": 0.8392,
      "step": 670
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.8495211601257324,
      "learning_rate": 0.00019379088471849866,
      "loss": 0.9235,
      "step": 680
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.8611652851104736,
      "learning_rate": 0.00019368364611260054,
      "loss": 0.7928,
      "step": 690
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.8592662811279297,
      "learning_rate": 0.00019357640750670242,
      "loss": 0.9265,
      "step": 700
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.8899686932563782,
      "learning_rate": 0.0001934691689008043,
      "loss": 0.9356,
      "step": 710
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.9577620029449463,
      "learning_rate": 0.00019336193029490618,
      "loss": 0.9004,
      "step": 720
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.9833397269248962,
      "learning_rate": 0.00019325469168900806,
      "loss": 0.9822,
      "step": 730
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.8401193022727966,
      "learning_rate": 0.00019314745308310994,
      "loss": 0.9006,
      "step": 740
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.2558715343475342,
      "learning_rate": 0.00019304021447721182,
      "loss": 0.8821,
      "step": 750
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.9751529097557068,
      "learning_rate": 0.0001929329758713137,
      "loss": 0.9369,
      "step": 760
    },
    {
      "epoch": 0.1232,
      "grad_norm": 1.0909233093261719,
      "learning_rate": 0.00019282573726541556,
      "loss": 0.8498,
      "step": 770
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.7678048610687256,
      "learning_rate": 0.00019271849865951744,
      "loss": 0.9021,
      "step": 780
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.7466853857040405,
      "learning_rate": 0.00019261126005361932,
      "loss": 0.9001,
      "step": 790
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.7474244832992554,
      "learning_rate": 0.0001925040214477212,
      "loss": 0.8493,
      "step": 800
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.9157787561416626,
      "learning_rate": 0.00019239678284182308,
      "loss": 0.9494,
      "step": 810
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.8662886023521423,
      "learning_rate": 0.00019228954423592494,
      "loss": 0.8655,
      "step": 820
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.8992890119552612,
      "learning_rate": 0.00019218230563002682,
      "loss": 0.932,
      "step": 830
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.8139104247093201,
      "learning_rate": 0.0001920857908847185,
      "loss": 0.9474,
      "step": 840
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.8114705085754395,
      "learning_rate": 0.00019197855227882037,
      "loss": 0.8536,
      "step": 850
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.864425539970398,
      "learning_rate": 0.00019187131367292225,
      "loss": 0.8584,
      "step": 860
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.8010866045951843,
      "learning_rate": 0.00019176407506702413,
      "loss": 0.8987,
      "step": 870
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.9866406917572021,
      "learning_rate": 0.000191656836461126,
      "loss": 0.9284,
      "step": 880
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.8174899816513062,
      "learning_rate": 0.00019154959785522787,
      "loss": 0.9325,
      "step": 890
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.8837630748748779,
      "learning_rate": 0.00019144235924932975,
      "loss": 0.9339,
      "step": 900
    },
    {
      "epoch": 0.1456,
      "grad_norm": 2.4067258834838867,
      "learning_rate": 0.00019133512064343163,
      "loss": 0.8554,
      "step": 910
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.8210653066635132,
      "learning_rate": 0.0001912278820375335,
      "loss": 0.8601,
      "step": 920
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.8453694581985474,
      "learning_rate": 0.0001911206434316354,
      "loss": 0.9012,
      "step": 930
    },
    {
      "epoch": 0.1504,
      "grad_norm": 1.0019705295562744,
      "learning_rate": 0.00019101340482573727,
      "loss": 0.8943,
      "step": 940
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.9000385999679565,
      "learning_rate": 0.00019090616621983915,
      "loss": 0.799,
      "step": 950
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.7858740091323853,
      "learning_rate": 0.00019079892761394103,
      "loss": 0.8361,
      "step": 960
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.7700988054275513,
      "learning_rate": 0.00019069168900804292,
      "loss": 0.9298,
      "step": 970
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.7719398140907288,
      "learning_rate": 0.0001905844504021448,
      "loss": 0.899,
      "step": 980
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.9069082736968994,
      "learning_rate": 0.00019047721179624668,
      "loss": 0.8662,
      "step": 990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.9099870920181274,
      "learning_rate": 0.00019036997319034853,
      "loss": 0.8965,
      "step": 1000
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.8324466943740845,
      "learning_rate": 0.0001902627345844504,
      "loss": 0.9574,
      "step": 1010
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.7982953190803528,
      "learning_rate": 0.0001901554959785523,
      "loss": 0.8269,
      "step": 1020
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.7387027144432068,
      "learning_rate": 0.00019004825737265417,
      "loss": 0.9429,
      "step": 1030
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.911464512348175,
      "learning_rate": 0.00018994101876675606,
      "loss": 0.9126,
      "step": 1040
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.7120506763458252,
      "learning_rate": 0.0001898337801608579,
      "loss": 0.8257,
      "step": 1050
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.8649467825889587,
      "learning_rate": 0.0001897265415549598,
      "loss": 0.8784,
      "step": 1060
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.7970702052116394,
      "learning_rate": 0.00018961930294906167,
      "loss": 0.8994,
      "step": 1070
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.8938263058662415,
      "learning_rate": 0.00018951206434316355,
      "loss": 0.8853,
      "step": 1080
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.7394495010375977,
      "learning_rate": 0.00018940482573726543,
      "loss": 0.8451,
      "step": 1090
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.7920446395874023,
      "learning_rate": 0.0001892975871313673,
      "loss": 0.8834,
      "step": 1100
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.8232906460762024,
      "learning_rate": 0.00018919034852546917,
      "loss": 0.8676,
      "step": 1110
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.8221573233604431,
      "learning_rate": 0.00018908310991957105,
      "loss": 0.8993,
      "step": 1120
    },
    {
      "epoch": 0.1808,
      "grad_norm": 1.1317265033721924,
      "learning_rate": 0.00018897587131367293,
      "loss": 0.9843,
      "step": 1130
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.7720553278923035,
      "learning_rate": 0.0001888686327077748,
      "loss": 0.8591,
      "step": 1140
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.8968775272369385,
      "learning_rate": 0.00018876139410187667,
      "loss": 0.9716,
      "step": 1150
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.7787005305290222,
      "learning_rate": 0.00018865415549597855,
      "loss": 0.799,
      "step": 1160
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.8166452050209045,
      "learning_rate": 0.00018854691689008043,
      "loss": 0.8269,
      "step": 1170
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.6854956150054932,
      "learning_rate": 0.0001884396782841823,
      "loss": 0.9837,
      "step": 1180
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.8505339622497559,
      "learning_rate": 0.0001883324396782842,
      "loss": 0.8214,
      "step": 1190
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.9062142968177795,
      "learning_rate": 0.00018822520107238607,
      "loss": 0.8595,
      "step": 1200
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.9424830079078674,
      "learning_rate": 0.00018811796246648795,
      "loss": 0.9228,
      "step": 1210
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.9004493951797485,
      "learning_rate": 0.00018801072386058983,
      "loss": 0.8787,
      "step": 1220
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.6871142387390137,
      "learning_rate": 0.00018790348525469171,
      "loss": 0.8677,
      "step": 1230
    },
    {
      "epoch": 0.1984,
      "grad_norm": 1.4858087301254272,
      "learning_rate": 0.0001877962466487936,
      "loss": 0.8476,
      "step": 1240
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.8311103582382202,
      "learning_rate": 0.00018768900804289545,
      "loss": 0.8822,
      "step": 1250
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.834722638130188,
      "learning_rate": 0.00018758176943699733,
      "loss": 0.8835,
      "step": 1260
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.9377318024635315,
      "learning_rate": 0.0001874745308310992,
      "loss": 0.9704,
      "step": 1270
    },
    {
      "epoch": 0.2048,
      "grad_norm": 1.3805216550827026,
      "learning_rate": 0.0001873672922252011,
      "loss": 0.7932,
      "step": 1280
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.7379728555679321,
      "learning_rate": 0.00018726005361930297,
      "loss": 0.8777,
      "step": 1290
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.8453062772750854,
      "learning_rate": 0.00018715281501340483,
      "loss": 0.9176,
      "step": 1300
    },
    {
      "epoch": 0.2096,
      "grad_norm": 1.2522515058517456,
      "learning_rate": 0.0001870455764075067,
      "loss": 0.9582,
      "step": 1310
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.7807235717773438,
      "learning_rate": 0.0001869383378016086,
      "loss": 0.8509,
      "step": 1320
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.7880012392997742,
      "learning_rate": 0.00018683109919571047,
      "loss": 0.9058,
      "step": 1330
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.7557216286659241,
      "learning_rate": 0.00018672386058981235,
      "loss": 0.9386,
      "step": 1340
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.6488235592842102,
      "learning_rate": 0.0001866166219839142,
      "loss": 0.9285,
      "step": 1350
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.6238276362419128,
      "learning_rate": 0.0001865093833780161,
      "loss": 0.8875,
      "step": 1360
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.8498126864433289,
      "learning_rate": 0.00018640214477211797,
      "loss": 0.9118,
      "step": 1370
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.6774661540985107,
      "learning_rate": 0.00018629490616621985,
      "loss": 0.853,
      "step": 1380
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.7188985347747803,
      "learning_rate": 0.00018618766756032173,
      "loss": 0.9022,
      "step": 1390
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.8028320670127869,
      "learning_rate": 0.00018608042895442358,
      "loss": 0.8411,
      "step": 1400
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.6613509058952332,
      "learning_rate": 0.00018597319034852546,
      "loss": 0.9329,
      "step": 1410
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.8341197371482849,
      "learning_rate": 0.00018586595174262735,
      "loss": 0.8237,
      "step": 1420
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.7358617186546326,
      "learning_rate": 0.00018575871313672923,
      "loss": 0.8723,
      "step": 1430
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.7494811415672302,
      "learning_rate": 0.0001856514745308311,
      "loss": 0.8792,
      "step": 1440
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.6952432990074158,
      "learning_rate": 0.00018554423592493296,
      "loss": 0.8743,
      "step": 1450
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.6771245002746582,
      "learning_rate": 0.00018543699731903484,
      "loss": 0.8973,
      "step": 1460
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.8513880372047424,
      "learning_rate": 0.00018532975871313672,
      "loss": 0.8791,
      "step": 1470
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.7610437273979187,
      "learning_rate": 0.0001852225201072386,
      "loss": 0.8463,
      "step": 1480
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.7743268013000488,
      "learning_rate": 0.00018511528150134049,
      "loss": 0.855,
      "step": 1490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.6749657392501831,
      "learning_rate": 0.00018500804289544237,
      "loss": 0.869,
      "step": 1500
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.9077020287513733,
      "learning_rate": 0.00018490080428954425,
      "loss": 0.8149,
      "step": 1510
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.7728896737098694,
      "learning_rate": 0.00018479356568364613,
      "loss": 0.8249,
      "step": 1520
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.7755787968635559,
      "learning_rate": 0.000184686327077748,
      "loss": 0.804,
      "step": 1530
    },
    {
      "epoch": 0.2464,
      "grad_norm": 1.0985220670700073,
      "learning_rate": 0.0001845790884718499,
      "loss": 0.892,
      "step": 1540
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.9769983887672424,
      "learning_rate": 0.00018447184986595177,
      "loss": 0.9141,
      "step": 1550
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.7647235989570618,
      "learning_rate": 0.00018436461126005363,
      "loss": 0.8399,
      "step": 1560
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.9280610680580139,
      "learning_rate": 0.0001842573726541555,
      "loss": 0.9101,
      "step": 1570
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.9047017097473145,
      "learning_rate": 0.0001841501340482574,
      "loss": 0.8753,
      "step": 1580
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.8223361372947693,
      "learning_rate": 0.00018404289544235927,
      "loss": 0.853,
      "step": 1590
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.7218338251113892,
      "learning_rate": 0.00018393565683646115,
      "loss": 0.8595,
      "step": 1600
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.7745298743247986,
      "learning_rate": 0.000183828418230563,
      "loss": 0.8227,
      "step": 1610
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.6152061820030212,
      "learning_rate": 0.00018372117962466489,
      "loss": 0.8513,
      "step": 1620
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.8258132934570312,
      "learning_rate": 0.00018361394101876677,
      "loss": 0.8774,
      "step": 1630
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.6880797147750854,
      "learning_rate": 0.00018350670241286865,
      "loss": 0.9373,
      "step": 1640
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.7212272882461548,
      "learning_rate": 0.00018339946380697053,
      "loss": 0.7827,
      "step": 1650
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.6787062883377075,
      "learning_rate": 0.00018329222520107238,
      "loss": 0.906,
      "step": 1660
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.8179613947868347,
      "learning_rate": 0.00018318498659517426,
      "loss": 0.8854,
      "step": 1670
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.8513094186782837,
      "learning_rate": 0.00018307774798927614,
      "loss": 0.9601,
      "step": 1680
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.6489505767822266,
      "learning_rate": 0.00018297050938337803,
      "loss": 0.8032,
      "step": 1690
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.7249452471733093,
      "learning_rate": 0.0001828632707774799,
      "loss": 0.8768,
      "step": 1700
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.7649744153022766,
      "learning_rate": 0.00018275603217158176,
      "loss": 0.9606,
      "step": 1710
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.8241142630577087,
      "learning_rate": 0.00018264879356568364,
      "loss": 0.8936,
      "step": 1720
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.8127676248550415,
      "learning_rate": 0.00018254155495978552,
      "loss": 0.8425,
      "step": 1730
    },
    {
      "epoch": 0.2784,
      "grad_norm": 1.0824902057647705,
      "learning_rate": 0.0001824343163538874,
      "loss": 0.7568,
      "step": 1740
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.7596487402915955,
      "learning_rate": 0.00018232707774798928,
      "loss": 0.8595,
      "step": 1750
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.7784481644630432,
      "learning_rate": 0.00018221983914209117,
      "loss": 0.9487,
      "step": 1760
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.6321104168891907,
      "learning_rate": 0.00018211260053619305,
      "loss": 0.8787,
      "step": 1770
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.681395411491394,
      "learning_rate": 0.00018200536193029493,
      "loss": 0.9427,
      "step": 1780
    },
    {
      "epoch": 0.2864,
      "grad_norm": 1.1072310209274292,
      "learning_rate": 0.0001818981233243968,
      "loss": 0.8345,
      "step": 1790
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.8371753692626953,
      "learning_rate": 0.0001817908847184987,
      "loss": 0.8592,
      "step": 1800
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.7412726283073425,
      "learning_rate": 0.00018168364611260054,
      "loss": 0.8817,
      "step": 1810
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.9910528659820557,
      "learning_rate": 0.00018157640750670243,
      "loss": 0.8044,
      "step": 1820
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.7543120384216309,
      "learning_rate": 0.0001814691689008043,
      "loss": 0.8338,
      "step": 1830
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.7129238843917847,
      "learning_rate": 0.0001813619302949062,
      "loss": 0.8539,
      "step": 1840
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.7742741703987122,
      "learning_rate": 0.00018125469168900807,
      "loss": 0.8748,
      "step": 1850
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.7011245489120483,
      "learning_rate": 0.00018114745308310992,
      "loss": 0.8916,
      "step": 1860
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.6329489350318909,
      "learning_rate": 0.0001810402144772118,
      "loss": 0.9272,
      "step": 1870
    },
    {
      "epoch": 0.3008,
      "grad_norm": 4.106503486633301,
      "learning_rate": 0.00018093297587131368,
      "loss": 0.986,
      "step": 1880
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.8954188227653503,
      "learning_rate": 0.00018082573726541557,
      "loss": 0.7866,
      "step": 1890
    },
    {
      "epoch": 0.304,
      "grad_norm": 1.174816608428955,
      "learning_rate": 0.00018071849865951745,
      "loss": 0.913,
      "step": 1900
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.671986997127533,
      "learning_rate": 0.0001806112600536193,
      "loss": 0.8527,
      "step": 1910
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.7872729897499084,
      "learning_rate": 0.00018050402144772118,
      "loss": 0.8476,
      "step": 1920
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.7867233157157898,
      "learning_rate": 0.00018039678284182306,
      "loss": 0.8521,
      "step": 1930
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.7404147982597351,
      "learning_rate": 0.00018028954423592494,
      "loss": 0.9682,
      "step": 1940
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.7949889898300171,
      "learning_rate": 0.00018018230563002682,
      "loss": 0.898,
      "step": 1950
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.691166877746582,
      "learning_rate": 0.00018007506702412868,
      "loss": 0.8515,
      "step": 1960
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.9829215407371521,
      "learning_rate": 0.00017996782841823056,
      "loss": 0.9084,
      "step": 1970
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.7532026171684265,
      "learning_rate": 0.00017986058981233244,
      "loss": 0.914,
      "step": 1980
    },
    {
      "epoch": 0.3184,
      "grad_norm": 1.1436431407928467,
      "learning_rate": 0.00017975335120643432,
      "loss": 0.9309,
      "step": 1990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.6934042572975159,
      "learning_rate": 0.0001796461126005362,
      "loss": 0.8694,
      "step": 2000
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.8859463930130005,
      "learning_rate": 0.00017953887399463806,
      "loss": 0.9153,
      "step": 2010
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.7088987231254578,
      "learning_rate": 0.00017943163538873994,
      "loss": 0.9877,
      "step": 2020
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.5874021053314209,
      "learning_rate": 0.00017932439678284182,
      "loss": 0.8852,
      "step": 2030
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.7841086387634277,
      "learning_rate": 0.0001792171581769437,
      "loss": 0.8171,
      "step": 2040
    },
    {
      "epoch": 0.328,
      "grad_norm": 1.5594496726989746,
      "learning_rate": 0.00017910991957104558,
      "loss": 0.901,
      "step": 2050
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.8441852927207947,
      "learning_rate": 0.00017900268096514746,
      "loss": 0.8909,
      "step": 2060
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.7392749190330505,
      "learning_rate": 0.00017889544235924934,
      "loss": 0.851,
      "step": 2070
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.7299705743789673,
      "learning_rate": 0.00017878820375335122,
      "loss": 0.8516,
      "step": 2080
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.9032450318336487,
      "learning_rate": 0.0001786809651474531,
      "loss": 0.8277,
      "step": 2090
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.7443990707397461,
      "learning_rate": 0.00017857372654155499,
      "loss": 0.8662,
      "step": 2100
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.7372574210166931,
      "learning_rate": 0.00017846648793565684,
      "loss": 0.9367,
      "step": 2110
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.6845428347587585,
      "learning_rate": 0.00017835924932975872,
      "loss": 0.8633,
      "step": 2120
    },
    {
      "epoch": 0.3408,
      "grad_norm": 1.279124140739441,
      "learning_rate": 0.0001782520107238606,
      "loss": 0.9267,
      "step": 2130
    },
    {
      "epoch": 0.3424,
      "grad_norm": 1.3480224609375,
      "learning_rate": 0.00017814477211796248,
      "loss": 0.8273,
      "step": 2140
    },
    {
      "epoch": 0.344,
      "grad_norm": 1.1060785055160522,
      "learning_rate": 0.00017803753351206436,
      "loss": 0.9086,
      "step": 2150
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.9187103509902954,
      "learning_rate": 0.00017793029490616625,
      "loss": 0.9202,
      "step": 2160
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.9423102736473083,
      "learning_rate": 0.0001778230563002681,
      "loss": 0.7851,
      "step": 2170
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.6951724290847778,
      "learning_rate": 0.00017771581769436998,
      "loss": 0.8695,
      "step": 2180
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.7465663552284241,
      "learning_rate": 0.00017760857908847186,
      "loss": 0.8355,
      "step": 2190
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.7223344445228577,
      "learning_rate": 0.00017750134048257374,
      "loss": 0.8464,
      "step": 2200
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.7396516799926758,
      "learning_rate": 0.00017739410187667562,
      "loss": 0.8694,
      "step": 2210
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.6730302572250366,
      "learning_rate": 0.00017728686327077748,
      "loss": 0.9343,
      "step": 2220
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.6728771924972534,
      "learning_rate": 0.00017717962466487936,
      "loss": 0.8591,
      "step": 2230
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.680022656917572,
      "learning_rate": 0.00017707238605898124,
      "loss": 0.8715,
      "step": 2240
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.7274619936943054,
      "learning_rate": 0.00017696514745308312,
      "loss": 0.8905,
      "step": 2250
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.7371740937232971,
      "learning_rate": 0.000176857908847185,
      "loss": 0.9416,
      "step": 2260
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.8241097331047058,
      "learning_rate": 0.00017675067024128686,
      "loss": 0.8718,
      "step": 2270
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.7415503263473511,
      "learning_rate": 0.00017664343163538874,
      "loss": 0.8751,
      "step": 2280
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.7234851121902466,
      "learning_rate": 0.00017653619302949062,
      "loss": 0.8834,
      "step": 2290
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.748561680316925,
      "learning_rate": 0.0001764289544235925,
      "loss": 0.8876,
      "step": 2300
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.6299905776977539,
      "learning_rate": 0.00017632171581769438,
      "loss": 0.8457,
      "step": 2310
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.7099586725234985,
      "learning_rate": 0.00017621447721179626,
      "loss": 0.8395,
      "step": 2320
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.6653634905815125,
      "learning_rate": 0.00017610723860589814,
      "loss": 0.9198,
      "step": 2330
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.7224014401435852,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.928,
      "step": 2340
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.770639181137085,
      "learning_rate": 0.0001758927613941019,
      "loss": 0.9167,
      "step": 2350
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.8135313987731934,
      "learning_rate": 0.00017578552278820378,
      "loss": 0.897,
      "step": 2360
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.8315836191177368,
      "learning_rate": 0.00017567828418230564,
      "loss": 0.8147,
      "step": 2370
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.6893476247787476,
      "learning_rate": 0.00017557104557640752,
      "loss": 0.8637,
      "step": 2380
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.8793593049049377,
      "learning_rate": 0.0001754638069705094,
      "loss": 0.8545,
      "step": 2390
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.6729063391685486,
      "learning_rate": 0.00017535656836461128,
      "loss": 0.8764,
      "step": 2400
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.7322403192520142,
      "learning_rate": 0.00017524932975871316,
      "loss": 0.8605,
      "step": 2410
    },
    {
      "epoch": 0.3872,
      "grad_norm": 1.0041600465774536,
      "learning_rate": 0.00017514209115281502,
      "loss": 0.9053,
      "step": 2420
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.8268337845802307,
      "learning_rate": 0.0001750348525469169,
      "loss": 0.8556,
      "step": 2430
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.9431918859481812,
      "learning_rate": 0.00017492761394101878,
      "loss": 0.9002,
      "step": 2440
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.7440994381904602,
      "learning_rate": 0.00017482037533512066,
      "loss": 0.9075,
      "step": 2450
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.6134217381477356,
      "learning_rate": 0.00017471313672922254,
      "loss": 0.8316,
      "step": 2460
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.638504147529602,
      "learning_rate": 0.0001746058981233244,
      "loss": 0.8933,
      "step": 2470
    },
    {
      "epoch": 0.3968,
      "grad_norm": 1.007965326309204,
      "learning_rate": 0.00017449865951742628,
      "loss": 0.9563,
      "step": 2480
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.8311794996261597,
      "learning_rate": 0.00017439142091152816,
      "loss": 0.9366,
      "step": 2490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.752845287322998,
      "learning_rate": 0.00017428418230563004,
      "loss": 0.8734,
      "step": 2500
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.8446177244186401,
      "learning_rate": 0.00017417694369973192,
      "loss": 0.9244,
      "step": 2510
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.7429757714271545,
      "learning_rate": 0.00017406970509383377,
      "loss": 0.9113,
      "step": 2520
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.6213990449905396,
      "learning_rate": 0.00017396246648793565,
      "loss": 0.858,
      "step": 2530
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.7069748640060425,
      "learning_rate": 0.00017385522788203754,
      "loss": 0.8365,
      "step": 2540
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.6436460018157959,
      "learning_rate": 0.00017374798927613942,
      "loss": 0.8499,
      "step": 2550
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.7219617366790771,
      "learning_rate": 0.0001736407506702413,
      "loss": 0.8464,
      "step": 2560
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.6973505616188049,
      "learning_rate": 0.00017353351206434315,
      "loss": 0.9167,
      "step": 2570
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.6729381680488586,
      "learning_rate": 0.00017342627345844503,
      "loss": 0.8748,
      "step": 2580
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.7224538326263428,
      "learning_rate": 0.0001733190348525469,
      "loss": 0.8767,
      "step": 2590
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.8523063063621521,
      "learning_rate": 0.0001732117962466488,
      "loss": 0.9485,
      "step": 2600
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.7083364129066467,
      "learning_rate": 0.00017310455764075068,
      "loss": 0.8427,
      "step": 2610
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.7409102916717529,
      "learning_rate": 0.00017299731903485256,
      "loss": 0.8518,
      "step": 2620
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.6123518347740173,
      "learning_rate": 0.00017289008042895444,
      "loss": 0.8796,
      "step": 2630
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.8303993940353394,
      "learning_rate": 0.00017278284182305632,
      "loss": 0.8908,
      "step": 2640
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.7695809602737427,
      "learning_rate": 0.0001726756032171582,
      "loss": 0.8307,
      "step": 2650
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.7853949666023254,
      "learning_rate": 0.00017256836461126008,
      "loss": 0.9235,
      "step": 2660
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.6889123320579529,
      "learning_rate": 0.00017246112600536193,
      "loss": 0.929,
      "step": 2670
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.8801751136779785,
      "learning_rate": 0.00017235388739946382,
      "loss": 0.926,
      "step": 2680
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.7026476860046387,
      "learning_rate": 0.0001722466487935657,
      "loss": 0.9218,
      "step": 2690
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.6489119529724121,
      "learning_rate": 0.00017213941018766758,
      "loss": 0.8977,
      "step": 2700
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.6459269523620605,
      "learning_rate": 0.00017203217158176946,
      "loss": 0.821,
      "step": 2710
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.8322688937187195,
      "learning_rate": 0.0001719249329758713,
      "loss": 0.7802,
      "step": 2720
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.6785658001899719,
      "learning_rate": 0.0001718176943699732,
      "loss": 0.8799,
      "step": 2730
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.722669243812561,
      "learning_rate": 0.00017171045576407507,
      "loss": 0.8892,
      "step": 2740
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.8064242005348206,
      "learning_rate": 0.00017160321715817696,
      "loss": 0.9112,
      "step": 2750
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.8838402628898621,
      "learning_rate": 0.00017149597855227884,
      "loss": 0.8819,
      "step": 2760
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.6824437379837036,
      "learning_rate": 0.0001713887399463807,
      "loss": 0.8899,
      "step": 2770
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.6893851161003113,
      "learning_rate": 0.00017128150134048257,
      "loss": 0.8406,
      "step": 2780
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.8203131556510925,
      "learning_rate": 0.00017117426273458445,
      "loss": 0.825,
      "step": 2790
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.904710590839386,
      "learning_rate": 0.00017106702412868633,
      "loss": 0.8893,
      "step": 2800
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.8523906469345093,
      "learning_rate": 0.00017095978552278821,
      "loss": 0.9284,
      "step": 2810
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.7057099938392639,
      "learning_rate": 0.0001708525469168901,
      "loss": 0.8272,
      "step": 2820
    },
    {
      "epoch": 0.4528,
      "grad_norm": 1.1558095216751099,
      "learning_rate": 0.00017074530831099195,
      "loss": 0.8581,
      "step": 2830
    },
    {
      "epoch": 0.4544,
      "grad_norm": 1.0168360471725464,
      "learning_rate": 0.00017063806970509383,
      "loss": 0.9596,
      "step": 2840
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.7900441288948059,
      "learning_rate": 0.0001705308310991957,
      "loss": 0.8974,
      "step": 2850
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.687448263168335,
      "learning_rate": 0.0001704235924932976,
      "loss": 0.8041,
      "step": 2860
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.61264967918396,
      "learning_rate": 0.00017031635388739947,
      "loss": 0.9051,
      "step": 2870
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.6862094402313232,
      "learning_rate": 0.00017020911528150133,
      "loss": 0.8959,
      "step": 2880
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.6198872923851013,
      "learning_rate": 0.0001701018766756032,
      "loss": 0.9144,
      "step": 2890
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.6880835890769958,
      "learning_rate": 0.0001699946380697051,
      "loss": 0.877,
      "step": 2900
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.6939767599105835,
      "learning_rate": 0.00016988739946380697,
      "loss": 0.8617,
      "step": 2910
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.6586921215057373,
      "learning_rate": 0.00016978016085790885,
      "loss": 0.8836,
      "step": 2920
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.7258588671684265,
      "learning_rate": 0.00016967292225201073,
      "loss": 0.9032,
      "step": 2930
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.6966094970703125,
      "learning_rate": 0.00016956568364611261,
      "loss": 0.9537,
      "step": 2940
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.0503250360488892,
      "learning_rate": 0.0001694584450402145,
      "loss": 0.7494,
      "step": 2950
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.8614645600318909,
      "learning_rate": 0.00016935120643431638,
      "loss": 0.8681,
      "step": 2960
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.6319312453269958,
      "learning_rate": 0.00016924396782841826,
      "loss": 0.928,
      "step": 2970
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.7538132667541504,
      "learning_rate": 0.0001691367292225201,
      "loss": 0.8785,
      "step": 2980
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.7448370456695557,
      "learning_rate": 0.000169029490616622,
      "loss": 0.8719,
      "step": 2990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.9063394665718079,
      "learning_rate": 0.00016892225201072387,
      "loss": 0.9146,
      "step": 3000
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.6801275610923767,
      "learning_rate": 0.00016881501340482575,
      "loss": 0.769,
      "step": 3010
    },
    {
      "epoch": 0.4832,
      "grad_norm": 1.0270321369171143,
      "learning_rate": 0.00016870777479892764,
      "loss": 0.8502,
      "step": 3020
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.7010207176208496,
      "learning_rate": 0.0001686005361930295,
      "loss": 0.8288,
      "step": 3030
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.9036770462989807,
      "learning_rate": 0.00016849329758713137,
      "loss": 0.9281,
      "step": 3040
    },
    {
      "epoch": 0.488,
      "grad_norm": 1.0085421800613403,
      "learning_rate": 0.00016838605898123325,
      "loss": 0.9106,
      "step": 3050
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.7287367582321167,
      "learning_rate": 0.00016827882037533513,
      "loss": 0.9072,
      "step": 3060
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.8114569187164307,
      "learning_rate": 0.00016817158176943701,
      "loss": 0.8644,
      "step": 3070
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.7148950099945068,
      "learning_rate": 0.00016806434316353887,
      "loss": 0.8069,
      "step": 3080
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.8207691311836243,
      "learning_rate": 0.00016795710455764075,
      "loss": 0.894,
      "step": 3090
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.6934559941291809,
      "learning_rate": 0.00016784986595174263,
      "loss": 0.8981,
      "step": 3100
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.9872117042541504,
      "learning_rate": 0.0001677426273458445,
      "loss": 0.9181,
      "step": 3110
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.6216044425964355,
      "learning_rate": 0.0001676353887399464,
      "loss": 0.8301,
      "step": 3120
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.7769114375114441,
      "learning_rate": 0.00016752815013404825,
      "loss": 0.8588,
      "step": 3130
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.775115430355072,
      "learning_rate": 0.00016742091152815013,
      "loss": 0.8736,
      "step": 3140
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.6840680241584778,
      "learning_rate": 0.000167313672922252,
      "loss": 0.8067,
      "step": 3150
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.7702258825302124,
      "learning_rate": 0.0001672064343163539,
      "loss": 0.8104,
      "step": 3160
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.8447009921073914,
      "learning_rate": 0.00016709919571045577,
      "loss": 0.8436,
      "step": 3170
    },
    {
      "epoch": 0.5088,
      "grad_norm": 1.4258061647415161,
      "learning_rate": 0.00016699195710455765,
      "loss": 0.8062,
      "step": 3180
    },
    {
      "epoch": 0.5104,
      "grad_norm": 1.051682710647583,
      "learning_rate": 0.00016688471849865953,
      "loss": 0.8422,
      "step": 3190
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.7960178852081299,
      "learning_rate": 0.0001667774798927614,
      "loss": 0.8648,
      "step": 3200
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.7240840792655945,
      "learning_rate": 0.0001666702412868633,
      "loss": 0.8843,
      "step": 3210
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.8022804260253906,
      "learning_rate": 0.00016656300268096518,
      "loss": 0.8583,
      "step": 3220
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.7278520464897156,
      "learning_rate": 0.00016645576407506703,
      "loss": 0.8494,
      "step": 3230
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.7889235615730286,
      "learning_rate": 0.0001663485254691689,
      "loss": 0.8997,
      "step": 3240
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.8065764307975769,
      "learning_rate": 0.0001662412868632708,
      "loss": 0.986,
      "step": 3250
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.6659116744995117,
      "learning_rate": 0.00016613404825737267,
      "loss": 0.8458,
      "step": 3260
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.9231314659118652,
      "learning_rate": 0.00016602680965147455,
      "loss": 0.8995,
      "step": 3270
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.7140438556671143,
      "learning_rate": 0.0001659195710455764,
      "loss": 0.8163,
      "step": 3280
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.6488239765167236,
      "learning_rate": 0.0001658123324396783,
      "loss": 0.8426,
      "step": 3290
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.8237093687057495,
      "learning_rate": 0.00016570509383378017,
      "loss": 0.8625,
      "step": 3300
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.7028046250343323,
      "learning_rate": 0.00016559785522788205,
      "loss": 0.8409,
      "step": 3310
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.7451520562171936,
      "learning_rate": 0.00016549061662198393,
      "loss": 0.9099,
      "step": 3320
    },
    {
      "epoch": 0.5328,
      "grad_norm": 1.1655620336532593,
      "learning_rate": 0.00016538337801608579,
      "loss": 0.9469,
      "step": 3330
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.8928742408752441,
      "learning_rate": 0.00016527613941018767,
      "loss": 0.8551,
      "step": 3340
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.8259825706481934,
      "learning_rate": 0.00016516890080428955,
      "loss": 0.8319,
      "step": 3350
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.6780745387077332,
      "learning_rate": 0.00016506166219839143,
      "loss": 0.879,
      "step": 3360
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.7309971451759338,
      "learning_rate": 0.0001649544235924933,
      "loss": 0.8992,
      "step": 3370
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.6899217963218689,
      "learning_rate": 0.00016484718498659516,
      "loss": 0.8751,
      "step": 3380
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.8757660388946533,
      "learning_rate": 0.00016473994638069704,
      "loss": 0.8211,
      "step": 3390
    },
    {
      "epoch": 0.544,
      "grad_norm": 1.135775089263916,
      "learning_rate": 0.00016463270777479893,
      "loss": 0.8522,
      "step": 3400
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.9337292313575745,
      "learning_rate": 0.0001645254691689008,
      "loss": 0.9297,
      "step": 3410
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.7074317336082458,
      "learning_rate": 0.0001644182305630027,
      "loss": 0.8336,
      "step": 3420
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.823207437992096,
      "learning_rate": 0.00016431099195710457,
      "loss": 0.9575,
      "step": 3430
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.7623286843299866,
      "learning_rate": 0.00016420375335120642,
      "loss": 0.9066,
      "step": 3440
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.8203531503677368,
      "learning_rate": 0.0001640965147453083,
      "loss": 0.8608,
      "step": 3450
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.7593421339988708,
      "learning_rate": 0.00016398927613941018,
      "loss": 0.8567,
      "step": 3460
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.7124677896499634,
      "learning_rate": 0.00016388203753351207,
      "loss": 0.973,
      "step": 3470
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.7330427169799805,
      "learning_rate": 0.00016377479892761395,
      "loss": 0.8121,
      "step": 3480
    },
    {
      "epoch": 0.5584,
      "grad_norm": 1.0068422555923462,
      "learning_rate": 0.00016366756032171583,
      "loss": 0.8003,
      "step": 3490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.6184676289558411,
      "learning_rate": 0.0001635603217158177,
      "loss": 0.8773,
      "step": 3500
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.7362436056137085,
      "learning_rate": 0.0001634530831099196,
      "loss": 0.9277,
      "step": 3510
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.6312103271484375,
      "learning_rate": 0.00016334584450402147,
      "loss": 0.8553,
      "step": 3520
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.7818768620491028,
      "learning_rate": 0.00016323860589812335,
      "loss": 0.8552,
      "step": 3530
    },
    {
      "epoch": 0.5664,
      "grad_norm": 1.2001996040344238,
      "learning_rate": 0.0001631313672922252,
      "loss": 0.8844,
      "step": 3540
    },
    {
      "epoch": 0.568,
      "grad_norm": 1.0087262392044067,
      "learning_rate": 0.0001630241286863271,
      "loss": 0.8695,
      "step": 3550
    },
    {
      "epoch": 0.5696,
      "grad_norm": 1.1089829206466675,
      "learning_rate": 0.00016291689008042897,
      "loss": 0.8526,
      "step": 3560
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.7011741399765015,
      "learning_rate": 0.00016280965147453085,
      "loss": 0.8574,
      "step": 3570
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.8714403510093689,
      "learning_rate": 0.00016270241286863273,
      "loss": 0.7731,
      "step": 3580
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.8100429177284241,
      "learning_rate": 0.00016259517426273458,
      "loss": 0.8677,
      "step": 3590
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.6110560894012451,
      "learning_rate": 0.00016248793565683647,
      "loss": 0.8026,
      "step": 3600
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.6621553301811218,
      "learning_rate": 0.00016238069705093835,
      "loss": 0.8091,
      "step": 3610
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.773910403251648,
      "learning_rate": 0.00016227345844504023,
      "loss": 0.8357,
      "step": 3620
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.8817921876907349,
      "learning_rate": 0.0001621662198391421,
      "loss": 0.8572,
      "step": 3630
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.7719159126281738,
      "learning_rate": 0.00016205898123324396,
      "loss": 0.8865,
      "step": 3640
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.9580062627792358,
      "learning_rate": 0.00016195174262734584,
      "loss": 0.8173,
      "step": 3650
    },
    {
      "epoch": 0.5856,
      "grad_norm": 1.045613169670105,
      "learning_rate": 0.00016184450402144772,
      "loss": 0.8462,
      "step": 3660
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.6325754523277283,
      "learning_rate": 0.0001617372654155496,
      "loss": 0.8939,
      "step": 3670
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.7757503986358643,
      "learning_rate": 0.00016163002680965149,
      "loss": 0.7989,
      "step": 3680
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.7151972651481628,
      "learning_rate": 0.00016152278820375334,
      "loss": 0.8784,
      "step": 3690
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.6797577142715454,
      "learning_rate": 0.00016141554959785522,
      "loss": 0.8647,
      "step": 3700
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.7467578649520874,
      "learning_rate": 0.0001613083109919571,
      "loss": 0.84,
      "step": 3710
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.9354897737503052,
      "learning_rate": 0.00016120107238605898,
      "loss": 0.8915,
      "step": 3720
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.9395861029624939,
      "learning_rate": 0.00016109383378016086,
      "loss": 0.906,
      "step": 3730
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.9643675088882446,
      "learning_rate": 0.00016098659517426275,
      "loss": 0.9149,
      "step": 3740
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.7661268711090088,
      "learning_rate": 0.00016087935656836463,
      "loss": 0.7852,
      "step": 3750
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.741691529750824,
      "learning_rate": 0.0001607721179624665,
      "loss": 0.879,
      "step": 3760
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.8401238918304443,
      "learning_rate": 0.0001606648793565684,
      "loss": 0.8114,
      "step": 3770
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.6109321713447571,
      "learning_rate": 0.00016055764075067027,
      "loss": 0.8237,
      "step": 3780
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.7054662108421326,
      "learning_rate": 0.00016045040214477212,
      "loss": 0.8191,
      "step": 3790
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.157281517982483,
      "learning_rate": 0.000160343163538874,
      "loss": 0.8586,
      "step": 3800
    },
    {
      "epoch": 0.6096,
      "grad_norm": 1.3112642765045166,
      "learning_rate": 0.00016023592493297589,
      "loss": 0.8853,
      "step": 3810
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.8728044033050537,
      "learning_rate": 0.00016012868632707777,
      "loss": 0.8998,
      "step": 3820
    },
    {
      "epoch": 0.6128,
      "grad_norm": 1.0966529846191406,
      "learning_rate": 0.00016002144772117965,
      "loss": 0.9379,
      "step": 3830
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.7533923983573914,
      "learning_rate": 0.0001599142091152815,
      "loss": 0.7662,
      "step": 3840
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.9675400257110596,
      "learning_rate": 0.00015980697050938338,
      "loss": 0.8251,
      "step": 3850
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.6993877291679382,
      "learning_rate": 0.00015969973190348526,
      "loss": 0.8819,
      "step": 3860
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.5622251629829407,
      "learning_rate": 0.00015959249329758714,
      "loss": 0.9617,
      "step": 3870
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.7116361856460571,
      "learning_rate": 0.00015948525469168903,
      "loss": 0.8728,
      "step": 3880
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.92882239818573,
      "learning_rate": 0.00015937801608579088,
      "loss": 0.8195,
      "step": 3890
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.7542548179626465,
      "learning_rate": 0.00015927077747989276,
      "loss": 0.9068,
      "step": 3900
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.9749752879142761,
      "learning_rate": 0.00015916353887399464,
      "loss": 0.8096,
      "step": 3910
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.8790366053581238,
      "learning_rate": 0.00015905630026809652,
      "loss": 0.8758,
      "step": 3920
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.7584472298622131,
      "learning_rate": 0.0001589490616621984,
      "loss": 0.8347,
      "step": 3930
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.6800808310508728,
      "learning_rate": 0.00015884182305630026,
      "loss": 0.8904,
      "step": 3940
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.7728254199028015,
      "learning_rate": 0.00015873458445040214,
      "loss": 0.8616,
      "step": 3950
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.5801419019699097,
      "learning_rate": 0.00015862734584450402,
      "loss": 0.8688,
      "step": 3960
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.6711939573287964,
      "learning_rate": 0.0001585201072386059,
      "loss": 0.9032,
      "step": 3970
    },
    {
      "epoch": 0.6368,
      "grad_norm": 1.011583924293518,
      "learning_rate": 0.00015841286863270778,
      "loss": 0.8637,
      "step": 3980
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.699591875076294,
      "learning_rate": 0.00015830563002680964,
      "loss": 0.862,
      "step": 3990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.694337785243988,
      "learning_rate": 0.00015819839142091152,
      "loss": 0.961,
      "step": 4000
    },
    {
      "epoch": 0.6416,
      "grad_norm": 1.037682056427002,
      "learning_rate": 0.0001580911528150134,
      "loss": 0.8093,
      "step": 4010
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.735322892665863,
      "learning_rate": 0.00015798391420911528,
      "loss": 0.7634,
      "step": 4020
    },
    {
      "epoch": 0.6448,
      "grad_norm": 1.084470510482788,
      "learning_rate": 0.00015787667560321716,
      "loss": 0.9213,
      "step": 4030
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.6463038325309753,
      "learning_rate": 0.00015776943699731904,
      "loss": 0.8971,
      "step": 4040
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.7222857475280762,
      "learning_rate": 0.00015766219839142092,
      "loss": 0.831,
      "step": 4050
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.6800944805145264,
      "learning_rate": 0.0001575549597855228,
      "loss": 0.8522,
      "step": 4060
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.974783182144165,
      "learning_rate": 0.00015744772117962468,
      "loss": 0.8688,
      "step": 4070
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.7430446147918701,
      "learning_rate": 0.00015734048257372657,
      "loss": 0.9169,
      "step": 4080
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.8265076875686646,
      "learning_rate": 0.00015723324396782845,
      "loss": 0.8634,
      "step": 4090
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.8744071125984192,
      "learning_rate": 0.0001571260053619303,
      "loss": 0.8878,
      "step": 4100
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.8624708652496338,
      "learning_rate": 0.00015701876675603218,
      "loss": 0.8703,
      "step": 4110
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.8421484231948853,
      "learning_rate": 0.00015691152815013406,
      "loss": 0.9026,
      "step": 4120
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.6911711692810059,
      "learning_rate": 0.00015680428954423594,
      "loss": 0.8748,
      "step": 4130
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.5639431476593018,
      "learning_rate": 0.00015669705093833782,
      "loss": 0.973,
      "step": 4140
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.7010010480880737,
      "learning_rate": 0.00015658981233243968,
      "loss": 0.8047,
      "step": 4150
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.7978969216346741,
      "learning_rate": 0.00015648257372654156,
      "loss": 0.8232,
      "step": 4160
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.6720172166824341,
      "learning_rate": 0.00015637533512064344,
      "loss": 0.8505,
      "step": 4170
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.7756274938583374,
      "learning_rate": 0.00015626809651474532,
      "loss": 0.8312,
      "step": 4180
    },
    {
      "epoch": 0.6704,
      "grad_norm": 1.48202645778656,
      "learning_rate": 0.0001561608579088472,
      "loss": 0.8705,
      "step": 4190
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.7310895919799805,
      "learning_rate": 0.00015605361930294906,
      "loss": 0.8678,
      "step": 4200
    },
    {
      "epoch": 0.6736,
      "grad_norm": 1.0043318271636963,
      "learning_rate": 0.00015594638069705094,
      "loss": 0.7756,
      "step": 4210
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.9966217279434204,
      "learning_rate": 0.00015583914209115282,
      "loss": 0.8779,
      "step": 4220
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.8083496689796448,
      "learning_rate": 0.0001557319034852547,
      "loss": 0.9219,
      "step": 4230
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.6859479546546936,
      "learning_rate": 0.00015562466487935658,
      "loss": 0.8558,
      "step": 4240
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.7450312972068787,
      "learning_rate": 0.00015551742627345843,
      "loss": 0.8513,
      "step": 4250
    },
    {
      "epoch": 0.6816,
      "grad_norm": 1.0655044317245483,
      "learning_rate": 0.00015541018766756032,
      "loss": 0.8575,
      "step": 4260
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.6165761351585388,
      "learning_rate": 0.0001553029490616622,
      "loss": 0.9203,
      "step": 4270
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.675071656703949,
      "learning_rate": 0.00015519571045576408,
      "loss": 0.8961,
      "step": 4280
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.8041562438011169,
      "learning_rate": 0.00015508847184986596,
      "loss": 0.8306,
      "step": 4290
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.7836536169052124,
      "learning_rate": 0.00015498123324396784,
      "loss": 0.9076,
      "step": 4300
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.8387345671653748,
      "learning_rate": 0.00015487399463806972,
      "loss": 0.8489,
      "step": 4310
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.9412363767623901,
      "learning_rate": 0.0001547667560321716,
      "loss": 0.8669,
      "step": 4320
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.6653386354446411,
      "learning_rate": 0.00015465951742627348,
      "loss": 0.9023,
      "step": 4330
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.6817538142204285,
      "learning_rate": 0.00015455227882037536,
      "loss": 0.8648,
      "step": 4340
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.7204171419143677,
      "learning_rate": 0.00015444504021447722,
      "loss": 0.8366,
      "step": 4350
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.9991927742958069,
      "learning_rate": 0.0001543378016085791,
      "loss": 0.8914,
      "step": 4360
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.9363240003585815,
      "learning_rate": 0.00015423056300268098,
      "loss": 0.752,
      "step": 4370
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.7689113020896912,
      "learning_rate": 0.00015412332439678286,
      "loss": 0.8755,
      "step": 4380
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.8113163113594055,
      "learning_rate": 0.00015401608579088474,
      "loss": 0.8959,
      "step": 4390
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.617612361907959,
      "learning_rate": 0.0001539088471849866,
      "loss": 0.8889,
      "step": 4400
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.7081194519996643,
      "learning_rate": 0.00015380160857908848,
      "loss": 0.9059,
      "step": 4410
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.9116732478141785,
      "learning_rate": 0.00015369436997319036,
      "loss": 0.8578,
      "step": 4420
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.9368065595626831,
      "learning_rate": 0.00015358713136729224,
      "loss": 0.7533,
      "step": 4430
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.6566431522369385,
      "learning_rate": 0.00015347989276139412,
      "loss": 0.8676,
      "step": 4440
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.7661554217338562,
      "learning_rate": 0.00015337265415549597,
      "loss": 0.8648,
      "step": 4450
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.6396783590316772,
      "learning_rate": 0.00015326541554959786,
      "loss": 0.9316,
      "step": 4460
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.6614757180213928,
      "learning_rate": 0.00015315817694369974,
      "loss": 0.8312,
      "step": 4470
    },
    {
      "epoch": 0.7168,
      "grad_norm": 1.178830623626709,
      "learning_rate": 0.00015305093833780162,
      "loss": 0.8461,
      "step": 4480
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.924221396446228,
      "learning_rate": 0.0001529436997319035,
      "loss": 0.8093,
      "step": 4490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.7869196534156799,
      "learning_rate": 0.00015283646112600535,
      "loss": 0.9076,
      "step": 4500
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.794657289981842,
      "learning_rate": 0.00015272922252010723,
      "loss": 0.8779,
      "step": 4510
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.7010740041732788,
      "learning_rate": 0.00015262198391420911,
      "loss": 0.7782,
      "step": 4520
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.8564744591712952,
      "learning_rate": 0.000152514745308311,
      "loss": 0.9114,
      "step": 4530
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.6569874882698059,
      "learning_rate": 0.00015240750670241288,
      "loss": 0.8396,
      "step": 4540
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.8523711562156677,
      "learning_rate": 0.00015230026809651473,
      "loss": 0.8384,
      "step": 4550
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.6351200938224792,
      "learning_rate": 0.0001521930294906166,
      "loss": 0.9109,
      "step": 4560
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.8600683212280273,
      "learning_rate": 0.0001520857908847185,
      "loss": 0.8288,
      "step": 4570
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.5774193406105042,
      "learning_rate": 0.00015197855227882037,
      "loss": 0.8776,
      "step": 4580
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.8937639594078064,
      "learning_rate": 0.00015187131367292225,
      "loss": 0.9094,
      "step": 4590
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.6737355589866638,
      "learning_rate": 0.00015176407506702414,
      "loss": 0.8492,
      "step": 4600
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.6744382381439209,
      "learning_rate": 0.00015165683646112602,
      "loss": 0.8368,
      "step": 4610
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.8673995137214661,
      "learning_rate": 0.0001515495978552279,
      "loss": 0.8162,
      "step": 4620
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.7784472107887268,
      "learning_rate": 0.00015144235924932978,
      "loss": 0.9214,
      "step": 4630
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.9046472907066345,
      "learning_rate": 0.00015133512064343166,
      "loss": 0.9235,
      "step": 4640
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.6117310523986816,
      "learning_rate": 0.00015122788203753354,
      "loss": 0.8674,
      "step": 4650
    },
    {
      "epoch": 0.7456,
      "grad_norm": 1.0274213552474976,
      "learning_rate": 0.0001511206434316354,
      "loss": 0.8181,
      "step": 4660
    },
    {
      "epoch": 0.7472,
      "grad_norm": 1.1416860818862915,
      "learning_rate": 0.00015101340482573728,
      "loss": 0.8708,
      "step": 4670
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.9854426980018616,
      "learning_rate": 0.00015090616621983916,
      "loss": 0.94,
      "step": 4680
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.666513204574585,
      "learning_rate": 0.00015079892761394104,
      "loss": 0.8051,
      "step": 4690
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.6955423951148987,
      "learning_rate": 0.00015069168900804292,
      "loss": 0.8814,
      "step": 4700
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.7076730728149414,
      "learning_rate": 0.00015058445040214477,
      "loss": 0.8836,
      "step": 4710
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.8440508842468262,
      "learning_rate": 0.00015047721179624665,
      "loss": 0.775,
      "step": 4720
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.7184525728225708,
      "learning_rate": 0.00015036997319034854,
      "loss": 0.8498,
      "step": 4730
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.8031149506568909,
      "learning_rate": 0.00015026273458445042,
      "loss": 0.8889,
      "step": 4740
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9360349178314209,
      "learning_rate": 0.0001501554959785523,
      "loss": 0.7882,
      "step": 4750
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.7087934613227844,
      "learning_rate": 0.00015004825737265415,
      "loss": 0.8459,
      "step": 4760
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.7520453333854675,
      "learning_rate": 0.00014994101876675603,
      "loss": 0.8379,
      "step": 4770
    },
    {
      "epoch": 0.7648,
      "grad_norm": 1.6336114406585693,
      "learning_rate": 0.0001498337801608579,
      "loss": 0.8805,
      "step": 4780
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.7964634895324707,
      "learning_rate": 0.0001497265415549598,
      "loss": 0.8281,
      "step": 4790
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.8507673144340515,
      "learning_rate": 0.00014961930294906168,
      "loss": 0.7949,
      "step": 4800
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.9259268641471863,
      "learning_rate": 0.00014951206434316353,
      "loss": 0.8027,
      "step": 4810
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.6952120661735535,
      "learning_rate": 0.0001494048257372654,
      "loss": 0.8788,
      "step": 4820
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.6997096538543701,
      "learning_rate": 0.0001492975871313673,
      "loss": 0.8975,
      "step": 4830
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.8944476246833801,
      "learning_rate": 0.00014919034852546917,
      "loss": 0.8746,
      "step": 4840
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.6927070021629333,
      "learning_rate": 0.00014908310991957105,
      "loss": 0.9593,
      "step": 4850
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.6903032660484314,
      "learning_rate": 0.00014897587131367293,
      "loss": 0.9201,
      "step": 4860
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.9382808208465576,
      "learning_rate": 0.00014886863270777482,
      "loss": 0.8477,
      "step": 4870
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.9319166541099548,
      "learning_rate": 0.0001487613941018767,
      "loss": 0.9783,
      "step": 4880
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.8032239675521851,
      "learning_rate": 0.00014865415549597858,
      "loss": 0.9352,
      "step": 4890
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.924776554107666,
      "learning_rate": 0.00014854691689008046,
      "loss": 0.8598,
      "step": 4900
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.6995887160301208,
      "learning_rate": 0.0001484396782841823,
      "loss": 0.787,
      "step": 4910
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.6908847093582153,
      "learning_rate": 0.0001483324396782842,
      "loss": 0.8137,
      "step": 4920
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.9836457371711731,
      "learning_rate": 0.00014822520107238607,
      "loss": 0.8617,
      "step": 4930
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.7120361328125,
      "learning_rate": 0.00014811796246648796,
      "loss": 0.9047,
      "step": 4940
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.5956265330314636,
      "learning_rate": 0.00014801072386058984,
      "loss": 0.86,
      "step": 4950
    },
    {
      "epoch": 0.7936,
      "grad_norm": 1.2090274095535278,
      "learning_rate": 0.0001479034852546917,
      "loss": 0.8202,
      "step": 4960
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.7165187001228333,
      "learning_rate": 0.00014779624664879357,
      "loss": 0.8423,
      "step": 4970
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.7105640769004822,
      "learning_rate": 0.00014768900804289545,
      "loss": 0.8246,
      "step": 4980
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.6005882620811462,
      "learning_rate": 0.00014758176943699733,
      "loss": 0.8237,
      "step": 4990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7547942996025085,
      "learning_rate": 0.00014747453083109921,
      "loss": 0.8079,
      "step": 5000
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.8484371304512024,
      "learning_rate": 0.00014736729222520107,
      "loss": 0.8634,
      "step": 5010
    },
    {
      "epoch": 0.8032,
      "grad_norm": 1.0458602905273438,
      "learning_rate": 0.00014726005361930295,
      "loss": 0.8733,
      "step": 5020
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.7359113097190857,
      "learning_rate": 0.00014715281501340483,
      "loss": 0.8601,
      "step": 5030
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.7352451086044312,
      "learning_rate": 0.0001470455764075067,
      "loss": 0.9051,
      "step": 5040
    },
    {
      "epoch": 0.808,
      "grad_norm": 1.1195111274719238,
      "learning_rate": 0.0001469383378016086,
      "loss": 0.8998,
      "step": 5050
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.5782513618469238,
      "learning_rate": 0.00014683109919571045,
      "loss": 0.9535,
      "step": 5060
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.6740774512290955,
      "learning_rate": 0.00014672386058981233,
      "loss": 0.8349,
      "step": 5070
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.776935338973999,
      "learning_rate": 0.0001466166219839142,
      "loss": 0.8367,
      "step": 5080
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.6032547950744629,
      "learning_rate": 0.0001465093833780161,
      "loss": 0.8913,
      "step": 5090
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.6294367909431458,
      "learning_rate": 0.00014640214477211797,
      "loss": 0.8296,
      "step": 5100
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.7210378646850586,
      "learning_rate": 0.00014629490616621983,
      "loss": 0.9509,
      "step": 5110
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.621893048286438,
      "learning_rate": 0.0001461876675603217,
      "loss": 0.9097,
      "step": 5120
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.7521538734436035,
      "learning_rate": 0.0001460804289544236,
      "loss": 0.8784,
      "step": 5130
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.8266655802726746,
      "learning_rate": 0.00014597319034852547,
      "loss": 0.8412,
      "step": 5140
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.7173001766204834,
      "learning_rate": 0.00014586595174262735,
      "loss": 0.8306,
      "step": 5150
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.8163952231407166,
      "learning_rate": 0.00014575871313672923,
      "loss": 0.9255,
      "step": 5160
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.9681163430213928,
      "learning_rate": 0.0001456514745308311,
      "loss": 0.9496,
      "step": 5170
    },
    {
      "epoch": 0.8288,
      "grad_norm": 1.1269347667694092,
      "learning_rate": 0.000145544235924933,
      "loss": 0.9328,
      "step": 5180
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.7882651090621948,
      "learning_rate": 0.00014543699731903487,
      "loss": 0.9473,
      "step": 5190
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.7138461470603943,
      "learning_rate": 0.00014532975871313675,
      "loss": 0.8782,
      "step": 5200
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.7527744174003601,
      "learning_rate": 0.0001452225201072386,
      "loss": 0.8154,
      "step": 5210
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.7286292910575867,
      "learning_rate": 0.0001451152815013405,
      "loss": 0.9529,
      "step": 5220
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.8472781181335449,
      "learning_rate": 0.00014500804289544237,
      "loss": 0.7794,
      "step": 5230
    },
    {
      "epoch": 0.8384,
      "grad_norm": 1.1447227001190186,
      "learning_rate": 0.00014490080428954425,
      "loss": 0.8267,
      "step": 5240
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.123854637145996,
      "learning_rate": 0.00014479356568364613,
      "loss": 0.932,
      "step": 5250
    },
    {
      "epoch": 0.8416,
      "grad_norm": 1.7000774145126343,
      "learning_rate": 0.000144686327077748,
      "loss": 0.7943,
      "step": 5260
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.8102423548698425,
      "learning_rate": 0.00014457908847184987,
      "loss": 0.8311,
      "step": 5270
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.6215223670005798,
      "learning_rate": 0.00014447184986595175,
      "loss": 0.823,
      "step": 5280
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.8263357877731323,
      "learning_rate": 0.00014436461126005363,
      "loss": 0.9108,
      "step": 5290
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.6109849214553833,
      "learning_rate": 0.0001442573726541555,
      "loss": 0.8531,
      "step": 5300
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.7543982267379761,
      "learning_rate": 0.0001441501340482574,
      "loss": 0.8561,
      "step": 5310
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.6023393273353577,
      "learning_rate": 0.00014404289544235925,
      "loss": 0.8942,
      "step": 5320
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.9367269277572632,
      "learning_rate": 0.00014393565683646113,
      "loss": 0.8278,
      "step": 5330
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.6600990295410156,
      "learning_rate": 0.000143828418230563,
      "loss": 0.9106,
      "step": 5340
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.6210778951644897,
      "learning_rate": 0.0001437211796246649,
      "loss": 0.8799,
      "step": 5350
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.9118412733078003,
      "learning_rate": 0.00014361394101876677,
      "loss": 0.8303,
      "step": 5360
    },
    {
      "epoch": 0.8592,
      "grad_norm": 1.1347882747650146,
      "learning_rate": 0.00014350670241286862,
      "loss": 0.8483,
      "step": 5370
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.6784111261367798,
      "learning_rate": 0.0001433994638069705,
      "loss": 0.8216,
      "step": 5380
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.6717342138290405,
      "learning_rate": 0.00014329222520107239,
      "loss": 0.7644,
      "step": 5390
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.8595510721206665,
      "learning_rate": 0.00014318498659517427,
      "loss": 0.9189,
      "step": 5400
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.6657931804656982,
      "learning_rate": 0.00014307774798927615,
      "loss": 0.889,
      "step": 5410
    },
    {
      "epoch": 0.8672,
      "grad_norm": 1.0895333290100098,
      "learning_rate": 0.000142970509383378,
      "loss": 0.8822,
      "step": 5420
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.6522995233535767,
      "learning_rate": 0.00014286327077747988,
      "loss": 0.8472,
      "step": 5430
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.7047842741012573,
      "learning_rate": 0.00014275603217158176,
      "loss": 0.897,
      "step": 5440
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.8704065084457397,
      "learning_rate": 0.00014264879356568365,
      "loss": 0.8213,
      "step": 5450
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.6602309346199036,
      "learning_rate": 0.00014254155495978553,
      "loss": 0.7939,
      "step": 5460
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.7825422883033752,
      "learning_rate": 0.0001424343163538874,
      "loss": 0.9254,
      "step": 5470
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.7434900999069214,
      "learning_rate": 0.0001423270777479893,
      "loss": 0.9107,
      "step": 5480
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.7966040968894958,
      "learning_rate": 0.00014221983914209117,
      "loss": 0.855,
      "step": 5490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.8731599450111389,
      "learning_rate": 0.00014211260053619305,
      "loss": 0.8423,
      "step": 5500
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.8052433133125305,
      "learning_rate": 0.00014201608579088472,
      "loss": 0.8845,
      "step": 5510
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.594792366027832,
      "learning_rate": 0.0001419088471849866,
      "loss": 0.8486,
      "step": 5520
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.6200520992279053,
      "learning_rate": 0.00014180160857908848,
      "loss": 0.8734,
      "step": 5530
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.831871747970581,
      "learning_rate": 0.00014169436997319036,
      "loss": 0.9149,
      "step": 5540
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.6231049299240112,
      "learning_rate": 0.00014158713136729222,
      "loss": 0.8162,
      "step": 5550
    },
    {
      "epoch": 0.8896,
      "grad_norm": 1.0548337697982788,
      "learning_rate": 0.0001414798927613941,
      "loss": 0.9224,
      "step": 5560
    },
    {
      "epoch": 0.8912,
      "grad_norm": 1.0876117944717407,
      "learning_rate": 0.00014137265415549598,
      "loss": 0.8446,
      "step": 5570
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.8023124933242798,
      "learning_rate": 0.00014126541554959786,
      "loss": 0.8984,
      "step": 5580
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.74778151512146,
      "learning_rate": 0.00014115817694369974,
      "loss": 0.8319,
      "step": 5590
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.8001161217689514,
      "learning_rate": 0.0001410509383378016,
      "loss": 0.8493,
      "step": 5600
    },
    {
      "epoch": 0.8976,
      "grad_norm": 1.1540591716766357,
      "learning_rate": 0.00014094369973190348,
      "loss": 0.9201,
      "step": 5610
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.8573755621910095,
      "learning_rate": 0.00014083646112600536,
      "loss": 0.8224,
      "step": 5620
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.7891312837600708,
      "learning_rate": 0.00014072922252010724,
      "loss": 0.8441,
      "step": 5630
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.6749699115753174,
      "learning_rate": 0.00014062198391420912,
      "loss": 0.8376,
      "step": 5640
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.655934751033783,
      "learning_rate": 0.000140514745308311,
      "loss": 0.8304,
      "step": 5650
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.9628641605377197,
      "learning_rate": 0.00014040750670241288,
      "loss": 0.8841,
      "step": 5660
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.9498627781867981,
      "learning_rate": 0.00014030026809651476,
      "loss": 0.869,
      "step": 5670
    },
    {
      "epoch": 0.9088,
      "grad_norm": 1.0261642932891846,
      "learning_rate": 0.00014019302949061665,
      "loss": 0.8526,
      "step": 5680
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.674695611000061,
      "learning_rate": 0.00014008579088471853,
      "loss": 0.8365,
      "step": 5690
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.6434338688850403,
      "learning_rate": 0.00013997855227882038,
      "loss": 0.8053,
      "step": 5700
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.9064446091651917,
      "learning_rate": 0.00013987131367292226,
      "loss": 0.918,
      "step": 5710
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.9754143953323364,
      "learning_rate": 0.00013976407506702414,
      "loss": 0.8571,
      "step": 5720
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.6456582546234131,
      "learning_rate": 0.00013965683646112602,
      "loss": 0.7937,
      "step": 5730
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.7911648154258728,
      "learning_rate": 0.0001395495978552279,
      "loss": 0.7797,
      "step": 5740
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.8828688859939575,
      "learning_rate": 0.00013944235924932976,
      "loss": 0.8343,
      "step": 5750
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.72029709815979,
      "learning_rate": 0.00013933512064343164,
      "loss": 0.8896,
      "step": 5760
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.7055197954177856,
      "learning_rate": 0.00013922788203753352,
      "loss": 0.8509,
      "step": 5770
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.8384346961975098,
      "learning_rate": 0.0001391206434316354,
      "loss": 0.8644,
      "step": 5780
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.6035821437835693,
      "learning_rate": 0.00013901340482573728,
      "loss": 0.8223,
      "step": 5790
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.7543339729309082,
      "learning_rate": 0.00013890616621983914,
      "loss": 0.8831,
      "step": 5800
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.7034002542495728,
      "learning_rate": 0.00013879892761394102,
      "loss": 0.8632,
      "step": 5810
    },
    {
      "epoch": 0.9312,
      "grad_norm": 1.8715260028839111,
      "learning_rate": 0.0001386916890080429,
      "loss": 0.8631,
      "step": 5820
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.6854667663574219,
      "learning_rate": 0.00013858445040214478,
      "loss": 0.8892,
      "step": 5830
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.7102998495101929,
      "learning_rate": 0.00013847721179624666,
      "loss": 0.8412,
      "step": 5840
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.7892393469810486,
      "learning_rate": 0.00013836997319034851,
      "loss": 0.8636,
      "step": 5850
    },
    {
      "epoch": 0.9376,
      "grad_norm": 1.1330403089523315,
      "learning_rate": 0.0001382627345844504,
      "loss": 0.8488,
      "step": 5860
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.8837218284606934,
      "learning_rate": 0.00013815549597855228,
      "loss": 0.8631,
      "step": 5870
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.8773149251937866,
      "learning_rate": 0.00013804825737265416,
      "loss": 0.8525,
      "step": 5880
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.9198513031005859,
      "learning_rate": 0.00013794101876675604,
      "loss": 0.8709,
      "step": 5890
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.914240837097168,
      "learning_rate": 0.0001378337801608579,
      "loss": 0.8354,
      "step": 5900
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.6590993404388428,
      "learning_rate": 0.00013772654155495977,
      "loss": 0.8064,
      "step": 5910
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.6952301263809204,
      "learning_rate": 0.00013761930294906165,
      "loss": 0.8631,
      "step": 5920
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.6436670422554016,
      "learning_rate": 0.00013751206434316354,
      "loss": 0.8781,
      "step": 5930
    },
    {
      "epoch": 0.9504,
      "grad_norm": 1.2145657539367676,
      "learning_rate": 0.00013740482573726542,
      "loss": 0.9545,
      "step": 5940
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.680648922920227,
      "learning_rate": 0.0001372975871313673,
      "loss": 0.8584,
      "step": 5950
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.7589607834815979,
      "learning_rate": 0.00013719034852546918,
      "loss": 0.9641,
      "step": 5960
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.6993913650512695,
      "learning_rate": 0.00013708310991957106,
      "loss": 0.8216,
      "step": 5970
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.7528903484344482,
      "learning_rate": 0.00013697587131367294,
      "loss": 0.8558,
      "step": 5980
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.6694440245628357,
      "learning_rate": 0.00013686863270777482,
      "loss": 0.8382,
      "step": 5990
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.0035324096679688,
      "learning_rate": 0.00013676139410187668,
      "loss": 0.9443,
      "step": 6000
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.7859004735946655,
      "learning_rate": 0.00013665415549597856,
      "loss": 0.8641,
      "step": 6010
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.6620575785636902,
      "learning_rate": 0.00013654691689008044,
      "loss": 0.8655,
      "step": 6020
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.7811099290847778,
      "learning_rate": 0.00013643967828418232,
      "loss": 0.9221,
      "step": 6030
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.6707330942153931,
      "learning_rate": 0.0001363324396782842,
      "loss": 0.8489,
      "step": 6040
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.8937121629714966,
      "learning_rate": 0.00013622520107238605,
      "loss": 0.9395,
      "step": 6050
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.8344330191612244,
      "learning_rate": 0.00013611796246648794,
      "loss": 0.8881,
      "step": 6060
    },
    {
      "epoch": 0.9712,
      "grad_norm": 1.0016038417816162,
      "learning_rate": 0.00013601072386058982,
      "loss": 0.8626,
      "step": 6070
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.6877920627593994,
      "learning_rate": 0.0001359034852546917,
      "loss": 0.8524,
      "step": 6080
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.7004625797271729,
      "learning_rate": 0.00013579624664879358,
      "loss": 0.8155,
      "step": 6090
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.6489235758781433,
      "learning_rate": 0.00013568900804289543,
      "loss": 0.8808,
      "step": 6100
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.6361744403839111,
      "learning_rate": 0.0001355817694369973,
      "loss": 0.7895,
      "step": 6110
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.6283714175224304,
      "learning_rate": 0.0001354745308310992,
      "loss": 0.8272,
      "step": 6120
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.7020604014396667,
      "learning_rate": 0.00013536729222520108,
      "loss": 0.9106,
      "step": 6130
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.7216275930404663,
      "learning_rate": 0.00013526005361930296,
      "loss": 0.891,
      "step": 6140
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.7286824584007263,
      "learning_rate": 0.00013515281501340484,
      "loss": 0.9066,
      "step": 6150
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.8751893043518066,
      "learning_rate": 0.0001350455764075067,
      "loss": 0.8949,
      "step": 6160
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.6798855662345886,
      "learning_rate": 0.00013493833780160857,
      "loss": 0.9395,
      "step": 6170
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.672888457775116,
      "learning_rate": 0.00013483109919571045,
      "loss": 0.898,
      "step": 6180
    },
    {
      "epoch": 0.9904,
      "grad_norm": 1.0528968572616577,
      "learning_rate": 0.00013472386058981233,
      "loss": 0.9009,
      "step": 6190
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.038225769996643,
      "learning_rate": 0.00013461662198391422,
      "loss": 0.8823,
      "step": 6200
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.8149604797363281,
      "learning_rate": 0.0001345093833780161,
      "loss": 0.828,
      "step": 6210
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.7040235996246338,
      "learning_rate": 0.00013440214477211798,
      "loss": 0.8946,
      "step": 6220
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.5490280985832214,
      "learning_rate": 0.00013429490616621986,
      "loss": 0.88,
      "step": 6230
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.9085442423820496,
      "learning_rate": 0.00013418766756032174,
      "loss": 0.8997,
      "step": 6240
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.9578904509544373,
      "learning_rate": 0.00013408042895442362,
      "loss": 0.8172,
      "step": 6250
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.8162602186203003,
      "learning_rate": 0.00013397319034852547,
      "loss": 0.8334,
      "step": 6260
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.701837956905365,
      "learning_rate": 0.00013386595174262736,
      "loss": 0.8445,
      "step": 6270
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.7475458979606628,
      "learning_rate": 0.00013375871313672924,
      "loss": 0.8974,
      "step": 6280
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.8270356059074402,
      "learning_rate": 0.00013365147453083112,
      "loss": 0.8919,
      "step": 6290
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.9091625809669495,
      "learning_rate": 0.000133544235924933,
      "loss": 0.7754,
      "step": 6300
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.9381205439567566,
      "learning_rate": 0.00013343699731903485,
      "loss": 0.8811,
      "step": 6310
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.7820879220962524,
      "learning_rate": 0.00013332975871313673,
      "loss": 0.7717,
      "step": 6320
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.7790158987045288,
      "learning_rate": 0.00013322252010723861,
      "loss": 0.7911,
      "step": 6330
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.6784656047821045,
      "learning_rate": 0.0001331152815013405,
      "loss": 0.8939,
      "step": 6340
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.851841151714325,
      "learning_rate": 0.00013300804289544238,
      "loss": 0.8944,
      "step": 6350
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.6462576985359192,
      "learning_rate": 0.00013290080428954423,
      "loss": 0.8328,
      "step": 6360
    },
    {
      "epoch": 1.0192,
      "grad_norm": 1.1000922918319702,
      "learning_rate": 0.0001327935656836461,
      "loss": 0.7592,
      "step": 6370
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.8280349373817444,
      "learning_rate": 0.000132686327077748,
      "loss": 0.8729,
      "step": 6380
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.6885379552841187,
      "learning_rate": 0.00013257908847184987,
      "loss": 0.8683,
      "step": 6390
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.6449106931686401,
      "learning_rate": 0.00013247184986595176,
      "loss": 0.7817,
      "step": 6400
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.883628785610199,
      "learning_rate": 0.0001323646112600536,
      "loss": 0.8311,
      "step": 6410
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.6391549110412598,
      "learning_rate": 0.0001322573726541555,
      "loss": 0.8608,
      "step": 6420
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.9004454612731934,
      "learning_rate": 0.00013215013404825737,
      "loss": 0.8376,
      "step": 6430
    },
    {
      "epoch": 1.0304,
      "grad_norm": 1.2369084358215332,
      "learning_rate": 0.00013204289544235925,
      "loss": 0.816,
      "step": 6440
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.788382351398468,
      "learning_rate": 0.00013193565683646113,
      "loss": 0.8404,
      "step": 6450
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.7206534743309021,
      "learning_rate": 0.000131828418230563,
      "loss": 0.8505,
      "step": 6460
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.7604343891143799,
      "learning_rate": 0.00013172117962466487,
      "loss": 0.8762,
      "step": 6470
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.7298634648323059,
      "learning_rate": 0.00013161394101876675,
      "loss": 0.8562,
      "step": 6480
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.8029415011405945,
      "learning_rate": 0.00013150670241286863,
      "loss": 0.8732,
      "step": 6490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.9091137051582336,
      "learning_rate": 0.0001313994638069705,
      "loss": 0.9286,
      "step": 6500
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.6284102201461792,
      "learning_rate": 0.0001312922252010724,
      "loss": 0.8569,
      "step": 6510
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.6720962524414062,
      "learning_rate": 0.00013118498659517427,
      "loss": 0.7512,
      "step": 6520
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.6658827066421509,
      "learning_rate": 0.00013107774798927615,
      "loss": 0.8166,
      "step": 6530
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.7294280529022217,
      "learning_rate": 0.00013097050938337804,
      "loss": 0.9109,
      "step": 6540
    },
    {
      "epoch": 1.048,
      "grad_norm": 1.3563833236694336,
      "learning_rate": 0.00013086327077747992,
      "loss": 0.7904,
      "step": 6550
    },
    {
      "epoch": 1.0496,
      "grad_norm": 1.0766116380691528,
      "learning_rate": 0.00013075603217158177,
      "loss": 0.7903,
      "step": 6560
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.685602068901062,
      "learning_rate": 0.00013064879356568365,
      "loss": 0.8624,
      "step": 6570
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.7759314775466919,
      "learning_rate": 0.00013054155495978553,
      "loss": 0.8615,
      "step": 6580
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.7532520294189453,
      "learning_rate": 0.00013043431635388741,
      "loss": 0.7671,
      "step": 6590
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.7525158524513245,
      "learning_rate": 0.0001303270777479893,
      "loss": 0.8985,
      "step": 6600
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.6593125462532043,
      "learning_rate": 0.00013021983914209115,
      "loss": 0.8297,
      "step": 6610
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.8335121870040894,
      "learning_rate": 0.00013011260053619303,
      "loss": 0.8638,
      "step": 6620
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.9442175030708313,
      "learning_rate": 0.0001300053619302949,
      "loss": 0.8643,
      "step": 6630
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.8488003611564636,
      "learning_rate": 0.0001298981233243968,
      "loss": 0.8031,
      "step": 6640
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.7199049592018127,
      "learning_rate": 0.00012979088471849867,
      "loss": 0.8477,
      "step": 6650
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.6602533459663391,
      "learning_rate": 0.00012968364611260053,
      "loss": 0.8747,
      "step": 6660
    },
    {
      "epoch": 1.0672,
      "grad_norm": 1.5889478921890259,
      "learning_rate": 0.0001295764075067024,
      "loss": 0.9076,
      "step": 6670
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.6624952554702759,
      "learning_rate": 0.0001294691689008043,
      "loss": 0.8204,
      "step": 6680
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.7729827761650085,
      "learning_rate": 0.00012936193029490617,
      "loss": 0.8636,
      "step": 6690
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.8333044052124023,
      "learning_rate": 0.00012926541554959787,
      "loss": 0.8867,
      "step": 6700
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.8796053528785706,
      "learning_rate": 0.00012915817694369975,
      "loss": 0.8764,
      "step": 6710
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.8685014247894287,
      "learning_rate": 0.00012905093833780163,
      "loss": 0.9241,
      "step": 6720
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.7861841320991516,
      "learning_rate": 0.0001289436997319035,
      "loss": 0.8083,
      "step": 6730
    },
    {
      "epoch": 1.0784,
      "grad_norm": 1.087423324584961,
      "learning_rate": 0.00012883646112600537,
      "loss": 0.8059,
      "step": 6740
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.290298342704773,
      "learning_rate": 0.00012872922252010725,
      "loss": 0.8545,
      "step": 6750
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.7973371744155884,
      "learning_rate": 0.00012862198391420913,
      "loss": 0.8754,
      "step": 6760
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.7732090950012207,
      "learning_rate": 0.000128514745308311,
      "loss": 0.8855,
      "step": 6770
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.6226170659065247,
      "learning_rate": 0.0001284075067024129,
      "loss": 0.8813,
      "step": 6780
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.9977923631668091,
      "learning_rate": 0.00012830026809651474,
      "loss": 0.7826,
      "step": 6790
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.8184403777122498,
      "learning_rate": 0.00012819302949061662,
      "loss": 0.903,
      "step": 6800
    },
    {
      "epoch": 1.0896,
      "grad_norm": 1.3738908767700195,
      "learning_rate": 0.0001280857908847185,
      "loss": 0.8403,
      "step": 6810
    },
    {
      "epoch": 1.0912,
      "grad_norm": 1.1200122833251953,
      "learning_rate": 0.0001279785522788204,
      "loss": 0.8461,
      "step": 6820
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.7386561632156372,
      "learning_rate": 0.00012787131367292227,
      "loss": 0.8238,
      "step": 6830
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.7797644734382629,
      "learning_rate": 0.00012776407506702412,
      "loss": 0.8598,
      "step": 6840
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.0585658550262451,
      "learning_rate": 0.000127656836461126,
      "loss": 0.786,
      "step": 6850
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.8921019434928894,
      "learning_rate": 0.00012754959785522788,
      "loss": 0.8941,
      "step": 6860
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.727715015411377,
      "learning_rate": 0.00012744235924932976,
      "loss": 0.8048,
      "step": 6870
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.676277756690979,
      "learning_rate": 0.00012733512064343165,
      "loss": 0.8658,
      "step": 6880
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.5623226165771484,
      "learning_rate": 0.0001272278820375335,
      "loss": 0.8537,
      "step": 6890
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.7048377394676208,
      "learning_rate": 0.00012712064343163538,
      "loss": 0.8845,
      "step": 6900
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.7784249782562256,
      "learning_rate": 0.00012701340482573726,
      "loss": 0.8018,
      "step": 6910
    },
    {
      "epoch": 1.1072,
      "grad_norm": 1.0713261365890503,
      "learning_rate": 0.00012690616621983914,
      "loss": 0.9025,
      "step": 6920
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.6879510283470154,
      "learning_rate": 0.00012679892761394102,
      "loss": 0.8435,
      "step": 6930
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.7956366539001465,
      "learning_rate": 0.00012669168900804288,
      "loss": 0.7693,
      "step": 6940
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.9688707590103149,
      "learning_rate": 0.00012658445040214476,
      "loss": 0.8455,
      "step": 6950
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.7094689011573792,
      "learning_rate": 0.00012647721179624664,
      "loss": 0.8288,
      "step": 6960
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.7948178052902222,
      "learning_rate": 0.00012636997319034852,
      "loss": 0.8403,
      "step": 6970
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.7700247764587402,
      "learning_rate": 0.0001262627345844504,
      "loss": 0.8741,
      "step": 6980
    },
    {
      "epoch": 1.1184,
      "grad_norm": 1.1550266742706299,
      "learning_rate": 0.00012615549597855228,
      "loss": 0.8564,
      "step": 6990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.7861050963401794,
      "learning_rate": 0.00012604825737265416,
      "loss": 0.8137,
      "step": 7000
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.6330599188804626,
      "learning_rate": 0.00012594101876675605,
      "loss": 0.8533,
      "step": 7010
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.8298109173774719,
      "learning_rate": 0.00012583378016085793,
      "loss": 0.9236,
      "step": 7020
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.6327009201049805,
      "learning_rate": 0.0001257265415549598,
      "loss": 0.8211,
      "step": 7030
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.6554884314537048,
      "learning_rate": 0.0001256193029490617,
      "loss": 0.8133,
      "step": 7040
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 1.006696105003357,
      "learning_rate": 0.00012551206434316354,
      "loss": 0.825,
      "step": 7050
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.6611584424972534,
      "learning_rate": 0.00012540482573726542,
      "loss": 0.7412,
      "step": 7060
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.6904637813568115,
      "learning_rate": 0.0001252975871313673,
      "loss": 0.8812,
      "step": 7070
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.8809226155281067,
      "learning_rate": 0.00012519034852546919,
      "loss": 0.8329,
      "step": 7080
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.6902014017105103,
      "learning_rate": 0.00012508310991957107,
      "loss": 0.7823,
      "step": 7090
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.7579082250595093,
      "learning_rate": 0.00012497587131367292,
      "loss": 0.8759,
      "step": 7100
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.7271606922149658,
      "learning_rate": 0.0001248686327077748,
      "loss": 0.862,
      "step": 7110
    },
    {
      "epoch": 1.1392,
      "grad_norm": 1.146973967552185,
      "learning_rate": 0.00012476139410187668,
      "loss": 0.8913,
      "step": 7120
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.7814152240753174,
      "learning_rate": 0.00012465415549597856,
      "loss": 0.8389,
      "step": 7130
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.8763638734817505,
      "learning_rate": 0.00012454691689008044,
      "loss": 0.8658,
      "step": 7140
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.9465649724006653,
      "learning_rate": 0.0001244396782841823,
      "loss": 0.9103,
      "step": 7150
    },
    {
      "epoch": 1.1456,
      "grad_norm": 1.1111406087875366,
      "learning_rate": 0.00012433243967828418,
      "loss": 0.827,
      "step": 7160
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.7526552677154541,
      "learning_rate": 0.00012422520107238606,
      "loss": 0.8184,
      "step": 7170
    },
    {
      "epoch": 1.1488,
      "grad_norm": 1.2333199977874756,
      "learning_rate": 0.00012411796246648794,
      "loss": 0.8837,
      "step": 7180
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.7654649019241333,
      "learning_rate": 0.00012401072386058982,
      "loss": 0.8262,
      "step": 7190
    },
    {
      "epoch": 1.152,
      "grad_norm": 1.0132273435592651,
      "learning_rate": 0.00012390348525469168,
      "loss": 0.8703,
      "step": 7200
    },
    {
      "epoch": 1.1536,
      "grad_norm": 1.2134859561920166,
      "learning_rate": 0.00012379624664879356,
      "loss": 0.7964,
      "step": 7210
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.7966977953910828,
      "learning_rate": 0.00012368900804289544,
      "loss": 0.8745,
      "step": 7220
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.8359336256980896,
      "learning_rate": 0.00012358176943699732,
      "loss": 0.8863,
      "step": 7230
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.9680129885673523,
      "learning_rate": 0.0001234745308310992,
      "loss": 0.8437,
      "step": 7240
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.6814773082733154,
      "learning_rate": 0.00012336729222520108,
      "loss": 0.8854,
      "step": 7250
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.7891114950180054,
      "learning_rate": 0.00012326005361930296,
      "loss": 0.7608,
      "step": 7260
    },
    {
      "epoch": 1.1632,
      "grad_norm": 1.3735368251800537,
      "learning_rate": 0.00012315281501340484,
      "loss": 0.8572,
      "step": 7270
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.6376801133155823,
      "learning_rate": 0.00012304557640750673,
      "loss": 0.8718,
      "step": 7280
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.7617769837379456,
      "learning_rate": 0.0001229383378016086,
      "loss": 0.8356,
      "step": 7290
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.7129773497581482,
      "learning_rate": 0.00012283109919571046,
      "loss": 0.8371,
      "step": 7300
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.8285951614379883,
      "learning_rate": 0.00012272386058981234,
      "loss": 0.8954,
      "step": 7310
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.7066611051559448,
      "learning_rate": 0.00012261662198391422,
      "loss": 0.8322,
      "step": 7320
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.732258677482605,
      "learning_rate": 0.0001225093833780161,
      "loss": 0.8809,
      "step": 7330
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.8058614134788513,
      "learning_rate": 0.00012240214477211798,
      "loss": 0.7997,
      "step": 7340
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.6901993155479431,
      "learning_rate": 0.00012229490616621984,
      "loss": 0.8318,
      "step": 7350
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.765743613243103,
      "learning_rate": 0.00012218766756032172,
      "loss": 0.8238,
      "step": 7360
    },
    {
      "epoch": 1.1792,
      "grad_norm": 1.138466238975525,
      "learning_rate": 0.0001220804289544236,
      "loss": 0.9967,
      "step": 7370
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.5716986060142517,
      "learning_rate": 0.00012197319034852548,
      "loss": 0.8875,
      "step": 7380
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.7190970778465271,
      "learning_rate": 0.00012186595174262736,
      "loss": 0.8791,
      "step": 7390
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.6683346033096313,
      "learning_rate": 0.00012175871313672922,
      "loss": 0.9233,
      "step": 7400
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.7511026263237,
      "learning_rate": 0.0001216514745308311,
      "loss": 0.8629,
      "step": 7410
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.7489085793495178,
      "learning_rate": 0.00012154423592493298,
      "loss": 0.8266,
      "step": 7420
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.8452939391136169,
      "learning_rate": 0.00012143699731903486,
      "loss": 0.8915,
      "step": 7430
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.7243160605430603,
      "learning_rate": 0.00012132975871313674,
      "loss": 0.8665,
      "step": 7440
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.717784583568573,
      "learning_rate": 0.00012122252010723861,
      "loss": 0.8306,
      "step": 7450
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.9671859741210938,
      "learning_rate": 0.00012111528150134049,
      "loss": 0.8248,
      "step": 7460
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.7566909790039062,
      "learning_rate": 0.00012100804289544237,
      "loss": 0.8231,
      "step": 7470
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.6900991201400757,
      "learning_rate": 0.00012090080428954425,
      "loss": 0.78,
      "step": 7480
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.6164857745170593,
      "learning_rate": 0.00012079356568364613,
      "loss": 0.8197,
      "step": 7490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.8322363495826721,
      "learning_rate": 0.00012068632707774799,
      "loss": 0.7891,
      "step": 7500
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.5999985933303833,
      "learning_rate": 0.00012057908847184987,
      "loss": 0.8045,
      "step": 7510
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.9694750308990479,
      "learning_rate": 0.00012047184986595175,
      "loss": 0.92,
      "step": 7520
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.7581349611282349,
      "learning_rate": 0.00012036461126005363,
      "loss": 0.9239,
      "step": 7530
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.8029752969741821,
      "learning_rate": 0.00012025737265415551,
      "loss": 0.873,
      "step": 7540
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.8404247760772705,
      "learning_rate": 0.00012015013404825736,
      "loss": 0.9125,
      "step": 7550
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.810469388961792,
      "learning_rate": 0.00012004289544235925,
      "loss": 0.8075,
      "step": 7560
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.781414270401001,
      "learning_rate": 0.00011993565683646113,
      "loss": 0.8402,
      "step": 7570
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.7678549885749817,
      "learning_rate": 0.00011982841823056301,
      "loss": 0.9057,
      "step": 7580
    },
    {
      "epoch": 1.2144,
      "grad_norm": 1.2338696718215942,
      "learning_rate": 0.00011972117962466489,
      "loss": 0.8692,
      "step": 7590
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.6180031895637512,
      "learning_rate": 0.00011961394101876676,
      "loss": 0.8194,
      "step": 7600
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.7373207211494446,
      "learning_rate": 0.00011950670241286864,
      "loss": 0.8595,
      "step": 7610
    },
    {
      "epoch": 1.2192,
      "grad_norm": 1.3689463138580322,
      "learning_rate": 0.00011939946380697052,
      "loss": 0.8123,
      "step": 7620
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.6808505058288574,
      "learning_rate": 0.0001192922252010724,
      "loss": 0.8948,
      "step": 7630
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.8183978199958801,
      "learning_rate": 0.00011918498659517428,
      "loss": 0.8939,
      "step": 7640
    },
    {
      "epoch": 1.224,
      "grad_norm": 1.1396633386611938,
      "learning_rate": 0.00011907774798927616,
      "loss": 0.8118,
      "step": 7650
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.6161453723907471,
      "learning_rate": 0.00011897050938337801,
      "loss": 0.7903,
      "step": 7660
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.6852953433990479,
      "learning_rate": 0.0001188632707774799,
      "loss": 0.8517,
      "step": 7670
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.7436910271644592,
      "learning_rate": 0.00011875603217158178,
      "loss": 0.8323,
      "step": 7680
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.9363355040550232,
      "learning_rate": 0.00011864879356568366,
      "loss": 0.804,
      "step": 7690
    },
    {
      "epoch": 1.232,
      "grad_norm": 1.138708472251892,
      "learning_rate": 0.00011854155495978554,
      "loss": 0.831,
      "step": 7700
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.7555583119392395,
      "learning_rate": 0.00011843431635388739,
      "loss": 0.914,
      "step": 7710
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.806606650352478,
      "learning_rate": 0.00011832707774798927,
      "loss": 0.8053,
      "step": 7720
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.958950400352478,
      "learning_rate": 0.00011821983914209116,
      "loss": 0.8312,
      "step": 7730
    },
    {
      "epoch": 1.2384,
      "grad_norm": 1.1532362699508667,
      "learning_rate": 0.00011811260053619304,
      "loss": 0.827,
      "step": 7740
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.7714444398880005,
      "learning_rate": 0.00011800536193029492,
      "loss": 0.9059,
      "step": 7750
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.7865336537361145,
      "learning_rate": 0.00011789812332439678,
      "loss": 0.8155,
      "step": 7760
    },
    {
      "epoch": 1.2432,
      "grad_norm": 1.026720643043518,
      "learning_rate": 0.00011779088471849867,
      "loss": 0.8465,
      "step": 7770
    },
    {
      "epoch": 1.2448,
      "grad_norm": 1.0494850873947144,
      "learning_rate": 0.00011768364611260055,
      "loss": 0.8983,
      "step": 7780
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.8067861795425415,
      "learning_rate": 0.00011757640750670243,
      "loss": 0.8035,
      "step": 7790
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.6828976273536682,
      "learning_rate": 0.00011746916890080431,
      "loss": 0.8018,
      "step": 7800
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.8115666508674622,
      "learning_rate": 0.00011736193029490616,
      "loss": 0.7854,
      "step": 7810
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.8623276352882385,
      "learning_rate": 0.00011725469168900804,
      "loss": 0.8219,
      "step": 7820
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.8502383828163147,
      "learning_rate": 0.00011714745308310992,
      "loss": 0.9582,
      "step": 7830
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.8226356506347656,
      "learning_rate": 0.0001170402144772118,
      "loss": 0.8841,
      "step": 7840
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.0820388793945312,
      "learning_rate": 0.00011693297587131369,
      "loss": 0.8651,
      "step": 7850
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.8738517165184021,
      "learning_rate": 0.00011682573726541554,
      "loss": 0.8652,
      "step": 7860
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.7510553598403931,
      "learning_rate": 0.00011671849865951742,
      "loss": 0.8739,
      "step": 7870
    },
    {
      "epoch": 1.2608,
      "grad_norm": 1.058408498764038,
      "learning_rate": 0.0001166112600536193,
      "loss": 0.8307,
      "step": 7880
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.7454684972763062,
      "learning_rate": 0.00011650402144772118,
      "loss": 0.8087,
      "step": 7890
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.5000860691070557,
      "learning_rate": 0.00011639678284182307,
      "loss": 0.8099,
      "step": 7900
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.8130568861961365,
      "learning_rate": 0.00011628954423592493,
      "loss": 0.8935,
      "step": 7910
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.7660238146781921,
      "learning_rate": 0.00011618230563002681,
      "loss": 0.7507,
      "step": 7920
    },
    {
      "epoch": 1.2688,
      "grad_norm": 1.3827930688858032,
      "learning_rate": 0.0001160750670241287,
      "loss": 0.886,
      "step": 7930
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.8016741871833801,
      "learning_rate": 0.00011596782841823058,
      "loss": 0.8814,
      "step": 7940
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.9932816028594971,
      "learning_rate": 0.00011586058981233246,
      "loss": 0.8639,
      "step": 7950
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.8079578280448914,
      "learning_rate": 0.00011575335120643431,
      "loss": 0.8915,
      "step": 7960
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.6158080101013184,
      "learning_rate": 0.00011564611260053619,
      "loss": 0.8371,
      "step": 7970
    },
    {
      "epoch": 1.2768,
      "grad_norm": 1.0179022550582886,
      "learning_rate": 0.00011553887399463807,
      "loss": 0.8713,
      "step": 7980
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.7209261655807495,
      "learning_rate": 0.00011543163538873995,
      "loss": 0.8206,
      "step": 7990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.7196680307388306,
      "learning_rate": 0.00011532439678284184,
      "loss": 0.9132,
      "step": 8000
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.939730167388916,
      "learning_rate": 0.0001152171581769437,
      "loss": 0.8316,
      "step": 8010
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.7687053084373474,
      "learning_rate": 0.00011510991957104558,
      "loss": 0.9081,
      "step": 8020
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.6317501068115234,
      "learning_rate": 0.00011500268096514746,
      "loss": 0.8327,
      "step": 8030
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.9522499442100525,
      "learning_rate": 0.00011489544235924935,
      "loss": 0.8702,
      "step": 8040
    },
    {
      "epoch": 1.288,
      "grad_norm": 1.0338014364242554,
      "learning_rate": 0.00011478820375335123,
      "loss": 0.7849,
      "step": 8050
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.7549642324447632,
      "learning_rate": 0.00011468096514745308,
      "loss": 0.7518,
      "step": 8060
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.5865004062652588,
      "learning_rate": 0.00011457372654155496,
      "loss": 0.8128,
      "step": 8070
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.7370739579200745,
      "learning_rate": 0.00011446648793565684,
      "loss": 0.8323,
      "step": 8080
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.8210893869400024,
      "learning_rate": 0.00011435924932975872,
      "loss": 0.8608,
      "step": 8090
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.0142992734909058,
      "learning_rate": 0.0001142520107238606,
      "loss": 0.8933,
      "step": 8100
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.787452220916748,
      "learning_rate": 0.00011414477211796246,
      "loss": 0.9068,
      "step": 8110
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.7022588849067688,
      "learning_rate": 0.00011403753351206434,
      "loss": 0.8161,
      "step": 8120
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.5636700987815857,
      "learning_rate": 0.00011393029490616622,
      "loss": 0.8354,
      "step": 8130
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.9188884496688843,
      "learning_rate": 0.0001138230563002681,
      "loss": 0.8523,
      "step": 8140
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.6978759169578552,
      "learning_rate": 0.00011371581769436998,
      "loss": 0.8809,
      "step": 8150
    },
    {
      "epoch": 1.3056,
      "grad_norm": 1.0146933794021606,
      "learning_rate": 0.00011360857908847185,
      "loss": 0.8843,
      "step": 8160
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.7103495001792908,
      "learning_rate": 0.00011350134048257373,
      "loss": 0.9403,
      "step": 8170
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.6215620636940002,
      "learning_rate": 0.00011339410187667561,
      "loss": 0.83,
      "step": 8180
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.8435578942298889,
      "learning_rate": 0.0001132868632707775,
      "loss": 0.8674,
      "step": 8190
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.9899494647979736,
      "learning_rate": 0.00011317962466487937,
      "loss": 0.8777,
      "step": 8200
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.7744184732437134,
      "learning_rate": 0.00011307238605898123,
      "loss": 0.8685,
      "step": 8210
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.7903131246566772,
      "learning_rate": 0.00011296514745308311,
      "loss": 0.8888,
      "step": 8220
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.9232704639434814,
      "learning_rate": 0.00011285790884718499,
      "loss": 0.8076,
      "step": 8230
    },
    {
      "epoch": 1.3184,
      "grad_norm": 1.3566744327545166,
      "learning_rate": 0.00011275067024128687,
      "loss": 0.7484,
      "step": 8240
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.8344554305076599,
      "learning_rate": 0.00011264343163538875,
      "loss": 0.8126,
      "step": 8250
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.8125115036964417,
      "learning_rate": 0.00011253619302949063,
      "loss": 0.887,
      "step": 8260
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.6903302073478699,
      "learning_rate": 0.00011242895442359249,
      "loss": 0.8951,
      "step": 8270
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.6720332503318787,
      "learning_rate": 0.00011232171581769437,
      "loss": 0.8327,
      "step": 8280
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.846854567527771,
      "learning_rate": 0.00011221447721179625,
      "loss": 0.8886,
      "step": 8290
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.6650847792625427,
      "learning_rate": 0.00011210723860589813,
      "loss": 0.8445,
      "step": 8300
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.7759448885917664,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.8513,
      "step": 8310
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.8178196549415588,
      "learning_rate": 0.00011189276139410188,
      "loss": 0.8595,
      "step": 8320
    },
    {
      "epoch": 1.3328,
      "grad_norm": 1.075750470161438,
      "learning_rate": 0.00011178552278820376,
      "loss": 0.8995,
      "step": 8330
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.8923493027687073,
      "learning_rate": 0.00011167828418230564,
      "loss": 0.8061,
      "step": 8340
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.7944667339324951,
      "learning_rate": 0.00011157104557640752,
      "loss": 0.873,
      "step": 8350
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.67420494556427,
      "learning_rate": 0.0001114638069705094,
      "loss": 0.856,
      "step": 8360
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.8524527549743652,
      "learning_rate": 0.00011135656836461126,
      "loss": 0.8204,
      "step": 8370
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.6256852746009827,
      "learning_rate": 0.00011124932975871314,
      "loss": 0.875,
      "step": 8380
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.931300938129425,
      "learning_rate": 0.00011114209115281502,
      "loss": 0.8785,
      "step": 8390
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.7536954879760742,
      "learning_rate": 0.0001110348525469169,
      "loss": 0.8473,
      "step": 8400
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 1.0778263807296753,
      "learning_rate": 0.00011092761394101878,
      "loss": 0.8593,
      "step": 8410
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.7514716386795044,
      "learning_rate": 0.00011082037533512064,
      "loss": 0.8762,
      "step": 8420
    },
    {
      "epoch": 1.3488,
      "grad_norm": 1.107401967048645,
      "learning_rate": 0.00011071313672922252,
      "loss": 0.7985,
      "step": 8430
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.6865145564079285,
      "learning_rate": 0.0001106058981233244,
      "loss": 0.9099,
      "step": 8440
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.774885356426239,
      "learning_rate": 0.00011049865951742628,
      "loss": 0.8647,
      "step": 8450
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.8721538186073303,
      "learning_rate": 0.00011039142091152816,
      "loss": 0.872,
      "step": 8460
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.6302286982536316,
      "learning_rate": 0.00011028418230563003,
      "loss": 0.8464,
      "step": 8470
    },
    {
      "epoch": 1.3568,
      "grad_norm": 1.0210870504379272,
      "learning_rate": 0.00011017694369973191,
      "loss": 0.7776,
      "step": 8480
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.7188956141471863,
      "learning_rate": 0.00011006970509383379,
      "loss": 0.882,
      "step": 8490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.827140748500824,
      "learning_rate": 0.00010996246648793567,
      "loss": 0.8305,
      "step": 8500
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.8587356209754944,
      "learning_rate": 0.00010985522788203755,
      "loss": 0.8333,
      "step": 8510
    },
    {
      "epoch": 1.3632,
      "grad_norm": 1.1533992290496826,
      "learning_rate": 0.0001097479892761394,
      "loss": 0.9437,
      "step": 8520
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.6979422569274902,
      "learning_rate": 0.00010964075067024129,
      "loss": 0.8344,
      "step": 8530
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.7505847215652466,
      "learning_rate": 0.00010953351206434317,
      "loss": 0.8592,
      "step": 8540
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.9648078680038452,
      "learning_rate": 0.00010942627345844505,
      "loss": 0.8589,
      "step": 8550
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.7713952660560608,
      "learning_rate": 0.00010931903485254693,
      "loss": 0.868,
      "step": 8560
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.7681396007537842,
      "learning_rate": 0.00010921179624664878,
      "loss": 0.8173,
      "step": 8570
    },
    {
      "epoch": 1.3728,
      "grad_norm": 1.0274291038513184,
      "learning_rate": 0.00010910455764075066,
      "loss": 0.8151,
      "step": 8580
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.8136070966720581,
      "learning_rate": 0.00010899731903485255,
      "loss": 0.9091,
      "step": 8590
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.6846120357513428,
      "learning_rate": 0.00010889008042895443,
      "loss": 0.798,
      "step": 8600
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.8589798212051392,
      "learning_rate": 0.00010878284182305631,
      "loss": 0.8373,
      "step": 8610
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.7022691965103149,
      "learning_rate": 0.00010867560321715818,
      "loss": 0.8125,
      "step": 8620
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.8511582612991333,
      "learning_rate": 0.00010856836461126006,
      "loss": 0.7836,
      "step": 8630
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.6368065476417542,
      "learning_rate": 0.00010846112600536194,
      "loss": 0.8288,
      "step": 8640
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.8141722083091736,
      "learning_rate": 0.00010835388739946382,
      "loss": 0.86,
      "step": 8650
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.9630683660507202,
      "learning_rate": 0.0001082466487935657,
      "loss": 0.799,
      "step": 8660
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.894921600818634,
      "learning_rate": 0.00010813941018766755,
      "loss": 0.8255,
      "step": 8670
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.7417926788330078,
      "learning_rate": 0.00010803217158176943,
      "loss": 0.8087,
      "step": 8680
    },
    {
      "epoch": 1.3904,
      "grad_norm": 1.178440809249878,
      "learning_rate": 0.00010792493297587132,
      "loss": 0.8915,
      "step": 8690
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.780384361743927,
      "learning_rate": 0.0001078176943699732,
      "loss": 0.926,
      "step": 8700
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.7490667104721069,
      "learning_rate": 0.00010771045576407508,
      "loss": 0.8582,
      "step": 8710
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.8848637342453003,
      "learning_rate": 0.00010760321715817694,
      "loss": 0.8108,
      "step": 8720
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.646701991558075,
      "learning_rate": 0.00010749597855227883,
      "loss": 0.7772,
      "step": 8730
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.6618716716766357,
      "learning_rate": 0.00010738873994638071,
      "loss": 0.8127,
      "step": 8740
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.7668470740318298,
      "learning_rate": 0.00010728150134048259,
      "loss": 0.8329,
      "step": 8750
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.847712516784668,
      "learning_rate": 0.00010717426273458447,
      "loss": 0.8557,
      "step": 8760
    },
    {
      "epoch": 1.4032,
      "grad_norm": 1.1248875856399536,
      "learning_rate": 0.00010706702412868632,
      "loss": 0.8462,
      "step": 8770
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.7464715838432312,
      "learning_rate": 0.0001069597855227882,
      "loss": 0.7847,
      "step": 8780
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.9378640055656433,
      "learning_rate": 0.00010685254691689009,
      "loss": 0.8593,
      "step": 8790
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.90317302942276,
      "learning_rate": 0.00010674530831099197,
      "loss": 0.8274,
      "step": 8800
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.8234524726867676,
      "learning_rate": 0.00010663806970509385,
      "loss": 0.8632,
      "step": 8810
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.6990185379981995,
      "learning_rate": 0.0001065308310991957,
      "loss": 0.7956,
      "step": 8820
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.9084164500236511,
      "learning_rate": 0.00010642359249329758,
      "loss": 0.8216,
      "step": 8830
    },
    {
      "epoch": 1.4144,
      "grad_norm": 1.0243891477584839,
      "learning_rate": 0.00010631635388739946,
      "loss": 0.8277,
      "step": 8840
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.1247206926345825,
      "learning_rate": 0.00010620911528150134,
      "loss": 0.8489,
      "step": 8850
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.7960952520370483,
      "learning_rate": 0.00010610187667560323,
      "loss": 0.7706,
      "step": 8860
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.8037925958633423,
      "learning_rate": 0.0001059946380697051,
      "loss": 0.8716,
      "step": 8870
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.7939996719360352,
      "learning_rate": 0.00010588739946380697,
      "loss": 0.8604,
      "step": 8880
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.8180233240127563,
      "learning_rate": 0.00010578016085790885,
      "loss": 0.8695,
      "step": 8890
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.66847825050354,
      "learning_rate": 0.00010567292225201074,
      "loss": 0.9105,
      "step": 8900
    },
    {
      "epoch": 1.4256,
      "grad_norm": 1.140098214149475,
      "learning_rate": 0.00010556568364611262,
      "loss": 0.8424,
      "step": 8910
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.7699580788612366,
      "learning_rate": 0.0001054584450402145,
      "loss": 0.8153,
      "step": 8920
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.8242067694664001,
      "learning_rate": 0.00010535120643431635,
      "loss": 0.8593,
      "step": 8930
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.7146673798561096,
      "learning_rate": 0.00010524396782841823,
      "loss": 0.8063,
      "step": 8940
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.7972414493560791,
      "learning_rate": 0.00010513672922252011,
      "loss": 0.9365,
      "step": 8950
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.6722739934921265,
      "learning_rate": 0.000105029490616622,
      "loss": 0.7594,
      "step": 8960
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.7608432769775391,
      "learning_rate": 0.00010492225201072388,
      "loss": 0.8695,
      "step": 8970
    },
    {
      "epoch": 1.4368,
      "grad_norm": 1.0022213459014893,
      "learning_rate": 0.00010481501340482573,
      "loss": 0.9281,
      "step": 8980
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.9435687065124512,
      "learning_rate": 0.00010470777479892761,
      "loss": 0.8968,
      "step": 8990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.827914297580719,
      "learning_rate": 0.00010460053619302949,
      "loss": 0.8532,
      "step": 9000
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.9281638264656067,
      "learning_rate": 0.00010449329758713137,
      "loss": 0.8337,
      "step": 9010
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.7698606252670288,
      "learning_rate": 0.00010438605898123325,
      "loss": 0.8555,
      "step": 9020
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 1.019506573677063,
      "learning_rate": 0.00010427882037533512,
      "loss": 0.9476,
      "step": 9030
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.83238285779953,
      "learning_rate": 0.000104171581769437,
      "loss": 0.8294,
      "step": 9040
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.6488916873931885,
      "learning_rate": 0.00010406434316353888,
      "loss": 0.8418,
      "step": 9050
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.7640004754066467,
      "learning_rate": 0.00010395710455764076,
      "loss": 0.8276,
      "step": 9060
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.8533654808998108,
      "learning_rate": 0.00010384986595174265,
      "loss": 0.9129,
      "step": 9070
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.6455601453781128,
      "learning_rate": 0.0001037426273458445,
      "loss": 0.8427,
      "step": 9080
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.8722265958786011,
      "learning_rate": 0.00010363538873994638,
      "loss": 0.7467,
      "step": 9090
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.805388867855072,
      "learning_rate": 0.00010352815013404826,
      "loss": 0.7973,
      "step": 9100
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.7470137476921082,
      "learning_rate": 0.00010342091152815014,
      "loss": 0.8486,
      "step": 9110
    },
    {
      "epoch": 1.4592,
      "grad_norm": 1.3013732433319092,
      "learning_rate": 0.00010331367292225202,
      "loss": 0.8489,
      "step": 9120
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.7683122754096985,
      "learning_rate": 0.00010320643431635388,
      "loss": 0.8693,
      "step": 9130
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.8026972413063049,
      "learning_rate": 0.00010309919571045576,
      "loss": 0.9147,
      "step": 9140
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.7403080463409424,
      "learning_rate": 0.00010299195710455764,
      "loss": 0.8686,
      "step": 9150
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.7424619197845459,
      "learning_rate": 0.00010288471849865952,
      "loss": 0.7773,
      "step": 9160
    },
    {
      "epoch": 1.4672,
      "grad_norm": 1.0539647340774536,
      "learning_rate": 0.0001027774798927614,
      "loss": 0.8674,
      "step": 9170
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.6974393129348755,
      "learning_rate": 0.00010267024128686327,
      "loss": 0.844,
      "step": 9180
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.8544633984565735,
      "learning_rate": 0.00010256300268096515,
      "loss": 0.8372,
      "step": 9190
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.0696418285369873,
      "learning_rate": 0.00010245576407506703,
      "loss": 0.7797,
      "step": 9200
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.878383457660675,
      "learning_rate": 0.00010234852546916891,
      "loss": 0.8103,
      "step": 9210
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.8097642064094543,
      "learning_rate": 0.0001022412868632708,
      "loss": 0.768,
      "step": 9220
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.726136326789856,
      "learning_rate": 0.00010213404825737265,
      "loss": 0.8597,
      "step": 9230
    },
    {
      "epoch": 1.4784,
      "grad_norm": 1.1142991781234741,
      "learning_rate": 0.00010202680965147453,
      "loss": 0.877,
      "step": 9240
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.8300592303276062,
      "learning_rate": 0.00010191957104557641,
      "loss": 0.7923,
      "step": 9250
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.8809575438499451,
      "learning_rate": 0.00010181233243967829,
      "loss": 0.9116,
      "step": 9260
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.9790599346160889,
      "learning_rate": 0.00010170509383378017,
      "loss": 0.8688,
      "step": 9270
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.6482682228088379,
      "learning_rate": 0.00010159785522788204,
      "loss": 0.8686,
      "step": 9280
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.7208887934684753,
      "learning_rate": 0.00010149061662198392,
      "loss": 0.9529,
      "step": 9290
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.6656303405761719,
      "learning_rate": 0.0001013833780160858,
      "loss": 0.8587,
      "step": 9300
    },
    {
      "epoch": 1.4896,
      "grad_norm": 1.4701755046844482,
      "learning_rate": 0.00010127613941018768,
      "loss": 0.8102,
      "step": 9310
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.9138115048408508,
      "learning_rate": 0.00010116890080428956,
      "loss": 0.8512,
      "step": 9320
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.7833682298660278,
      "learning_rate": 0.00010106166219839142,
      "loss": 0.7991,
      "step": 9330
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.8646272420883179,
      "learning_rate": 0.0001009544235924933,
      "loss": 0.9035,
      "step": 9340
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.7660470604896545,
      "learning_rate": 0.00010084718498659518,
      "loss": 0.8668,
      "step": 9350
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.7657608389854431,
      "learning_rate": 0.00010073994638069706,
      "loss": 0.8452,
      "step": 9360
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.9634708762168884,
      "learning_rate": 0.00010063270777479894,
      "loss": 0.8558,
      "step": 9370
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.6272878050804138,
      "learning_rate": 0.0001005254691689008,
      "loss": 0.8118,
      "step": 9380
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.7710092067718506,
      "learning_rate": 0.00010041823056300268,
      "loss": 0.8411,
      "step": 9390
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.7260866761207581,
      "learning_rate": 0.00010031099195710456,
      "loss": 0.8786,
      "step": 9400
    },
    {
      "epoch": 1.5056,
      "grad_norm": 1.1356652975082397,
      "learning_rate": 0.00010020375335120644,
      "loss": 0.8255,
      "step": 9410
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.7879981398582458,
      "learning_rate": 0.00010009651474530832,
      "loss": 0.8352,
      "step": 9420
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.7889931797981262,
      "learning_rate": 9.998927613941019e-05,
      "loss": 0.7679,
      "step": 9430
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.5746079087257385,
      "learning_rate": 9.988203753351207e-05,
      "loss": 0.8049,
      "step": 9440
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.831603467464447,
      "learning_rate": 9.977479892761395e-05,
      "loss": 0.8414,
      "step": 9450
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.7939062118530273,
      "learning_rate": 9.966756032171583e-05,
      "loss": 0.8614,
      "step": 9460
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.6905663013458252,
      "learning_rate": 9.95603217158177e-05,
      "loss": 0.8971,
      "step": 9470
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.7967296838760376,
      "learning_rate": 9.945308310991958e-05,
      "loss": 0.8039,
      "step": 9480
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.7575479745864868,
      "learning_rate": 9.934584450402146e-05,
      "loss": 0.9162,
      "step": 9490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.922710120677948,
      "learning_rate": 9.923860589812333e-05,
      "loss": 0.8465,
      "step": 9500
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.6570655703544617,
      "learning_rate": 9.913136729222521e-05,
      "loss": 0.8432,
      "step": 9510
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.7011279463768005,
      "learning_rate": 9.902412868632708e-05,
      "loss": 0.8615,
      "step": 9520
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.9303910732269287,
      "learning_rate": 9.891689008042896e-05,
      "loss": 0.8182,
      "step": 9530
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.6306563019752502,
      "learning_rate": 9.880965147453084e-05,
      "loss": 0.7722,
      "step": 9540
    },
    {
      "epoch": 1.528,
      "grad_norm": 1.2915867567062378,
      "learning_rate": 9.87024128686327e-05,
      "loss": 0.8267,
      "step": 9550
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.8499345779418945,
      "learning_rate": 9.859517426273459e-05,
      "loss": 0.7446,
      "step": 9560
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.8304775357246399,
      "learning_rate": 9.848793565683645e-05,
      "loss": 0.8973,
      "step": 9570
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.6521310806274414,
      "learning_rate": 9.838069705093834e-05,
      "loss": 0.8171,
      "step": 9580
    },
    {
      "epoch": 1.5344,
      "grad_norm": 1.0401581525802612,
      "learning_rate": 9.827345844504022e-05,
      "loss": 0.8345,
      "step": 9590
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.9576348662376404,
      "learning_rate": 9.81662198391421e-05,
      "loss": 0.8444,
      "step": 9600
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 1.0270365476608276,
      "learning_rate": 9.805898123324398e-05,
      "loss": 0.812,
      "step": 9610
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 1.5206891298294067,
      "learning_rate": 9.795174262734585e-05,
      "loss": 0.8529,
      "step": 9620
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.7710651159286499,
      "learning_rate": 9.784450402144773e-05,
      "loss": 0.854,
      "step": 9630
    },
    {
      "epoch": 1.5424,
      "grad_norm": 1.208555817604065,
      "learning_rate": 9.773726541554961e-05,
      "loss": 0.8951,
      "step": 9640
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.6676220893859863,
      "learning_rate": 9.763002680965148e-05,
      "loss": 0.846,
      "step": 9650
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.8834171891212463,
      "learning_rate": 9.752278820375336e-05,
      "loss": 0.8729,
      "step": 9660
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.9313750267028809,
      "learning_rate": 9.741554959785524e-05,
      "loss": 0.8583,
      "step": 9670
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.9921720027923584,
      "learning_rate": 9.73083109919571e-05,
      "loss": 0.8321,
      "step": 9680
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.6937475204467773,
      "learning_rate": 9.720107238605899e-05,
      "loss": 0.7966,
      "step": 9690
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.7983928322792053,
      "learning_rate": 9.709383378016085e-05,
      "loss": 0.8397,
      "step": 9700
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 1.7156800031661987,
      "learning_rate": 9.698659517426273e-05,
      "loss": 0.8634,
      "step": 9710
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 1.438923954963684,
      "learning_rate": 9.687935656836462e-05,
      "loss": 0.872,
      "step": 9720
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.6016554236412048,
      "learning_rate": 9.67721179624665e-05,
      "loss": 0.8289,
      "step": 9730
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.6368888020515442,
      "learning_rate": 9.666487935656838e-05,
      "loss": 0.8295,
      "step": 9740
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.7486962080001831,
      "learning_rate": 9.655764075067025e-05,
      "loss": 0.8378,
      "step": 9750
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.6254255771636963,
      "learning_rate": 9.645040214477213e-05,
      "loss": 0.8875,
      "step": 9760
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 1.0732066631317139,
      "learning_rate": 9.634316353887401e-05,
      "loss": 0.888,
      "step": 9770
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.699000895023346,
      "learning_rate": 9.623592493297587e-05,
      "loss": 0.8126,
      "step": 9780
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.5827119946479797,
      "learning_rate": 9.612868632707776e-05,
      "loss": 0.8624,
      "step": 9790
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.6752502918243408,
      "learning_rate": 9.602144772117962e-05,
      "loss": 0.8075,
      "step": 9800
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.7806357741355896,
      "learning_rate": 9.59142091152815e-05,
      "loss": 0.8276,
      "step": 9810
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.9716693758964539,
      "learning_rate": 9.580697050938339e-05,
      "loss": 0.8481,
      "step": 9820
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.695128858089447,
      "learning_rate": 9.569973190348525e-05,
      "loss": 0.8758,
      "step": 9830
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.8658702969551086,
      "learning_rate": 9.559249329758713e-05,
      "loss": 0.8392,
      "step": 9840
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.6922406554222107,
      "learning_rate": 9.5485254691689e-05,
      "loss": 0.8608,
      "step": 9850
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.741242527961731,
      "learning_rate": 9.537801608579088e-05,
      "loss": 0.8475,
      "step": 9860
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.8996413350105286,
      "learning_rate": 9.527077747989276e-05,
      "loss": 0.8314,
      "step": 9870
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.6389515399932861,
      "learning_rate": 9.516353887399464e-05,
      "loss": 0.9196,
      "step": 9880
    },
    {
      "epoch": 1.5824,
      "grad_norm": 1.6407662630081177,
      "learning_rate": 9.505630026809653e-05,
      "loss": 0.8037,
      "step": 9890
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.6271130442619324,
      "learning_rate": 9.49490616621984e-05,
      "loss": 0.8196,
      "step": 9900
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.7726937532424927,
      "learning_rate": 9.484182305630027e-05,
      "loss": 0.848,
      "step": 9910
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.8370164036750793,
      "learning_rate": 9.473458445040216e-05,
      "loss": 0.8097,
      "step": 9920
    },
    {
      "epoch": 1.5888,
      "grad_norm": 2.4939191341400146,
      "learning_rate": 9.462734584450402e-05,
      "loss": 0.7827,
      "step": 9930
    },
    {
      "epoch": 1.5904,
      "grad_norm": 1.3970472812652588,
      "learning_rate": 9.45201072386059e-05,
      "loss": 0.8234,
      "step": 9940
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.3581777811050415,
      "learning_rate": 9.441286863270777e-05,
      "loss": 0.8587,
      "step": 9950
    },
    {
      "epoch": 1.5936,
      "grad_norm": 1.1270102262496948,
      "learning_rate": 9.430563002680965e-05,
      "loss": 0.8625,
      "step": 9960
    },
    {
      "epoch": 1.5952,
      "grad_norm": 1.2239376306533813,
      "learning_rate": 9.419839142091153e-05,
      "loss": 0.8097,
      "step": 9970
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.7585681080818176,
      "learning_rate": 9.40911528150134e-05,
      "loss": 0.8408,
      "step": 9980
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.5884614586830139,
      "learning_rate": 9.398391420911528e-05,
      "loss": 0.8527,
      "step": 9990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.6537351608276367,
      "learning_rate": 9.387667560321716e-05,
      "loss": 0.8188,
      "step": 10000
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.6218771934509277,
      "learning_rate": 9.376943699731904e-05,
      "loss": 0.8121,
      "step": 10010
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.8153889179229736,
      "learning_rate": 9.366219839142093e-05,
      "loss": 0.8705,
      "step": 10020
    },
    {
      "epoch": 1.6048,
      "grad_norm": 1.3628877401351929,
      "learning_rate": 9.355495978552279e-05,
      "loss": 0.7832,
      "step": 10030
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.7958081960678101,
      "learning_rate": 9.344772117962467e-05,
      "loss": 0.7537,
      "step": 10040
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.2291091680526733,
      "learning_rate": 9.334048257372655e-05,
      "loss": 0.8655,
      "step": 10050
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.804685115814209,
      "learning_rate": 9.323324396782842e-05,
      "loss": 0.8347,
      "step": 10060
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.8101108074188232,
      "learning_rate": 9.31260053619303e-05,
      "loss": 0.7988,
      "step": 10070
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.6243981719017029,
      "learning_rate": 9.301876675603217e-05,
      "loss": 0.8053,
      "step": 10080
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.9430791735649109,
      "learning_rate": 9.291152815013405e-05,
      "loss": 0.8642,
      "step": 10090
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.7182321548461914,
      "learning_rate": 9.280428954423593e-05,
      "loss": 0.83,
      "step": 10100
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.7036623954772949,
      "learning_rate": 9.26970509383378e-05,
      "loss": 0.8084,
      "step": 10110
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.842020571231842,
      "learning_rate": 9.258981233243968e-05,
      "loss": 0.8361,
      "step": 10120
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.9201138019561768,
      "learning_rate": 9.248257372654155e-05,
      "loss": 0.8506,
      "step": 10130
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.9433355927467346,
      "learning_rate": 9.237533512064343e-05,
      "loss": 0.811,
      "step": 10140
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.9316338896751404,
      "learning_rate": 9.226809651474531e-05,
      "loss": 0.8913,
      "step": 10150
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.5925838351249695,
      "learning_rate": 9.216085790884719e-05,
      "loss": 0.8121,
      "step": 10160
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.7859848737716675,
      "learning_rate": 9.205361930294907e-05,
      "loss": 0.8795,
      "step": 10170
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.7700618505477905,
      "learning_rate": 9.194638069705094e-05,
      "loss": 0.8906,
      "step": 10180
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 1.4031180143356323,
      "learning_rate": 9.183914209115282e-05,
      "loss": 0.7769,
      "step": 10190
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.8281263113021851,
      "learning_rate": 9.17319034852547e-05,
      "loss": 0.8667,
      "step": 10200
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.848534882068634,
      "learning_rate": 9.162466487935657e-05,
      "loss": 0.8713,
      "step": 10210
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.7621541023254395,
      "learning_rate": 9.151742627345845e-05,
      "loss": 0.9279,
      "step": 10220
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.9033078551292419,
      "learning_rate": 9.141018766756032e-05,
      "loss": 0.8295,
      "step": 10230
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.6751384139060974,
      "learning_rate": 9.13029490616622e-05,
      "loss": 0.8544,
      "step": 10240
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.8276211023330688,
      "learning_rate": 9.119571045576408e-05,
      "loss": 0.8714,
      "step": 10250
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.7302138805389404,
      "learning_rate": 9.108847184986595e-05,
      "loss": 0.818,
      "step": 10260
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.6620172262191772,
      "learning_rate": 9.099195710455765e-05,
      "loss": 0.7422,
      "step": 10270
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.7390761971473694,
      "learning_rate": 9.088471849865953e-05,
      "loss": 0.8087,
      "step": 10280
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.7132805585861206,
      "learning_rate": 9.07774798927614e-05,
      "loss": 0.8562,
      "step": 10290
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.6530610918998718,
      "learning_rate": 9.067024128686328e-05,
      "loss": 0.8238,
      "step": 10300
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.655888020992279,
      "learning_rate": 9.056300268096514e-05,
      "loss": 0.7622,
      "step": 10310
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.8884538412094116,
      "learning_rate": 9.045576407506702e-05,
      "loss": 0.8295,
      "step": 10320
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.8583500385284424,
      "learning_rate": 9.03485254691689e-05,
      "loss": 0.8507,
      "step": 10330
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.8165667057037354,
      "learning_rate": 9.024128686327077e-05,
      "loss": 0.8085,
      "step": 10340
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.7809566855430603,
      "learning_rate": 9.013404825737265e-05,
      "loss": 0.874,
      "step": 10350
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.8504471182823181,
      "learning_rate": 9.002680965147454e-05,
      "loss": 0.7976,
      "step": 10360
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.6274678707122803,
      "learning_rate": 8.991957104557642e-05,
      "loss": 0.8393,
      "step": 10370
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.7252296805381775,
      "learning_rate": 8.98123324396783e-05,
      "loss": 0.8548,
      "step": 10380
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.8765546679496765,
      "learning_rate": 8.970509383378016e-05,
      "loss": 0.8221,
      "step": 10390
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.8964016437530518,
      "learning_rate": 8.959785522788205e-05,
      "loss": 0.8811,
      "step": 10400
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.9885873794555664,
      "learning_rate": 8.949061662198391e-05,
      "loss": 0.9006,
      "step": 10410
    },
    {
      "epoch": 1.6672,
      "grad_norm": 2.3369314670562744,
      "learning_rate": 8.93833780160858e-05,
      "loss": 0.8619,
      "step": 10420
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.7862232327461243,
      "learning_rate": 8.927613941018768e-05,
      "loss": 0.8373,
      "step": 10430
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.6971012353897095,
      "learning_rate": 8.916890080428954e-05,
      "loss": 0.7832,
      "step": 10440
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.6713917255401611,
      "learning_rate": 8.906166219839142e-05,
      "loss": 0.8093,
      "step": 10450
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.9198905229568481,
      "learning_rate": 8.895442359249329e-05,
      "loss": 0.7959,
      "step": 10460
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.7852377891540527,
      "learning_rate": 8.884718498659517e-05,
      "loss": 0.8801,
      "step": 10470
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.7017005681991577,
      "learning_rate": 8.873994638069705e-05,
      "loss": 0.8438,
      "step": 10480
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.6680333018302917,
      "learning_rate": 8.863270777479893e-05,
      "loss": 0.8493,
      "step": 10490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.9038196802139282,
      "learning_rate": 8.852546916890082e-05,
      "loss": 0.8434,
      "step": 10500
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.6364898085594177,
      "learning_rate": 8.841823056300268e-05,
      "loss": 0.8619,
      "step": 10510
    },
    {
      "epoch": 1.6832,
      "grad_norm": 1.1195399761199951,
      "learning_rate": 8.831099195710456e-05,
      "loss": 0.8304,
      "step": 10520
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.8469708561897278,
      "learning_rate": 8.820375335120645e-05,
      "loss": 0.8264,
      "step": 10530
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.8648480772972107,
      "learning_rate": 8.809651474530831e-05,
      "loss": 0.8129,
      "step": 10540
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.6905026435852051,
      "learning_rate": 8.79892761394102e-05,
      "loss": 0.8038,
      "step": 10550
    },
    {
      "epoch": 1.6896,
      "grad_norm": 1.2272661924362183,
      "learning_rate": 8.788203753351207e-05,
      "loss": 0.8285,
      "step": 10560
    },
    {
      "epoch": 1.6912,
      "grad_norm": 1.1343629360198975,
      "learning_rate": 8.777479892761394e-05,
      "loss": 0.8401,
      "step": 10570
    },
    {
      "epoch": 1.6928,
      "grad_norm": 1.0413144826889038,
      "learning_rate": 8.766756032171582e-05,
      "loss": 0.8929,
      "step": 10580
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.6627730131149292,
      "learning_rate": 8.756032171581769e-05,
      "loss": 0.8535,
      "step": 10590
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.8983824253082275,
      "learning_rate": 8.745308310991957e-05,
      "loss": 0.8217,
      "step": 10600
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.6598224639892578,
      "learning_rate": 8.734584450402145e-05,
      "loss": 0.7786,
      "step": 10610
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.7800338268280029,
      "learning_rate": 8.723860589812332e-05,
      "loss": 0.9076,
      "step": 10620
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.9602718949317932,
      "learning_rate": 8.71313672922252e-05,
      "loss": 0.8427,
      "step": 10630
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.9350053668022156,
      "learning_rate": 8.702412868632708e-05,
      "loss": 0.8535,
      "step": 10640
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.6120880842208862,
      "learning_rate": 8.691689008042896e-05,
      "loss": 0.8489,
      "step": 10650
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.9766734838485718,
      "learning_rate": 8.680965147453084e-05,
      "loss": 0.765,
      "step": 10660
    },
    {
      "epoch": 1.7072,
      "grad_norm": 1.143330693244934,
      "learning_rate": 8.670241286863271e-05,
      "loss": 0.8347,
      "step": 10670
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.7418290376663208,
      "learning_rate": 8.65951742627346e-05,
      "loss": 0.9053,
      "step": 10680
    },
    {
      "epoch": 1.7104,
      "grad_norm": 1.008745789527893,
      "learning_rate": 8.648793565683646e-05,
      "loss": 0.9658,
      "step": 10690
    },
    {
      "epoch": 1.712,
      "grad_norm": 1.1511855125427246,
      "learning_rate": 8.638069705093834e-05,
      "loss": 0.87,
      "step": 10700
    },
    {
      "epoch": 1.7136,
      "grad_norm": 1.023902177810669,
      "learning_rate": 8.627345844504022e-05,
      "loss": 0.9068,
      "step": 10710
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 1.4180686473846436,
      "learning_rate": 8.616621983914209e-05,
      "loss": 0.8512,
      "step": 10720
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.664261519908905,
      "learning_rate": 8.605898123324397e-05,
      "loss": 0.8252,
      "step": 10730
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.8898172378540039,
      "learning_rate": 8.595174262734584e-05,
      "loss": 0.7943,
      "step": 10740
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.7175270318984985,
      "learning_rate": 8.584450402144772e-05,
      "loss": 0.8034,
      "step": 10750
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.7384528517723083,
      "learning_rate": 8.57372654155496e-05,
      "loss": 0.904,
      "step": 10760
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 1.1524426937103271,
      "learning_rate": 8.563002680965148e-05,
      "loss": 0.8588,
      "step": 10770
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.7985631227493286,
      "learning_rate": 8.552278820375336e-05,
      "loss": 0.8236,
      "step": 10780
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.7666189670562744,
      "learning_rate": 8.541554959785523e-05,
      "loss": 0.8022,
      "step": 10790
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.6610608696937561,
      "learning_rate": 8.530831099195711e-05,
      "loss": 0.8615,
      "step": 10800
    },
    {
      "epoch": 1.7296,
      "grad_norm": 1.2146836519241333,
      "learning_rate": 8.520107238605899e-05,
      "loss": 0.8994,
      "step": 10810
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 1.6445592641830444,
      "learning_rate": 8.509383378016086e-05,
      "loss": 0.8769,
      "step": 10820
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.638031542301178,
      "learning_rate": 8.498659517426274e-05,
      "loss": 0.7816,
      "step": 10830
    },
    {
      "epoch": 1.7344,
      "grad_norm": 1.1715507507324219,
      "learning_rate": 8.487935656836462e-05,
      "loss": 0.8852,
      "step": 10840
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.7250113487243652,
      "learning_rate": 8.477211796246649e-05,
      "loss": 0.8571,
      "step": 10850
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.8013997673988342,
      "learning_rate": 8.466487935656837e-05,
      "loss": 0.8568,
      "step": 10860
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.6494153738021851,
      "learning_rate": 8.455764075067024e-05,
      "loss": 0.7996,
      "step": 10870
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.9868687987327576,
      "learning_rate": 8.445040214477212e-05,
      "loss": 0.9296,
      "step": 10880
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.8937762975692749,
      "learning_rate": 8.4343163538874e-05,
      "loss": 0.839,
      "step": 10890
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.8125815987586975,
      "learning_rate": 8.423592493297587e-05,
      "loss": 0.8566,
      "step": 10900
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.735331118106842,
      "learning_rate": 8.412868632707775e-05,
      "loss": 0.815,
      "step": 10910
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.897667646408081,
      "learning_rate": 8.402144772117963e-05,
      "loss": 0.8623,
      "step": 10920
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.8070967197418213,
      "learning_rate": 8.391420911528151e-05,
      "loss": 0.8587,
      "step": 10930
    },
    {
      "epoch": 1.7504,
      "grad_norm": 1.0469483137130737,
      "learning_rate": 8.380697050938339e-05,
      "loss": 0.8018,
      "step": 10940
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.9660547971725464,
      "learning_rate": 8.369973190348526e-05,
      "loss": 0.8338,
      "step": 10950
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.8973113298416138,
      "learning_rate": 8.359249329758714e-05,
      "loss": 0.8297,
      "step": 10960
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.7397811412811279,
      "learning_rate": 8.348525469168901e-05,
      "loss": 0.8616,
      "step": 10970
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.8440439701080322,
      "learning_rate": 8.337801608579089e-05,
      "loss": 0.836,
      "step": 10980
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.9192354083061218,
      "learning_rate": 8.327077747989277e-05,
      "loss": 0.8256,
      "step": 10990
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.6504552364349365,
      "learning_rate": 8.316353887399464e-05,
      "loss": 0.8279,
      "step": 11000
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.6788103580474854,
      "learning_rate": 8.305630026809652e-05,
      "loss": 0.7778,
      "step": 11010
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 1.1261765956878662,
      "learning_rate": 8.294906166219839e-05,
      "loss": 0.8,
      "step": 11020
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 1.1559679508209229,
      "learning_rate": 8.284182305630027e-05,
      "loss": 0.8839,
      "step": 11030
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.9021129012107849,
      "learning_rate": 8.273458445040215e-05,
      "loss": 0.7983,
      "step": 11040
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.8004227876663208,
      "learning_rate": 8.262734584450403e-05,
      "loss": 0.827,
      "step": 11050
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.7944678068161011,
      "learning_rate": 8.252010723860591e-05,
      "loss": 0.7835,
      "step": 11060
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.7320665717124939,
      "learning_rate": 8.241286863270778e-05,
      "loss": 0.8115,
      "step": 11070
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.7638764977455139,
      "learning_rate": 8.230563002680966e-05,
      "loss": 0.8298,
      "step": 11080
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.9389942288398743,
      "learning_rate": 8.219839142091154e-05,
      "loss": 0.8551,
      "step": 11090
    },
    {
      "epoch": 1.776,
      "grad_norm": 1.0087182521820068,
      "learning_rate": 8.209115281501341e-05,
      "loss": 0.8731,
      "step": 11100
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.9071784615516663,
      "learning_rate": 8.198391420911529e-05,
      "loss": 0.8427,
      "step": 11110
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.593526303768158,
      "learning_rate": 8.187667560321716e-05,
      "loss": 0.8788,
      "step": 11120
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.7095714211463928,
      "learning_rate": 8.176943699731904e-05,
      "loss": 0.7867,
      "step": 11130
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.8493101596832275,
      "learning_rate": 8.166219839142092e-05,
      "loss": 0.7286,
      "step": 11140
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.6585631966590881,
      "learning_rate": 8.155495978552279e-05,
      "loss": 0.87,
      "step": 11150
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.74917072057724,
      "learning_rate": 8.144772117962467e-05,
      "loss": 0.8368,
      "step": 11160
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.6021828651428223,
      "learning_rate": 8.134048257372655e-05,
      "loss": 0.8485,
      "step": 11170
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.6505814790725708,
      "learning_rate": 8.123324396782842e-05,
      "loss": 0.7747,
      "step": 11180
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.7447399497032166,
      "learning_rate": 8.11260053619303e-05,
      "loss": 0.8754,
      "step": 11190
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.6758383512496948,
      "learning_rate": 8.101876675603218e-05,
      "loss": 0.8372,
      "step": 11200
    },
    {
      "epoch": 1.7936,
      "grad_norm": 1.0027645826339722,
      "learning_rate": 8.091152815013406e-05,
      "loss": 0.8626,
      "step": 11210
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.7576929926872253,
      "learning_rate": 8.080428954423594e-05,
      "loss": 0.8501,
      "step": 11220
    },
    {
      "epoch": 1.7968,
      "grad_norm": 1.1193459033966064,
      "learning_rate": 8.069705093833781e-05,
      "loss": 0.769,
      "step": 11230
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.7496766448020935,
      "learning_rate": 8.058981233243969e-05,
      "loss": 0.8006,
      "step": 11240
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.6073459386825562,
      "learning_rate": 8.048257372654156e-05,
      "loss": 0.8961,
      "step": 11250
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.970072329044342,
      "learning_rate": 8.037533512064344e-05,
      "loss": 0.8371,
      "step": 11260
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.7998124361038208,
      "learning_rate": 8.026809651474532e-05,
      "loss": 0.8472,
      "step": 11270
    },
    {
      "epoch": 1.8048,
      "grad_norm": 1.0455820560455322,
      "learning_rate": 8.016085790884718e-05,
      "loss": 0.7606,
      "step": 11280
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.7618204951286316,
      "learning_rate": 8.005361930294907e-05,
      "loss": 0.8122,
      "step": 11290
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.6198642253875732,
      "learning_rate": 7.994638069705093e-05,
      "loss": 0.8177,
      "step": 11300
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.6855021119117737,
      "learning_rate": 7.983914209115281e-05,
      "loss": 0.8831,
      "step": 11310
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.8977163434028625,
      "learning_rate": 7.97319034852547e-05,
      "loss": 0.8696,
      "step": 11320
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.9591643214225769,
      "learning_rate": 7.962466487935656e-05,
      "loss": 0.8577,
      "step": 11330
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.7267610430717468,
      "learning_rate": 7.951742627345844e-05,
      "loss": 0.8478,
      "step": 11340
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.837492048740387,
      "learning_rate": 7.941018766756033e-05,
      "loss": 0.796,
      "step": 11350
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.8453765511512756,
      "learning_rate": 7.93029490616622e-05,
      "loss": 0.8811,
      "step": 11360
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.8677658438682556,
      "learning_rate": 7.919571045576409e-05,
      "loss": 0.907,
      "step": 11370
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.6793787479400635,
      "learning_rate": 7.908847184986595e-05,
      "loss": 0.8939,
      "step": 11380
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.7249651551246643,
      "learning_rate": 7.898123324396784e-05,
      "loss": 0.8578,
      "step": 11390
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.0356565713882446,
      "learning_rate": 7.88739946380697e-05,
      "loss": 0.7782,
      "step": 11400
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.9468764662742615,
      "learning_rate": 7.876675603217158e-05,
      "loss": 0.9265,
      "step": 11410
    },
    {
      "epoch": 1.8272,
      "grad_norm": 1.2691311836242676,
      "learning_rate": 7.865951742627347e-05,
      "loss": 0.8385,
      "step": 11420
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.7415022850036621,
      "learning_rate": 7.855227882037533e-05,
      "loss": 0.8156,
      "step": 11430
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.919198215007782,
      "learning_rate": 7.844504021447721e-05,
      "loss": 0.8006,
      "step": 11440
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.6617584228515625,
      "learning_rate": 7.833780160857908e-05,
      "loss": 0.8275,
      "step": 11450
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.7482409477233887,
      "learning_rate": 7.823056300268096e-05,
      "loss": 0.7719,
      "step": 11460
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.6530118584632874,
      "learning_rate": 7.812332439678284e-05,
      "loss": 0.9667,
      "step": 11470
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.8169069886207581,
      "learning_rate": 7.801608579088472e-05,
      "loss": 0.8338,
      "step": 11480
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.9085694551467896,
      "learning_rate": 7.79088471849866e-05,
      "loss": 0.9269,
      "step": 11490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.7682809829711914,
      "learning_rate": 7.780160857908849e-05,
      "loss": 0.8591,
      "step": 11500
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.7586818933486938,
      "learning_rate": 7.769436997319035e-05,
      "loss": 0.8453,
      "step": 11510
    },
    {
      "epoch": 1.8432,
      "grad_norm": 1.2350448369979858,
      "learning_rate": 7.758713136729224e-05,
      "loss": 0.8226,
      "step": 11520
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.9441657662391663,
      "learning_rate": 7.74798927613941e-05,
      "loss": 0.7729,
      "step": 11530
    },
    {
      "epoch": 1.8464,
      "grad_norm": 1.1260769367218018,
      "learning_rate": 7.737265415549598e-05,
      "loss": 0.8288,
      "step": 11540
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.6966758370399475,
      "learning_rate": 7.726541554959786e-05,
      "loss": 0.8317,
      "step": 11550
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 1.2685295343399048,
      "learning_rate": 7.715817694369973e-05,
      "loss": 0.8442,
      "step": 11560
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.8575935363769531,
      "learning_rate": 7.705093833780161e-05,
      "loss": 0.8118,
      "step": 11570
    },
    {
      "epoch": 1.8528,
      "grad_norm": 1.1634303331375122,
      "learning_rate": 7.694369973190348e-05,
      "loss": 0.9392,
      "step": 11580
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.696411669254303,
      "learning_rate": 7.683646112600536e-05,
      "loss": 0.8254,
      "step": 11590
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.6473267674446106,
      "learning_rate": 7.672922252010724e-05,
      "loss": 0.7985,
      "step": 11600
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.7853213548660278,
      "learning_rate": 7.662198391420911e-05,
      "loss": 0.7721,
      "step": 11610
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.799015998840332,
      "learning_rate": 7.651474530831099e-05,
      "loss": 0.8768,
      "step": 11620
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.9013909101486206,
      "learning_rate": 7.640750670241287e-05,
      "loss": 0.8748,
      "step": 11630
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.7187890410423279,
      "learning_rate": 7.630026809651475e-05,
      "loss": 0.9243,
      "step": 11640
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.7813549637794495,
      "learning_rate": 7.619302949061663e-05,
      "loss": 0.9078,
      "step": 11650
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 1.381710171699524,
      "learning_rate": 7.60857908847185e-05,
      "loss": 0.872,
      "step": 11660
    },
    {
      "epoch": 1.8672,
      "grad_norm": 1.2696216106414795,
      "learning_rate": 7.597855227882038e-05,
      "loss": 0.8491,
      "step": 11670
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.7119147777557373,
      "learning_rate": 7.587131367292225e-05,
      "loss": 0.8329,
      "step": 11680
    },
    {
      "epoch": 1.8704,
      "grad_norm": 1.2644990682601929,
      "learning_rate": 7.576407506702413e-05,
      "loss": 0.8713,
      "step": 11690
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.7563492059707642,
      "learning_rate": 7.565683646112601e-05,
      "loss": 0.7899,
      "step": 11700
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 1.1858607530593872,
      "learning_rate": 7.554959785522788e-05,
      "loss": 0.7585,
      "step": 11710
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.7292085289955139,
      "learning_rate": 7.544235924932976e-05,
      "loss": 0.8451,
      "step": 11720
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.896178126335144,
      "learning_rate": 7.533512064343163e-05,
      "loss": 0.8707,
      "step": 11730
    },
    {
      "epoch": 1.8784,
      "grad_norm": 1.2930045127868652,
      "learning_rate": 7.522788203753351e-05,
      "loss": 0.8576,
      "step": 11740
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.9151625633239746,
      "learning_rate": 7.512064343163539e-05,
      "loss": 0.7952,
      "step": 11750
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.9428492188453674,
      "learning_rate": 7.501340482573727e-05,
      "loss": 0.8367,
      "step": 11760
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.8858353495597839,
      "learning_rate": 7.490616621983915e-05,
      "loss": 0.8345,
      "step": 11770
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.7220137119293213,
      "learning_rate": 7.479892761394103e-05,
      "loss": 0.807,
      "step": 11780
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.6657624244689941,
      "learning_rate": 7.46916890080429e-05,
      "loss": 0.9054,
      "step": 11790
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.0740687847137451,
      "learning_rate": 7.458445040214478e-05,
      "loss": 0.8033,
      "step": 11800
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.7464215159416199,
      "learning_rate": 7.447721179624665e-05,
      "loss": 0.8503,
      "step": 11810
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.704880952835083,
      "learning_rate": 7.436997319034853e-05,
      "loss": 0.8638,
      "step": 11820
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.9251201152801514,
      "learning_rate": 7.426273458445041e-05,
      "loss": 0.7279,
      "step": 11830
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.7544312477111816,
      "learning_rate": 7.415549597855228e-05,
      "loss": 0.8752,
      "step": 11840
    },
    {
      "epoch": 1.896,
      "grad_norm": 1.359207272529602,
      "learning_rate": 7.404825737265416e-05,
      "loss": 0.8359,
      "step": 11850
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.685627281665802,
      "learning_rate": 7.394101876675603e-05,
      "loss": 0.8008,
      "step": 11860
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.8135024905204773,
      "learning_rate": 7.383378016085791e-05,
      "loss": 0.7536,
      "step": 11870
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.6309419274330139,
      "learning_rate": 7.372654155495979e-05,
      "loss": 0.8167,
      "step": 11880
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.6793241500854492,
      "learning_rate": 7.361930294906166e-05,
      "loss": 0.8767,
      "step": 11890
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.8311890363693237,
      "learning_rate": 7.351206434316354e-05,
      "loss": 0.904,
      "step": 11900
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.6989061236381531,
      "learning_rate": 7.340482573726542e-05,
      "loss": 0.8919,
      "step": 11910
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.7952362895011902,
      "learning_rate": 7.32975871313673e-05,
      "loss": 0.8595,
      "step": 11920
    },
    {
      "epoch": 1.9088,
      "grad_norm": 1.4918924570083618,
      "learning_rate": 7.319034852546918e-05,
      "loss": 0.8231,
      "step": 11930
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.781165599822998,
      "learning_rate": 7.308310991957105e-05,
      "loss": 0.8651,
      "step": 11940
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.9575464129447937,
      "learning_rate": 7.297587131367293e-05,
      "loss": 0.8697,
      "step": 11950
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.8170806169509888,
      "learning_rate": 7.28686327077748e-05,
      "loss": 0.7799,
      "step": 11960
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.7693600654602051,
      "learning_rate": 7.276139410187668e-05,
      "loss": 0.824,
      "step": 11970
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 1.0413137674331665,
      "learning_rate": 7.265415549597856e-05,
      "loss": 0.8758,
      "step": 11980
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.8446535468101501,
      "learning_rate": 7.254691689008043e-05,
      "loss": 0.8147,
      "step": 11990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.6966274976730347,
      "learning_rate": 7.243967828418231e-05,
      "loss": 0.8177,
      "step": 12000
    },
    {
      "epoch": 1.9216,
      "grad_norm": 1.2310209274291992,
      "learning_rate": 7.233243967828418e-05,
      "loss": 0.8737,
      "step": 12010
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.8097180724143982,
      "learning_rate": 7.222520107238606e-05,
      "loss": 0.8567,
      "step": 12020
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 1.2034118175506592,
      "learning_rate": 7.211796246648794e-05,
      "loss": 0.8317,
      "step": 12030
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.9292973279953003,
      "learning_rate": 7.201072386058982e-05,
      "loss": 0.7945,
      "step": 12040
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.8213017582893372,
      "learning_rate": 7.19034852546917e-05,
      "loss": 0.7963,
      "step": 12050
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.8009082078933716,
      "learning_rate": 7.179624664879357e-05,
      "loss": 0.7723,
      "step": 12060
    },
    {
      "epoch": 1.9312,
      "grad_norm": 1.015441656112671,
      "learning_rate": 7.168900804289545e-05,
      "loss": 0.8689,
      "step": 12070
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.9089308977127075,
      "learning_rate": 7.158176943699733e-05,
      "loss": 0.9048,
      "step": 12080
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.7459598183631897,
      "learning_rate": 7.14745308310992e-05,
      "loss": 0.9323,
      "step": 12090
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.7765095233917236,
      "learning_rate": 7.136729222520108e-05,
      "loss": 0.8357,
      "step": 12100
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.8861911296844482,
      "learning_rate": 7.126005361930296e-05,
      "loss": 0.8452,
      "step": 12110
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.7952036261558533,
      "learning_rate": 7.115281501340483e-05,
      "loss": 0.8072,
      "step": 12120
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.6778646111488342,
      "learning_rate": 7.104557640750671e-05,
      "loss": 0.7432,
      "step": 12130
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.6859794855117798,
      "learning_rate": 7.093833780160858e-05,
      "loss": 0.8782,
      "step": 12140
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.9417238831520081,
      "learning_rate": 7.083109919571046e-05,
      "loss": 0.8769,
      "step": 12150
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.669838011264801,
      "learning_rate": 7.072386058981234e-05,
      "loss": 0.8359,
      "step": 12160
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.6778668761253357,
      "learning_rate": 7.06166219839142e-05,
      "loss": 0.8268,
      "step": 12170
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.7381985187530518,
      "learning_rate": 7.050938337801609e-05,
      "loss": 0.8076,
      "step": 12180
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.7107844948768616,
      "learning_rate": 7.040214477211797e-05,
      "loss": 0.8343,
      "step": 12190
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.3225897550582886,
      "learning_rate": 7.029490616621985e-05,
      "loss": 0.8394,
      "step": 12200
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.7549343705177307,
      "learning_rate": 7.018766756032173e-05,
      "loss": 0.8645,
      "step": 12210
    },
    {
      "epoch": 1.9552,
      "grad_norm": 1.1038538217544556,
      "learning_rate": 7.00804289544236e-05,
      "loss": 0.817,
      "step": 12220
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.9130417704582214,
      "learning_rate": 6.997319034852548e-05,
      "loss": 0.8673,
      "step": 12230
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.9010113477706909,
      "learning_rate": 6.986595174262735e-05,
      "loss": 0.8333,
      "step": 12240
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.7634526491165161,
      "learning_rate": 6.975871313672923e-05,
      "loss": 0.825,
      "step": 12250
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.8572220206260681,
      "learning_rate": 6.965147453083111e-05,
      "loss": 0.855,
      "step": 12260
    },
    {
      "epoch": 1.9632,
      "grad_norm": 1.2398943901062012,
      "learning_rate": 6.954423592493297e-05,
      "loss": 0.9097,
      "step": 12270
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.8137394785881042,
      "learning_rate": 6.943699731903486e-05,
      "loss": 0.8443,
      "step": 12280
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.9411426782608032,
      "learning_rate": 6.932975871313672e-05,
      "loss": 0.8054,
      "step": 12290
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.7485193610191345,
      "learning_rate": 6.92225201072386e-05,
      "loss": 0.8915,
      "step": 12300
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.838483989238739,
      "learning_rate": 6.911528150134049e-05,
      "loss": 0.817,
      "step": 12310
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.8997262716293335,
      "learning_rate": 6.900804289544237e-05,
      "loss": 0.8964,
      "step": 12320
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 1.8780544996261597,
      "learning_rate": 6.890080428954425e-05,
      "loss": 0.9539,
      "step": 12330
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 1.543928623199463,
      "learning_rate": 6.879356568364611e-05,
      "loss": 0.9553,
      "step": 12340
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.8235310316085815,
      "learning_rate": 6.8686327077748e-05,
      "loss": 0.8864,
      "step": 12350
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.6466172933578491,
      "learning_rate": 6.857908847184988e-05,
      "loss": 0.8404,
      "step": 12360
    },
    {
      "epoch": 1.9792,
      "grad_norm": 0.739140510559082,
      "learning_rate": 6.847184986595174e-05,
      "loss": 0.8367,
      "step": 12370
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.8046956658363342,
      "learning_rate": 6.836461126005363e-05,
      "loss": 0.7728,
      "step": 12380
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 0.794851541519165,
      "learning_rate": 6.82573726541555e-05,
      "loss": 0.8609,
      "step": 12390
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.786741316318512,
      "learning_rate": 6.815013404825737e-05,
      "loss": 0.8535,
      "step": 12400
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.7554112076759338,
      "learning_rate": 6.804289544235926e-05,
      "loss": 0.8206,
      "step": 12410
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.9575395584106445,
      "learning_rate": 6.793565683646112e-05,
      "loss": 0.8697,
      "step": 12420
    },
    {
      "epoch": 1.9888,
      "grad_norm": 0.9100741147994995,
      "learning_rate": 6.7828418230563e-05,
      "loss": 0.8161,
      "step": 12430
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.779083788394928,
      "learning_rate": 6.772117962466488e-05,
      "loss": 0.8098,
      "step": 12440
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.7167852520942688,
      "learning_rate": 6.761394101876675e-05,
      "loss": 0.8465,
      "step": 12450
    },
    {
      "epoch": 1.9936,
      "grad_norm": 0.7154662609100342,
      "learning_rate": 6.750670241286863e-05,
      "loss": 0.7856,
      "step": 12460
    },
    {
      "epoch": 1.9952,
      "grad_norm": 0.961073100566864,
      "learning_rate": 6.739946380697051e-05,
      "loss": 0.8887,
      "step": 12470
    },
    {
      "epoch": 1.9968,
      "grad_norm": 1.3809688091278076,
      "learning_rate": 6.72922252010724e-05,
      "loss": 0.8461,
      "step": 12480
    },
    {
      "epoch": 1.9984,
      "grad_norm": 0.795051634311676,
      "learning_rate": 6.718498659517428e-05,
      "loss": 0.8577,
      "step": 12490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.7578532099723816,
      "learning_rate": 6.707774798927614e-05,
      "loss": 0.909,
      "step": 12500
    },
    {
      "epoch": 2.0016,
      "grad_norm": 0.6246334314346313,
      "learning_rate": 6.697050938337802e-05,
      "loss": 0.8337,
      "step": 12510
    },
    {
      "epoch": 2.0032,
      "grad_norm": 0.9548331499099731,
      "learning_rate": 6.686327077747989e-05,
      "loss": 0.7715,
      "step": 12520
    },
    {
      "epoch": 2.0048,
      "grad_norm": 0.968660831451416,
      "learning_rate": 6.675603217158177e-05,
      "loss": 0.8177,
      "step": 12530
    },
    {
      "epoch": 2.0064,
      "grad_norm": 0.8141041398048401,
      "learning_rate": 6.664879356568365e-05,
      "loss": 0.8447,
      "step": 12540
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.5764250755310059,
      "learning_rate": 6.654155495978552e-05,
      "loss": 0.8489,
      "step": 12550
    },
    {
      "epoch": 2.0096,
      "grad_norm": 0.8813090324401855,
      "learning_rate": 6.64343163538874e-05,
      "loss": 0.8077,
      "step": 12560
    },
    {
      "epoch": 2.0112,
      "grad_norm": 1.007606863975525,
      "learning_rate": 6.632707774798927e-05,
      "loss": 0.79,
      "step": 12570
    },
    {
      "epoch": 2.0128,
      "grad_norm": 1.6982454061508179,
      "learning_rate": 6.621983914209115e-05,
      "loss": 0.8507,
      "step": 12580
    },
    {
      "epoch": 2.0144,
      "grad_norm": 0.8263561129570007,
      "learning_rate": 6.611260053619303e-05,
      "loss": 0.8657,
      "step": 12590
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.7029777765274048,
      "learning_rate": 6.60053619302949e-05,
      "loss": 0.8478,
      "step": 12600
    },
    {
      "epoch": 2.0176,
      "grad_norm": 0.9508182406425476,
      "learning_rate": 6.589812332439678e-05,
      "loss": 0.8354,
      "step": 12610
    },
    {
      "epoch": 2.0192,
      "grad_norm": 0.7270359992980957,
      "learning_rate": 6.579088471849866e-05,
      "loss": 0.8488,
      "step": 12620
    },
    {
      "epoch": 2.0208,
      "grad_norm": 0.527594268321991,
      "learning_rate": 6.568364611260054e-05,
      "loss": 0.7034,
      "step": 12630
    },
    {
      "epoch": 2.0224,
      "grad_norm": 1.0070644617080688,
      "learning_rate": 6.557640750670242e-05,
      "loss": 0.9038,
      "step": 12640
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.7624707818031311,
      "learning_rate": 6.546916890080429e-05,
      "loss": 0.8502,
      "step": 12650
    },
    {
      "epoch": 2.0256,
      "grad_norm": 0.6048062443733215,
      "learning_rate": 6.536193029490617e-05,
      "loss": 0.8487,
      "step": 12660
    },
    {
      "epoch": 2.0272,
      "grad_norm": 1.3346089124679565,
      "learning_rate": 6.525469168900804e-05,
      "loss": 0.8467,
      "step": 12670
    },
    {
      "epoch": 2.0288,
      "grad_norm": 0.9181366562843323,
      "learning_rate": 6.514745308310992e-05,
      "loss": 0.8155,
      "step": 12680
    },
    {
      "epoch": 2.0304,
      "grad_norm": 0.7316889762878418,
      "learning_rate": 6.50402144772118e-05,
      "loss": 0.7593,
      "step": 12690
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.808541476726532,
      "learning_rate": 6.493297587131367e-05,
      "loss": 0.8539,
      "step": 12700
    },
    {
      "epoch": 2.0336,
      "grad_norm": 0.7829915881156921,
      "learning_rate": 6.482573726541555e-05,
      "loss": 0.796,
      "step": 12710
    },
    {
      "epoch": 2.0352,
      "grad_norm": 1.1515871286392212,
      "learning_rate": 6.471849865951743e-05,
      "loss": 0.7916,
      "step": 12720
    },
    {
      "epoch": 2.0368,
      "grad_norm": 0.6587433815002441,
      "learning_rate": 6.46112600536193e-05,
      "loss": 0.884,
      "step": 12730
    },
    {
      "epoch": 2.0384,
      "grad_norm": 1.2849113941192627,
      "learning_rate": 6.450402144772118e-05,
      "loss": 0.7517,
      "step": 12740
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.8516396880149841,
      "learning_rate": 6.439678284182306e-05,
      "loss": 0.7879,
      "step": 12750
    },
    {
      "epoch": 2.0416,
      "grad_norm": 1.1570414304733276,
      "learning_rate": 6.428954423592494e-05,
      "loss": 0.9031,
      "step": 12760
    },
    {
      "epoch": 2.0432,
      "grad_norm": 0.7748183608055115,
      "learning_rate": 6.418230563002682e-05,
      "loss": 0.8513,
      "step": 12770
    },
    {
      "epoch": 2.0448,
      "grad_norm": 0.586301326751709,
      "learning_rate": 6.407506702412869e-05,
      "loss": 0.9131,
      "step": 12780
    },
    {
      "epoch": 2.0464,
      "grad_norm": 0.9591742157936096,
      "learning_rate": 6.396782841823057e-05,
      "loss": 0.8197,
      "step": 12790
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.9088757038116455,
      "learning_rate": 6.386058981233244e-05,
      "loss": 0.8824,
      "step": 12800
    },
    {
      "epoch": 2.0496,
      "grad_norm": 0.8161341547966003,
      "learning_rate": 6.375335120643432e-05,
      "loss": 0.8333,
      "step": 12810
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.7294403314590454,
      "learning_rate": 6.36461126005362e-05,
      "loss": 0.7777,
      "step": 12820
    },
    {
      "epoch": 2.0528,
      "grad_norm": 0.7112034559249878,
      "learning_rate": 6.353887399463807e-05,
      "loss": 0.8022,
      "step": 12830
    },
    {
      "epoch": 2.0544,
      "grad_norm": 1.0883264541625977,
      "learning_rate": 6.343163538873995e-05,
      "loss": 0.8937,
      "step": 12840
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.7486952543258667,
      "learning_rate": 6.332439678284182e-05,
      "loss": 0.8755,
      "step": 12850
    },
    {
      "epoch": 2.0576,
      "grad_norm": 0.6945886015892029,
      "learning_rate": 6.32171581769437e-05,
      "loss": 0.8366,
      "step": 12860
    },
    {
      "epoch": 2.0592,
      "grad_norm": 0.716637372970581,
      "learning_rate": 6.310991957104558e-05,
      "loss": 0.8013,
      "step": 12870
    },
    {
      "epoch": 2.0608,
      "grad_norm": 0.8502846360206604,
      "learning_rate": 6.300268096514745e-05,
      "loss": 0.806,
      "step": 12880
    },
    {
      "epoch": 2.0624,
      "grad_norm": 0.7740595936775208,
      "learning_rate": 6.289544235924933e-05,
      "loss": 0.8558,
      "step": 12890
    },
    {
      "epoch": 2.064,
      "grad_norm": 1.0921536684036255,
      "learning_rate": 6.278820375335121e-05,
      "loss": 0.8137,
      "step": 12900
    },
    {
      "epoch": 2.0656,
      "grad_norm": 0.851533055305481,
      "learning_rate": 6.268096514745309e-05,
      "loss": 0.8956,
      "step": 12910
    },
    {
      "epoch": 2.0672,
      "grad_norm": 0.8554213047027588,
      "learning_rate": 6.257372654155497e-05,
      "loss": 0.804,
      "step": 12920
    },
    {
      "epoch": 2.0688,
      "grad_norm": 0.8194383382797241,
      "learning_rate": 6.246648793565684e-05,
      "loss": 0.8689,
      "step": 12930
    },
    {
      "epoch": 2.0704,
      "grad_norm": 0.6666750311851501,
      "learning_rate": 6.235924932975872e-05,
      "loss": 0.8237,
      "step": 12940
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.659503161907196,
      "learning_rate": 6.225201072386059e-05,
      "loss": 0.8475,
      "step": 12950
    },
    {
      "epoch": 2.0736,
      "grad_norm": 0.7922540307044983,
      "learning_rate": 6.214477211796247e-05,
      "loss": 0.7705,
      "step": 12960
    },
    {
      "epoch": 2.0752,
      "grad_norm": 0.8457297086715698,
      "learning_rate": 6.203753351206435e-05,
      "loss": 0.7471,
      "step": 12970
    },
    {
      "epoch": 2.0768,
      "grad_norm": 1.2012622356414795,
      "learning_rate": 6.193029490616622e-05,
      "loss": 0.8567,
      "step": 12980
    },
    {
      "epoch": 2.0784,
      "grad_norm": 0.5638806223869324,
      "learning_rate": 6.18230563002681e-05,
      "loss": 0.8056,
      "step": 12990
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.9515891671180725,
      "learning_rate": 6.171581769436997e-05,
      "loss": 0.7665,
      "step": 13000
    },
    {
      "epoch": 2.0816,
      "grad_norm": 0.8228248953819275,
      "learning_rate": 6.160857908847185e-05,
      "loss": 0.7899,
      "step": 13010
    },
    {
      "epoch": 2.0832,
      "grad_norm": 1.0159599781036377,
      "learning_rate": 6.150134048257373e-05,
      "loss": 0.864,
      "step": 13020
    },
    {
      "epoch": 2.0848,
      "grad_norm": 0.8239181041717529,
      "learning_rate": 6.139410187667561e-05,
      "loss": 0.7382,
      "step": 13030
    },
    {
      "epoch": 2.0864,
      "grad_norm": 0.6706916689872742,
      "learning_rate": 6.128686327077749e-05,
      "loss": 0.859,
      "step": 13040
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.8124274611473083,
      "learning_rate": 6.117962466487937e-05,
      "loss": 0.8644,
      "step": 13050
    },
    {
      "epoch": 2.0896,
      "grad_norm": 0.6790260076522827,
      "learning_rate": 6.107238605898124e-05,
      "loss": 0.8081,
      "step": 13060
    },
    {
      "epoch": 2.0912,
      "grad_norm": 0.8665491938591003,
      "learning_rate": 6.096514745308311e-05,
      "loss": 0.8415,
      "step": 13070
    },
    {
      "epoch": 2.0928,
      "grad_norm": 0.702814519405365,
      "learning_rate": 6.085790884718499e-05,
      "loss": 0.7545,
      "step": 13080
    },
    {
      "epoch": 2.0944,
      "grad_norm": 1.0310105085372925,
      "learning_rate": 6.075067024128687e-05,
      "loss": 0.8116,
      "step": 13090
    },
    {
      "epoch": 2.096,
      "grad_norm": 1.2194428443908691,
      "learning_rate": 6.064343163538875e-05,
      "loss": 0.8731,
      "step": 13100
    },
    {
      "epoch": 2.0976,
      "grad_norm": 0.7506647109985352,
      "learning_rate": 6.053619302949062e-05,
      "loss": 0.8101,
      "step": 13110
    },
    {
      "epoch": 2.0992,
      "grad_norm": 1.2749701738357544,
      "learning_rate": 6.04289544235925e-05,
      "loss": 0.9026,
      "step": 13120
    },
    {
      "epoch": 2.1008,
      "grad_norm": 0.7024416327476501,
      "learning_rate": 6.032171581769437e-05,
      "loss": 0.8865,
      "step": 13130
    },
    {
      "epoch": 2.1024,
      "grad_norm": 1.0131523609161377,
      "learning_rate": 6.021447721179625e-05,
      "loss": 0.8917,
      "step": 13140
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.8741119503974915,
      "learning_rate": 6.0107238605898134e-05,
      "loss": 0.8249,
      "step": 13150
    },
    {
      "epoch": 2.1056,
      "grad_norm": 1.0337024927139282,
      "learning_rate": 6e-05,
      "loss": 0.8189,
      "step": 13160
    },
    {
      "epoch": 2.1072,
      "grad_norm": 1.0764153003692627,
      "learning_rate": 5.989276139410188e-05,
      "loss": 0.9341,
      "step": 13170
    },
    {
      "epoch": 2.1088,
      "grad_norm": 0.6920382976531982,
      "learning_rate": 5.978552278820375e-05,
      "loss": 0.8073,
      "step": 13180
    },
    {
      "epoch": 2.1104,
      "grad_norm": 0.7598079442977905,
      "learning_rate": 5.967828418230563e-05,
      "loss": 0.8829,
      "step": 13190
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.9078041315078735,
      "learning_rate": 5.957104557640751e-05,
      "loss": 0.8299,
      "step": 13200
    },
    {
      "epoch": 2.1136,
      "grad_norm": 0.8645558953285217,
      "learning_rate": 5.9463806970509386e-05,
      "loss": 0.841,
      "step": 13210
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.7370976805686951,
      "learning_rate": 5.935656836461127e-05,
      "loss": 0.8074,
      "step": 13220
    },
    {
      "epoch": 2.1168,
      "grad_norm": 0.8788979053497314,
      "learning_rate": 5.9249329758713135e-05,
      "loss": 0.766,
      "step": 13230
    },
    {
      "epoch": 2.1184,
      "grad_norm": 1.1354559659957886,
      "learning_rate": 5.9142091152815016e-05,
      "loss": 0.8256,
      "step": 13240
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.7900239825248718,
      "learning_rate": 5.90348525469169e-05,
      "loss": 0.7435,
      "step": 13250
    },
    {
      "epoch": 2.1216,
      "grad_norm": 0.6028889417648315,
      "learning_rate": 5.8927613941018765e-05,
      "loss": 0.7461,
      "step": 13260
    },
    {
      "epoch": 2.1232,
      "grad_norm": 0.830634593963623,
      "learning_rate": 5.8820375335120646e-05,
      "loss": 0.9087,
      "step": 13270
    },
    {
      "epoch": 2.1248,
      "grad_norm": 0.9022502303123474,
      "learning_rate": 5.871313672922252e-05,
      "loss": 0.7565,
      "step": 13280
    },
    {
      "epoch": 2.1264,
      "grad_norm": 1.1684043407440186,
      "learning_rate": 5.86058981233244e-05,
      "loss": 0.8511,
      "step": 13290
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.7994701266288757,
      "learning_rate": 5.849865951742628e-05,
      "loss": 0.8471,
      "step": 13300
    },
    {
      "epoch": 2.1296,
      "grad_norm": 1.10104238986969,
      "learning_rate": 5.839142091152815e-05,
      "loss": 0.7701,
      "step": 13310
    },
    {
      "epoch": 2.1312,
      "grad_norm": 0.6793085932731628,
      "learning_rate": 5.828418230563003e-05,
      "loss": 0.8548,
      "step": 13320
    },
    {
      "epoch": 2.1328,
      "grad_norm": 0.947797417640686,
      "learning_rate": 5.817694369973191e-05,
      "loss": 0.8411,
      "step": 13330
    },
    {
      "epoch": 2.1344,
      "grad_norm": 1.1265010833740234,
      "learning_rate": 5.806970509383378e-05,
      "loss": 0.8335,
      "step": 13340
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.7430242300033569,
      "learning_rate": 5.796246648793566e-05,
      "loss": 0.8644,
      "step": 13350
    },
    {
      "epoch": 2.1376,
      "grad_norm": 0.7469946146011353,
      "learning_rate": 5.7855227882037534e-05,
      "loss": 0.8985,
      "step": 13360
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 0.6783010959625244,
      "learning_rate": 5.7747989276139415e-05,
      "loss": 0.8781,
      "step": 13370
    },
    {
      "epoch": 2.1408,
      "grad_norm": 0.6589625477790833,
      "learning_rate": 5.7640750670241296e-05,
      "loss": 0.8111,
      "step": 13380
    },
    {
      "epoch": 2.1424,
      "grad_norm": 0.6501937508583069,
      "learning_rate": 5.7533512064343164e-05,
      "loss": 0.8186,
      "step": 13390
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.7616930603981018,
      "learning_rate": 5.7426273458445045e-05,
      "loss": 0.7084,
      "step": 13400
    },
    {
      "epoch": 2.1456,
      "grad_norm": 0.6681488156318665,
      "learning_rate": 5.731903485254691e-05,
      "loss": 0.8173,
      "step": 13410
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 0.797766923904419,
      "learning_rate": 5.7211796246648793e-05,
      "loss": 0.8773,
      "step": 13420
    },
    {
      "epoch": 2.1488,
      "grad_norm": 0.8621376752853394,
      "learning_rate": 5.7104557640750675e-05,
      "loss": 0.8757,
      "step": 13430
    },
    {
      "epoch": 2.1504,
      "grad_norm": 0.8373953700065613,
      "learning_rate": 5.699731903485255e-05,
      "loss": 0.8099,
      "step": 13440
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.6680001020431519,
      "learning_rate": 5.689008042895443e-05,
      "loss": 0.8458,
      "step": 13450
    },
    {
      "epoch": 2.1536,
      "grad_norm": 0.7425032258033752,
      "learning_rate": 5.67828418230563e-05,
      "loss": 0.8383,
      "step": 13460
    },
    {
      "epoch": 2.1552,
      "grad_norm": 1.0136644840240479,
      "learning_rate": 5.667560321715818e-05,
      "loss": 0.8485,
      "step": 13470
    },
    {
      "epoch": 2.1568,
      "grad_norm": 0.7916663289070129,
      "learning_rate": 5.656836461126006e-05,
      "loss": 0.8169,
      "step": 13480
    },
    {
      "epoch": 2.1584,
      "grad_norm": 1.2514876127243042,
      "learning_rate": 5.6461126005361934e-05,
      "loss": 0.8318,
      "step": 13490
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.8020888566970825,
      "learning_rate": 5.6353887399463815e-05,
      "loss": 0.8687,
      "step": 13500
    },
    {
      "epoch": 2.1616,
      "grad_norm": 1.021443486213684,
      "learning_rate": 5.624664879356568e-05,
      "loss": 0.8423,
      "step": 13510
    },
    {
      "epoch": 2.1632,
      "grad_norm": 1.1061954498291016,
      "learning_rate": 5.613941018766756e-05,
      "loss": 0.8367,
      "step": 13520
    },
    {
      "epoch": 2.1648,
      "grad_norm": 0.7971450090408325,
      "learning_rate": 5.6032171581769444e-05,
      "loss": 0.8093,
      "step": 13530
    },
    {
      "epoch": 2.1664,
      "grad_norm": 0.8365283012390137,
      "learning_rate": 5.592493297587131e-05,
      "loss": 0.8055,
      "step": 13540
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.8028196692466736,
      "learning_rate": 5.581769436997319e-05,
      "loss": 0.8153,
      "step": 13550
    },
    {
      "epoch": 2.1696,
      "grad_norm": 1.2448335886001587,
      "learning_rate": 5.571045576407507e-05,
      "loss": 0.8535,
      "step": 13560
    },
    {
      "epoch": 2.1712,
      "grad_norm": 0.7956322431564331,
      "learning_rate": 5.560321715817695e-05,
      "loss": 0.7948,
      "step": 13570
    },
    {
      "epoch": 2.1728,
      "grad_norm": 0.8863545656204224,
      "learning_rate": 5.549597855227883e-05,
      "loss": 0.8457,
      "step": 13580
    },
    {
      "epoch": 2.1744,
      "grad_norm": 1.041221022605896,
      "learning_rate": 5.53887399463807e-05,
      "loss": 0.7963,
      "step": 13590
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.7991377711296082,
      "learning_rate": 5.528150134048258e-05,
      "loss": 0.8426,
      "step": 13600
    },
    {
      "epoch": 2.1776,
      "grad_norm": 1.1436781883239746,
      "learning_rate": 5.5174262734584445e-05,
      "loss": 0.8338,
      "step": 13610
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.7919995784759521,
      "learning_rate": 5.5067024128686326e-05,
      "loss": 0.8231,
      "step": 13620
    },
    {
      "epoch": 2.1808,
      "grad_norm": 1.0260441303253174,
      "learning_rate": 5.495978552278821e-05,
      "loss": 0.8385,
      "step": 13630
    },
    {
      "epoch": 2.1824,
      "grad_norm": 0.8293560743331909,
      "learning_rate": 5.485254691689008e-05,
      "loss": 0.8556,
      "step": 13640
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.7066289782524109,
      "learning_rate": 5.474530831099196e-05,
      "loss": 0.8208,
      "step": 13650
    },
    {
      "epoch": 2.1856,
      "grad_norm": 0.9106187224388123,
      "learning_rate": 5.4648793565683654e-05,
      "loss": 0.8012,
      "step": 13660
    },
    {
      "epoch": 2.1872,
      "grad_norm": 0.808586061000824,
      "learning_rate": 5.454155495978552e-05,
      "loss": 0.798,
      "step": 13670
    },
    {
      "epoch": 2.1888,
      "grad_norm": 0.9748609662055969,
      "learning_rate": 5.44343163538874e-05,
      "loss": 0.8913,
      "step": 13680
    },
    {
      "epoch": 2.1904,
      "grad_norm": 0.8166178464889526,
      "learning_rate": 5.432707774798928e-05,
      "loss": 0.7905,
      "step": 13690
    },
    {
      "epoch": 2.192,
      "grad_norm": 1.0226166248321533,
      "learning_rate": 5.421983914209116e-05,
      "loss": 0.8552,
      "step": 13700
    },
    {
      "epoch": 2.1936,
      "grad_norm": 0.7781971096992493,
      "learning_rate": 5.411260053619304e-05,
      "loss": 0.7851,
      "step": 13710
    },
    {
      "epoch": 2.1952,
      "grad_norm": 0.8366127610206604,
      "learning_rate": 5.400536193029491e-05,
      "loss": 0.8294,
      "step": 13720
    },
    {
      "epoch": 2.1968,
      "grad_norm": 0.6774125695228577,
      "learning_rate": 5.389812332439679e-05,
      "loss": 0.8031,
      "step": 13730
    },
    {
      "epoch": 2.1984,
      "grad_norm": 0.7284771800041199,
      "learning_rate": 5.3790884718498655e-05,
      "loss": 0.825,
      "step": 13740
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.0784276723861694,
      "learning_rate": 5.3683646112600536e-05,
      "loss": 0.8364,
      "step": 13750
    },
    {
      "epoch": 2.2016,
      "grad_norm": 1.4411953687667847,
      "learning_rate": 5.357640750670242e-05,
      "loss": 0.7752,
      "step": 13760
    },
    {
      "epoch": 2.2032,
      "grad_norm": 0.8538468480110168,
      "learning_rate": 5.346916890080429e-05,
      "loss": 0.8229,
      "step": 13770
    },
    {
      "epoch": 2.2048,
      "grad_norm": 0.7314260601997375,
      "learning_rate": 5.336193029490617e-05,
      "loss": 0.8454,
      "step": 13780
    },
    {
      "epoch": 2.2064,
      "grad_norm": 1.0832151174545288,
      "learning_rate": 5.325469168900804e-05,
      "loss": 0.8269,
      "step": 13790
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.7738056182861328,
      "learning_rate": 5.314745308310992e-05,
      "loss": 0.8844,
      "step": 13800
    },
    {
      "epoch": 2.2096,
      "grad_norm": 0.820478081703186,
      "learning_rate": 5.30402144772118e-05,
      "loss": 0.829,
      "step": 13810
    },
    {
      "epoch": 2.2112,
      "grad_norm": 1.1069207191467285,
      "learning_rate": 5.293297587131367e-05,
      "loss": 0.8524,
      "step": 13820
    },
    {
      "epoch": 2.2128,
      "grad_norm": 0.9989071488380432,
      "learning_rate": 5.282573726541555e-05,
      "loss": 0.8172,
      "step": 13830
    },
    {
      "epoch": 2.2144,
      "grad_norm": 0.8499134182929993,
      "learning_rate": 5.2718498659517425e-05,
      "loss": 0.8466,
      "step": 13840
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.727942168712616,
      "learning_rate": 5.2611260053619306e-05,
      "loss": 0.8103,
      "step": 13850
    },
    {
      "epoch": 2.2176,
      "grad_norm": 0.8101792335510254,
      "learning_rate": 5.250402144772119e-05,
      "loss": 0.7731,
      "step": 13860
    },
    {
      "epoch": 2.2192,
      "grad_norm": 0.7348183989524841,
      "learning_rate": 5.2396782841823055e-05,
      "loss": 0.8309,
      "step": 13870
    },
    {
      "epoch": 2.2208,
      "grad_norm": 0.6983317136764526,
      "learning_rate": 5.2289544235924936e-05,
      "loss": 0.8367,
      "step": 13880
    },
    {
      "epoch": 2.2224,
      "grad_norm": 0.7957729697227478,
      "learning_rate": 5.218230563002682e-05,
      "loss": 0.7926,
      "step": 13890
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.9796537756919861,
      "learning_rate": 5.207506702412869e-05,
      "loss": 0.8463,
      "step": 13900
    },
    {
      "epoch": 2.2256,
      "grad_norm": 0.8723346590995789,
      "learning_rate": 5.196782841823057e-05,
      "loss": 0.8813,
      "step": 13910
    },
    {
      "epoch": 2.2272,
      "grad_norm": 0.970628559589386,
      "learning_rate": 5.186058981233244e-05,
      "loss": 0.9126,
      "step": 13920
    },
    {
      "epoch": 2.2288,
      "grad_norm": 0.8145798444747925,
      "learning_rate": 5.175335120643432e-05,
      "loss": 0.8264,
      "step": 13930
    },
    {
      "epoch": 2.2304,
      "grad_norm": 0.9178122282028198,
      "learning_rate": 5.16461126005362e-05,
      "loss": 0.8053,
      "step": 13940
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.8401444554328918,
      "learning_rate": 5.153887399463807e-05,
      "loss": 0.8131,
      "step": 13950
    },
    {
      "epoch": 2.2336,
      "grad_norm": 0.655012845993042,
      "learning_rate": 5.143163538873995e-05,
      "loss": 0.6918,
      "step": 13960
    },
    {
      "epoch": 2.2352,
      "grad_norm": 0.9622705578804016,
      "learning_rate": 5.1324396782841824e-05,
      "loss": 0.9058,
      "step": 13970
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.7899739742279053,
      "learning_rate": 5.1217158176943705e-05,
      "loss": 0.8372,
      "step": 13980
    },
    {
      "epoch": 2.2384,
      "grad_norm": 0.8887616395950317,
      "learning_rate": 5.1109919571045586e-05,
      "loss": 0.8382,
      "step": 13990
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.7223886251449585,
      "learning_rate": 5.1002680965147454e-05,
      "loss": 0.8834,
      "step": 14000
    },
    {
      "epoch": 2.2416,
      "grad_norm": 0.8936949372291565,
      "learning_rate": 5.0895442359249335e-05,
      "loss": 0.8679,
      "step": 14010
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.8101176023483276,
      "learning_rate": 5.07882037533512e-05,
      "loss": 0.9428,
      "step": 14020
    },
    {
      "epoch": 2.2448,
      "grad_norm": 0.7880266308784485,
      "learning_rate": 5.0680965147453084e-05,
      "loss": 0.7772,
      "step": 14030
    },
    {
      "epoch": 2.2464,
      "grad_norm": 1.0656782388687134,
      "learning_rate": 5.0573726541554965e-05,
      "loss": 0.8101,
      "step": 14040
    },
    {
      "epoch": 2.248,
      "grad_norm": 1.3215022087097168,
      "learning_rate": 5.046648793565684e-05,
      "loss": 0.8443,
      "step": 14050
    },
    {
      "epoch": 2.2496,
      "grad_norm": 0.7881143093109131,
      "learning_rate": 5.035924932975872e-05,
      "loss": 0.8269,
      "step": 14060
    },
    {
      "epoch": 2.2512,
      "grad_norm": 0.722632884979248,
      "learning_rate": 5.025201072386059e-05,
      "loss": 0.8761,
      "step": 14070
    },
    {
      "epoch": 2.2528,
      "grad_norm": 0.8409917950630188,
      "learning_rate": 5.014477211796247e-05,
      "loss": 0.7755,
      "step": 14080
    },
    {
      "epoch": 2.2544,
      "grad_norm": 1.103825330734253,
      "learning_rate": 5.003753351206435e-05,
      "loss": 0.8727,
      "step": 14090
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.8257770538330078,
      "learning_rate": 4.993029490616622e-05,
      "loss": 0.8451,
      "step": 14100
    },
    {
      "epoch": 2.2576,
      "grad_norm": 0.730223536491394,
      "learning_rate": 4.98230563002681e-05,
      "loss": 0.8653,
      "step": 14110
    },
    {
      "epoch": 2.2592,
      "grad_norm": 0.9169902801513672,
      "learning_rate": 4.971581769436998e-05,
      "loss": 0.9129,
      "step": 14120
    },
    {
      "epoch": 2.2608,
      "grad_norm": 1.001930594444275,
      "learning_rate": 4.960857908847185e-05,
      "loss": 0.823,
      "step": 14130
    },
    {
      "epoch": 2.2624,
      "grad_norm": 0.7318617105484009,
      "learning_rate": 4.950134048257373e-05,
      "loss": 0.8531,
      "step": 14140
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.7496247291564941,
      "learning_rate": 4.939410187667561e-05,
      "loss": 0.7928,
      "step": 14150
    },
    {
      "epoch": 2.2656,
      "grad_norm": 0.859277069568634,
      "learning_rate": 4.928686327077748e-05,
      "loss": 0.8584,
      "step": 14160
    },
    {
      "epoch": 2.2672,
      "grad_norm": 1.0216418504714966,
      "learning_rate": 4.917962466487936e-05,
      "loss": 0.908,
      "step": 14170
    },
    {
      "epoch": 2.2688,
      "grad_norm": 0.7988573908805847,
      "learning_rate": 4.907238605898123e-05,
      "loss": 0.7654,
      "step": 14180
    },
    {
      "epoch": 2.2704,
      "grad_norm": 0.951750636100769,
      "learning_rate": 4.896514745308311e-05,
      "loss": 0.8264,
      "step": 14190
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 1.2406095266342163,
      "learning_rate": 4.8857908847184994e-05,
      "loss": 0.8398,
      "step": 14200
    },
    {
      "epoch": 2.2736,
      "grad_norm": 0.8759526610374451,
      "learning_rate": 4.875067024128687e-05,
      "loss": 0.8714,
      "step": 14210
    },
    {
      "epoch": 2.2752,
      "grad_norm": 1.1156625747680664,
      "learning_rate": 4.864343163538874e-05,
      "loss": 0.8225,
      "step": 14220
    },
    {
      "epoch": 2.2768,
      "grad_norm": 0.9509252905845642,
      "learning_rate": 4.8536193029490616e-05,
      "loss": 0.7708,
      "step": 14230
    },
    {
      "epoch": 2.2784,
      "grad_norm": 1.355931043624878,
      "learning_rate": 4.842895442359249e-05,
      "loss": 0.923,
      "step": 14240
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.7982757687568665,
      "learning_rate": 4.832171581769437e-05,
      "loss": 0.847,
      "step": 14250
    },
    {
      "epoch": 2.2816,
      "grad_norm": 0.7214258313179016,
      "learning_rate": 4.821447721179625e-05,
      "loss": 0.8163,
      "step": 14260
    },
    {
      "epoch": 2.2832,
      "grad_norm": 0.8418561816215515,
      "learning_rate": 4.810723860589813e-05,
      "loss": 0.8001,
      "step": 14270
    },
    {
      "epoch": 2.2848,
      "grad_norm": 0.9978705048561096,
      "learning_rate": 4.8e-05,
      "loss": 0.7919,
      "step": 14280
    },
    {
      "epoch": 2.2864,
      "grad_norm": 0.7342694997787476,
      "learning_rate": 4.789276139410188e-05,
      "loss": 0.8145,
      "step": 14290
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.8325066566467285,
      "learning_rate": 4.7785522788203757e-05,
      "loss": 0.8149,
      "step": 14300
    },
    {
      "epoch": 2.2896,
      "grad_norm": 1.1281026601791382,
      "learning_rate": 4.767828418230563e-05,
      "loss": 0.7774,
      "step": 14310
    },
    {
      "epoch": 2.2912,
      "grad_norm": 0.7804362773895264,
      "learning_rate": 4.7571045576407505e-05,
      "loss": 0.7763,
      "step": 14320
    },
    {
      "epoch": 2.2928,
      "grad_norm": 0.7989362478256226,
      "learning_rate": 4.7463806970509386e-05,
      "loss": 0.8049,
      "step": 14330
    },
    {
      "epoch": 2.2944,
      "grad_norm": 0.6416993737220764,
      "learning_rate": 4.735656836461127e-05,
      "loss": 0.8727,
      "step": 14340
    },
    {
      "epoch": 2.296,
      "grad_norm": 1.4715698957443237,
      "learning_rate": 4.724932975871314e-05,
      "loss": 0.8193,
      "step": 14350
    },
    {
      "epoch": 2.2976,
      "grad_norm": 1.1009997129440308,
      "learning_rate": 4.7142091152815016e-05,
      "loss": 0.9193,
      "step": 14360
    },
    {
      "epoch": 2.2992,
      "grad_norm": 1.0277714729309082,
      "learning_rate": 4.703485254691689e-05,
      "loss": 0.8237,
      "step": 14370
    },
    {
      "epoch": 2.3008,
      "grad_norm": 1.198967456817627,
      "learning_rate": 4.6927613941018764e-05,
      "loss": 0.8472,
      "step": 14380
    },
    {
      "epoch": 2.3024,
      "grad_norm": 0.7612708210945129,
      "learning_rate": 4.6820375335120645e-05,
      "loss": 0.8647,
      "step": 14390
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.9510583281517029,
      "learning_rate": 4.6713136729222526e-05,
      "loss": 0.8227,
      "step": 14400
    },
    {
      "epoch": 2.3056,
      "grad_norm": 1.1101106405258179,
      "learning_rate": 4.66058981233244e-05,
      "loss": 0.8156,
      "step": 14410
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.8398374319076538,
      "learning_rate": 4.6498659517426275e-05,
      "loss": 0.9024,
      "step": 14420
    },
    {
      "epoch": 2.3088,
      "grad_norm": 0.9875405430793762,
      "learning_rate": 4.6391420911528156e-05,
      "loss": 0.8278,
      "step": 14430
    },
    {
      "epoch": 2.3104,
      "grad_norm": 0.9972812533378601,
      "learning_rate": 4.628418230563003e-05,
      "loss": 0.8769,
      "step": 14440
    },
    {
      "epoch": 2.312,
      "grad_norm": 1.1890157461166382,
      "learning_rate": 4.6176943699731904e-05,
      "loss": 0.7984,
      "step": 14450
    },
    {
      "epoch": 2.3136,
      "grad_norm": 0.6243973970413208,
      "learning_rate": 4.606970509383378e-05,
      "loss": 0.8199,
      "step": 14460
    },
    {
      "epoch": 2.3152,
      "grad_norm": 1.2988767623901367,
      "learning_rate": 4.596246648793566e-05,
      "loss": 0.8216,
      "step": 14470
    },
    {
      "epoch": 2.3168,
      "grad_norm": 0.9302882552146912,
      "learning_rate": 4.585522788203754e-05,
      "loss": 0.8704,
      "step": 14480
    },
    {
      "epoch": 2.3184,
      "grad_norm": 0.8893622159957886,
      "learning_rate": 4.5747989276139415e-05,
      "loss": 0.8657,
      "step": 14490
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.6261080503463745,
      "learning_rate": 4.564075067024129e-05,
      "loss": 0.8232,
      "step": 14500
    },
    {
      "epoch": 2.3216,
      "grad_norm": 0.839778482913971,
      "learning_rate": 4.5533512064343164e-05,
      "loss": 0.8537,
      "step": 14510
    },
    {
      "epoch": 2.3232,
      "grad_norm": 1.4330216646194458,
      "learning_rate": 4.542627345844504e-05,
      "loss": 0.8399,
      "step": 14520
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 0.5793083310127258,
      "learning_rate": 4.531903485254692e-05,
      "loss": 0.8248,
      "step": 14530
    },
    {
      "epoch": 2.3264,
      "grad_norm": 1.1596903800964355,
      "learning_rate": 4.521179624664879e-05,
      "loss": 0.8638,
      "step": 14540
    },
    {
      "epoch": 2.328,
      "grad_norm": 1.0252559185028076,
      "learning_rate": 4.5104557640750674e-05,
      "loss": 0.7831,
      "step": 14550
    },
    {
      "epoch": 2.3296,
      "grad_norm": 1.4799890518188477,
      "learning_rate": 4.499731903485255e-05,
      "loss": 0.8511,
      "step": 14560
    },
    {
      "epoch": 2.3312,
      "grad_norm": 0.8782053589820862,
      "learning_rate": 4.489008042895442e-05,
      "loss": 0.8467,
      "step": 14570
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 0.7829334139823914,
      "learning_rate": 4.4782841823056304e-05,
      "loss": 0.7642,
      "step": 14580
    },
    {
      "epoch": 2.3344,
      "grad_norm": 0.7397594451904297,
      "learning_rate": 4.467560321715818e-05,
      "loss": 0.8657,
      "step": 14590
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.7882485389709473,
      "learning_rate": 4.456836461126005e-05,
      "loss": 0.8096,
      "step": 14600
    },
    {
      "epoch": 2.3376,
      "grad_norm": 0.9367702007293701,
      "learning_rate": 4.4461126005361933e-05,
      "loss": 0.7694,
      "step": 14610
    },
    {
      "epoch": 2.3392,
      "grad_norm": 1.1006828546524048,
      "learning_rate": 4.4353887399463814e-05,
      "loss": 0.8656,
      "step": 14620
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 1.0139334201812744,
      "learning_rate": 4.424664879356569e-05,
      "loss": 0.7998,
      "step": 14630
    },
    {
      "epoch": 2.3424,
      "grad_norm": 0.9047491550445557,
      "learning_rate": 4.413941018766756e-05,
      "loss": 0.8707,
      "step": 14640
    },
    {
      "epoch": 2.344,
      "grad_norm": 1.0839918851852417,
      "learning_rate": 4.403217158176944e-05,
      "loss": 0.7788,
      "step": 14650
    },
    {
      "epoch": 2.3456,
      "grad_norm": 1.1283056735992432,
      "learning_rate": 4.392493297587131e-05,
      "loss": 0.8199,
      "step": 14660
    },
    {
      "epoch": 2.3472,
      "grad_norm": 0.8855826258659363,
      "learning_rate": 4.381769436997319e-05,
      "loss": 0.8182,
      "step": 14670
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 1.1495883464813232,
      "learning_rate": 4.371045576407507e-05,
      "loss": 0.8208,
      "step": 14680
    },
    {
      "epoch": 2.3504,
      "grad_norm": 0.9917513132095337,
      "learning_rate": 4.360321715817695e-05,
      "loss": 0.8638,
      "step": 14690
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.8318446278572083,
      "learning_rate": 4.349597855227882e-05,
      "loss": 0.8622,
      "step": 14700
    },
    {
      "epoch": 2.3536,
      "grad_norm": 0.8881087303161621,
      "learning_rate": 4.3388739946380696e-05,
      "loss": 0.8358,
      "step": 14710
    },
    {
      "epoch": 2.3552,
      "grad_norm": 0.6994739174842834,
      "learning_rate": 4.328150134048258e-05,
      "loss": 0.9389,
      "step": 14720
    },
    {
      "epoch": 2.3568,
      "grad_norm": 0.6943391561508179,
      "learning_rate": 4.317426273458445e-05,
      "loss": 0.8144,
      "step": 14730
    },
    {
      "epoch": 2.3584,
      "grad_norm": 1.134625792503357,
      "learning_rate": 4.3067024128686326e-05,
      "loss": 0.7928,
      "step": 14740
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.9975481033325195,
      "learning_rate": 4.295978552278821e-05,
      "loss": 0.8686,
      "step": 14750
    },
    {
      "epoch": 2.3616,
      "grad_norm": 0.7907272577285767,
      "learning_rate": 4.285254691689009e-05,
      "loss": 0.7845,
      "step": 14760
    },
    {
      "epoch": 2.3632,
      "grad_norm": 1.0268899202346802,
      "learning_rate": 4.274530831099196e-05,
      "loss": 0.7891,
      "step": 14770
    },
    {
      "epoch": 2.3648,
      "grad_norm": 0.773635983467102,
      "learning_rate": 4.263806970509384e-05,
      "loss": 0.7698,
      "step": 14780
    },
    {
      "epoch": 2.3664,
      "grad_norm": 1.333815336227417,
      "learning_rate": 4.253083109919571e-05,
      "loss": 0.7347,
      "step": 14790
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.7439146637916565,
      "learning_rate": 4.2423592493297585e-05,
      "loss": 0.764,
      "step": 14800
    },
    {
      "epoch": 2.3696,
      "grad_norm": 0.7623971700668335,
      "learning_rate": 4.2316353887399466e-05,
      "loss": 0.9072,
      "step": 14810
    },
    {
      "epoch": 2.3712,
      "grad_norm": 1.6407392024993896,
      "learning_rate": 4.220911528150134e-05,
      "loss": 0.8341,
      "step": 14820
    },
    {
      "epoch": 2.3728,
      "grad_norm": 0.736764132976532,
      "learning_rate": 4.210187667560322e-05,
      "loss": 0.7747,
      "step": 14830
    },
    {
      "epoch": 2.3744,
      "grad_norm": 0.8707314133644104,
      "learning_rate": 4.1994638069705096e-05,
      "loss": 0.8394,
      "step": 14840
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.7666900157928467,
      "learning_rate": 4.188739946380697e-05,
      "loss": 0.7927,
      "step": 14850
    },
    {
      "epoch": 2.3776,
      "grad_norm": 0.929828405380249,
      "learning_rate": 4.178016085790885e-05,
      "loss": 0.7577,
      "step": 14860
    },
    {
      "epoch": 2.3792,
      "grad_norm": 0.9284354448318481,
      "learning_rate": 4.1672922252010725e-05,
      "loss": 0.8135,
      "step": 14870
    },
    {
      "epoch": 2.3808,
      "grad_norm": 0.694873034954071,
      "learning_rate": 4.15656836461126e-05,
      "loss": 0.8021,
      "step": 14880
    },
    {
      "epoch": 2.3824,
      "grad_norm": 0.7794443368911743,
      "learning_rate": 4.145844504021448e-05,
      "loss": 0.7565,
      "step": 14890
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.8053529262542725,
      "learning_rate": 4.135120643431636e-05,
      "loss": 0.8535,
      "step": 14900
    },
    {
      "epoch": 2.3856,
      "grad_norm": 1.1246247291564941,
      "learning_rate": 4.1243967828418236e-05,
      "loss": 0.8245,
      "step": 14910
    },
    {
      "epoch": 2.3872,
      "grad_norm": 0.806413471698761,
      "learning_rate": 4.113672922252011e-05,
      "loss": 0.8239,
      "step": 14920
    },
    {
      "epoch": 2.3888,
      "grad_norm": 0.7145475745201111,
      "learning_rate": 4.1029490616621985e-05,
      "loss": 0.8127,
      "step": 14930
    },
    {
      "epoch": 2.3904,
      "grad_norm": 1.4027513265609741,
      "learning_rate": 4.092225201072386e-05,
      "loss": 0.8516,
      "step": 14940
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.8413079380989075,
      "learning_rate": 4.081501340482574e-05,
      "loss": 0.8533,
      "step": 14950
    },
    {
      "epoch": 2.3936,
      "grad_norm": 0.6781667470932007,
      "learning_rate": 4.0707774798927614e-05,
      "loss": 0.8475,
      "step": 14960
    },
    {
      "epoch": 2.3952,
      "grad_norm": 0.7704837322235107,
      "learning_rate": 4.0600536193029495e-05,
      "loss": 0.789,
      "step": 14970
    },
    {
      "epoch": 2.3968,
      "grad_norm": 0.9723442792892456,
      "learning_rate": 4.049329758713137e-05,
      "loss": 0.8274,
      "step": 14980
    },
    {
      "epoch": 2.3984,
      "grad_norm": 1.3244991302490234,
      "learning_rate": 4.0386058981233244e-05,
      "loss": 0.7957,
      "step": 14990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.7499590516090393,
      "learning_rate": 4.0278820375335125e-05,
      "loss": 0.8406,
      "step": 15000
    },
    {
      "epoch": 2.4016,
      "grad_norm": 1.421767234802246,
      "learning_rate": 4.0171581769437e-05,
      "loss": 0.7984,
      "step": 15010
    },
    {
      "epoch": 2.4032,
      "grad_norm": 1.3265196084976196,
      "learning_rate": 4.006434316353887e-05,
      "loss": 0.8127,
      "step": 15020
    },
    {
      "epoch": 2.4048,
      "grad_norm": 0.9084720611572266,
      "learning_rate": 3.9957104557640754e-05,
      "loss": 0.7972,
      "step": 15030
    },
    {
      "epoch": 2.4064,
      "grad_norm": 0.6288601160049438,
      "learning_rate": 3.984986595174263e-05,
      "loss": 0.7897,
      "step": 15040
    },
    {
      "epoch": 2.408,
      "grad_norm": 1.0832703113555908,
      "learning_rate": 3.974262734584451e-05,
      "loss": 0.8098,
      "step": 15050
    },
    {
      "epoch": 2.4096,
      "grad_norm": 0.8685666918754578,
      "learning_rate": 3.9635388739946384e-05,
      "loss": 0.7957,
      "step": 15060
    },
    {
      "epoch": 2.4112,
      "grad_norm": 0.8380210995674133,
      "learning_rate": 3.952815013404826e-05,
      "loss": 0.8545,
      "step": 15070
    },
    {
      "epoch": 2.4128,
      "grad_norm": 0.7434290647506714,
      "learning_rate": 3.942091152815013e-05,
      "loss": 0.8287,
      "step": 15080
    },
    {
      "epoch": 2.4144,
      "grad_norm": 1.0863640308380127,
      "learning_rate": 3.9313672922252014e-05,
      "loss": 0.8458,
      "step": 15090
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.6220597624778748,
      "learning_rate": 3.920643431635389e-05,
      "loss": 0.8083,
      "step": 15100
    },
    {
      "epoch": 2.4176,
      "grad_norm": 0.8955385088920593,
      "learning_rate": 3.909919571045577e-05,
      "loss": 0.8545,
      "step": 15110
    },
    {
      "epoch": 2.4192,
      "grad_norm": 0.7877848148345947,
      "learning_rate": 3.899195710455764e-05,
      "loss": 0.8796,
      "step": 15120
    },
    {
      "epoch": 2.4208,
      "grad_norm": 0.8298577070236206,
      "learning_rate": 3.888471849865952e-05,
      "loss": 0.8724,
      "step": 15130
    },
    {
      "epoch": 2.4224,
      "grad_norm": 0.9332702159881592,
      "learning_rate": 3.87774798927614e-05,
      "loss": 0.7593,
      "step": 15140
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.9319168329238892,
      "learning_rate": 3.867024128686327e-05,
      "loss": 0.742,
      "step": 15150
    },
    {
      "epoch": 2.4256,
      "grad_norm": 0.7945131659507751,
      "learning_rate": 3.856300268096515e-05,
      "loss": 0.8344,
      "step": 15160
    },
    {
      "epoch": 2.4272,
      "grad_norm": 1.107114553451538,
      "learning_rate": 3.845576407506702e-05,
      "loss": 0.842,
      "step": 15170
    },
    {
      "epoch": 2.4288,
      "grad_norm": 0.6584388017654419,
      "learning_rate": 3.83485254691689e-05,
      "loss": 0.8154,
      "step": 15180
    },
    {
      "epoch": 2.4304,
      "grad_norm": 0.9178208112716675,
      "learning_rate": 3.824128686327078e-05,
      "loss": 0.8456,
      "step": 15190
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.7803821563720703,
      "learning_rate": 3.813404825737266e-05,
      "loss": 0.8372,
      "step": 15200
    },
    {
      "epoch": 2.4336,
      "grad_norm": 1.1825140714645386,
      "learning_rate": 3.802680965147453e-05,
      "loss": 0.7452,
      "step": 15210
    },
    {
      "epoch": 2.4352,
      "grad_norm": 1.0082526206970215,
      "learning_rate": 3.7919571045576406e-05,
      "loss": 0.7866,
      "step": 15220
    },
    {
      "epoch": 2.4368,
      "grad_norm": 0.8482801914215088,
      "learning_rate": 3.781233243967829e-05,
      "loss": 0.8641,
      "step": 15230
    },
    {
      "epoch": 2.4384,
      "grad_norm": 1.0812040567398071,
      "learning_rate": 3.770509383378016e-05,
      "loss": 0.883,
      "step": 15240
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.8164716362953186,
      "learning_rate": 3.759785522788204e-05,
      "loss": 0.8882,
      "step": 15250
    },
    {
      "epoch": 2.4416,
      "grad_norm": 1.4169944524765015,
      "learning_rate": 3.749061662198392e-05,
      "loss": 0.8249,
      "step": 15260
    },
    {
      "epoch": 2.4432,
      "grad_norm": 0.7648755311965942,
      "learning_rate": 3.738337801608579e-05,
      "loss": 0.7959,
      "step": 15270
    },
    {
      "epoch": 2.4448,
      "grad_norm": 0.773367166519165,
      "learning_rate": 3.727613941018767e-05,
      "loss": 0.8672,
      "step": 15280
    },
    {
      "epoch": 2.4464,
      "grad_norm": 1.0796886682510376,
      "learning_rate": 3.7168900804289546e-05,
      "loss": 0.7723,
      "step": 15290
    },
    {
      "epoch": 2.448,
      "grad_norm": 1.1051982641220093,
      "learning_rate": 3.706166219839142e-05,
      "loss": 0.8321,
      "step": 15300
    },
    {
      "epoch": 2.4496,
      "grad_norm": 0.8062421679496765,
      "learning_rate": 3.6954423592493295e-05,
      "loss": 0.7863,
      "step": 15310
    },
    {
      "epoch": 2.4512,
      "grad_norm": 0.6997774839401245,
      "learning_rate": 3.6847184986595176e-05,
      "loss": 0.8378,
      "step": 15320
    },
    {
      "epoch": 2.4528,
      "grad_norm": 0.7475248575210571,
      "learning_rate": 3.673994638069706e-05,
      "loss": 0.87,
      "step": 15330
    },
    {
      "epoch": 2.4544,
      "grad_norm": 0.665935754776001,
      "learning_rate": 3.663270777479893e-05,
      "loss": 0.793,
      "step": 15340
    },
    {
      "epoch": 2.456,
      "grad_norm": 1.2546919584274292,
      "learning_rate": 3.6525469168900805e-05,
      "loss": 0.9126,
      "step": 15350
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 0.9480689764022827,
      "learning_rate": 3.641823056300268e-05,
      "loss": 0.8201,
      "step": 15360
    },
    {
      "epoch": 2.4592,
      "grad_norm": 0.7776657342910767,
      "learning_rate": 3.631099195710456e-05,
      "loss": 0.8373,
      "step": 15370
    },
    {
      "epoch": 2.4608,
      "grad_norm": 0.85870760679245,
      "learning_rate": 3.6203753351206435e-05,
      "loss": 0.7864,
      "step": 15380
    },
    {
      "epoch": 2.4624,
      "grad_norm": 0.7698888182640076,
      "learning_rate": 3.6096514745308316e-05,
      "loss": 0.7858,
      "step": 15390
    },
    {
      "epoch": 2.464,
      "grad_norm": 1.0680873394012451,
      "learning_rate": 3.598927613941019e-05,
      "loss": 0.7533,
      "step": 15400
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 1.166480541229248,
      "learning_rate": 3.5882037533512065e-05,
      "loss": 0.8102,
      "step": 15410
    },
    {
      "epoch": 2.4672,
      "grad_norm": 0.939075231552124,
      "learning_rate": 3.5774798927613946e-05,
      "loss": 0.8538,
      "step": 15420
    },
    {
      "epoch": 2.4688,
      "grad_norm": 1.0383881330490112,
      "learning_rate": 3.566756032171582e-05,
      "loss": 0.8653,
      "step": 15430
    },
    {
      "epoch": 2.4704,
      "grad_norm": 0.8921603560447693,
      "learning_rate": 3.5560321715817694e-05,
      "loss": 0.8153,
      "step": 15440
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.8457823395729065,
      "learning_rate": 3.545308310991957e-05,
      "loss": 0.7859,
      "step": 15450
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 1.2175259590148926,
      "learning_rate": 3.534584450402145e-05,
      "loss": 0.8465,
      "step": 15460
    },
    {
      "epoch": 2.4752,
      "grad_norm": 0.9311419725418091,
      "learning_rate": 3.523860589812333e-05,
      "loss": 0.7848,
      "step": 15470
    },
    {
      "epoch": 2.4768,
      "grad_norm": 0.9301177263259888,
      "learning_rate": 3.5131367292225205e-05,
      "loss": 0.8258,
      "step": 15480
    },
    {
      "epoch": 2.4784,
      "grad_norm": 0.7081460952758789,
      "learning_rate": 3.502412868632708e-05,
      "loss": 0.7911,
      "step": 15490
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.0578619241714478,
      "learning_rate": 3.4916890080428953e-05,
      "loss": 0.8873,
      "step": 15500
    },
    {
      "epoch": 2.4816,
      "grad_norm": 0.605165421962738,
      "learning_rate": 3.4809651474530834e-05,
      "loss": 0.7987,
      "step": 15510
    },
    {
      "epoch": 2.4832,
      "grad_norm": 0.7327511310577393,
      "learning_rate": 3.470241286863271e-05,
      "loss": 0.7506,
      "step": 15520
    },
    {
      "epoch": 2.4848,
      "grad_norm": 0.7193056344985962,
      "learning_rate": 3.459517426273459e-05,
      "loss": 0.8558,
      "step": 15530
    },
    {
      "epoch": 2.4864,
      "grad_norm": 1.0113540887832642,
      "learning_rate": 3.4487935656836464e-05,
      "loss": 0.7903,
      "step": 15540
    },
    {
      "epoch": 2.488,
      "grad_norm": 1.0199668407440186,
      "learning_rate": 3.438069705093834e-05,
      "loss": 0.8364,
      "step": 15550
    },
    {
      "epoch": 2.4896,
      "grad_norm": 0.7287130951881409,
      "learning_rate": 3.427345844504022e-05,
      "loss": 0.8438,
      "step": 15560
    },
    {
      "epoch": 2.4912,
      "grad_norm": 0.9766309857368469,
      "learning_rate": 3.4166219839142094e-05,
      "loss": 0.7877,
      "step": 15570
    },
    {
      "epoch": 2.4928,
      "grad_norm": 0.7016355991363525,
      "learning_rate": 3.405898123324397e-05,
      "loss": 0.8571,
      "step": 15580
    },
    {
      "epoch": 2.4944,
      "grad_norm": 0.7395488023757935,
      "learning_rate": 3.395174262734584e-05,
      "loss": 0.83,
      "step": 15590
    },
    {
      "epoch": 2.496,
      "grad_norm": 1.0713599920272827,
      "learning_rate": 3.384450402144772e-05,
      "loss": 0.8113,
      "step": 15600
    },
    {
      "epoch": 2.4976,
      "grad_norm": 0.706335723400116,
      "learning_rate": 3.3737265415549604e-05,
      "loss": 0.9246,
      "step": 15610
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.8500580191612244,
      "learning_rate": 3.363002680965148e-05,
      "loss": 0.7784,
      "step": 15620
    },
    {
      "epoch": 2.5008,
      "grad_norm": 1.038711667060852,
      "learning_rate": 3.352278820375335e-05,
      "loss": 0.7704,
      "step": 15630
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 0.7977675199508667,
      "learning_rate": 3.341554959785523e-05,
      "loss": 0.8281,
      "step": 15640
    },
    {
      "epoch": 2.504,
      "grad_norm": 1.0638673305511475,
      "learning_rate": 3.33083109919571e-05,
      "loss": 0.8198,
      "step": 15650
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 0.7416402101516724,
      "learning_rate": 3.320107238605898e-05,
      "loss": 0.8098,
      "step": 15660
    },
    {
      "epoch": 2.5072,
      "grad_norm": 0.8880519270896912,
      "learning_rate": 3.309383378016086e-05,
      "loss": 0.8476,
      "step": 15670
    },
    {
      "epoch": 2.5088,
      "grad_norm": 0.9696807265281677,
      "learning_rate": 3.298659517426274e-05,
      "loss": 0.8289,
      "step": 15680
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 0.8509998321533203,
      "learning_rate": 3.287935656836461e-05,
      "loss": 0.8439,
      "step": 15690
    },
    {
      "epoch": 2.512,
      "grad_norm": 1.0232733488082886,
      "learning_rate": 3.277211796246649e-05,
      "loss": 0.8458,
      "step": 15700
    },
    {
      "epoch": 2.5136,
      "grad_norm": 0.9922228455543518,
      "learning_rate": 3.266487935656837e-05,
      "loss": 0.8345,
      "step": 15710
    },
    {
      "epoch": 2.5152,
      "grad_norm": 1.0063496828079224,
      "learning_rate": 3.255764075067024e-05,
      "loss": 0.8444,
      "step": 15720
    },
    {
      "epoch": 2.5168,
      "grad_norm": 1.035085916519165,
      "learning_rate": 3.2450402144772116e-05,
      "loss": 0.8378,
      "step": 15730
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 0.8196950554847717,
      "learning_rate": 3.2343163538874e-05,
      "loss": 0.7967,
      "step": 15740
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.975558340549469,
      "learning_rate": 3.223592493297588e-05,
      "loss": 0.827,
      "step": 15750
    },
    {
      "epoch": 2.5216,
      "grad_norm": 0.9026345014572144,
      "learning_rate": 3.212868632707775e-05,
      "loss": 0.8294,
      "step": 15760
    },
    {
      "epoch": 2.5232,
      "grad_norm": 0.9539042115211487,
      "learning_rate": 3.2021447721179626e-05,
      "loss": 0.7952,
      "step": 15770
    },
    {
      "epoch": 2.5248,
      "grad_norm": 0.7241846919059753,
      "learning_rate": 3.19142091152815e-05,
      "loss": 0.825,
      "step": 15780
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 0.7487494349479675,
      "learning_rate": 3.1806970509383375e-05,
      "loss": 0.9082,
      "step": 15790
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.7661206722259521,
      "learning_rate": 3.1699731903485256e-05,
      "loss": 0.8506,
      "step": 15800
    },
    {
      "epoch": 2.5296,
      "grad_norm": 0.6690381169319153,
      "learning_rate": 3.159249329758713e-05,
      "loss": 0.8478,
      "step": 15810
    },
    {
      "epoch": 2.5312,
      "grad_norm": 1.140853762626648,
      "learning_rate": 3.148525469168901e-05,
      "loss": 0.905,
      "step": 15820
    },
    {
      "epoch": 2.5328,
      "grad_norm": 0.768457293510437,
      "learning_rate": 3.1378016085790886e-05,
      "loss": 0.8422,
      "step": 15830
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 1.113046407699585,
      "learning_rate": 3.127077747989277e-05,
      "loss": 0.8628,
      "step": 15840
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.7998359203338623,
      "learning_rate": 3.116353887399464e-05,
      "loss": 0.8816,
      "step": 15850
    },
    {
      "epoch": 2.5376,
      "grad_norm": 1.1754499673843384,
      "learning_rate": 3.1056300268096515e-05,
      "loss": 0.7793,
      "step": 15860
    },
    {
      "epoch": 2.5392,
      "grad_norm": 0.9234809279441833,
      "learning_rate": 3.094906166219839e-05,
      "loss": 0.9355,
      "step": 15870
    },
    {
      "epoch": 2.5408,
      "grad_norm": 1.097946047782898,
      "learning_rate": 3.084182305630027e-05,
      "loss": 0.8994,
      "step": 15880
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 0.880011796951294,
      "learning_rate": 3.073458445040215e-05,
      "loss": 0.8037,
      "step": 15890
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.9956504106521606,
      "learning_rate": 3.0627345844504026e-05,
      "loss": 0.8444,
      "step": 15900
    },
    {
      "epoch": 2.5456,
      "grad_norm": 1.0746989250183105,
      "learning_rate": 3.05201072386059e-05,
      "loss": 0.7757,
      "step": 15910
    },
    {
      "epoch": 2.5472,
      "grad_norm": 0.8105619549751282,
      "learning_rate": 3.0412868632707774e-05,
      "loss": 0.8361,
      "step": 15920
    },
    {
      "epoch": 2.5488,
      "grad_norm": 0.698244035243988,
      "learning_rate": 3.0305630026809652e-05,
      "loss": 0.8675,
      "step": 15930
    },
    {
      "epoch": 2.5504,
      "grad_norm": 0.8262417316436768,
      "learning_rate": 3.0198391420911533e-05,
      "loss": 0.7551,
      "step": 15940
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.7418916821479797,
      "learning_rate": 3.0091152815013407e-05,
      "loss": 0.8317,
      "step": 15950
    },
    {
      "epoch": 2.5536,
      "grad_norm": 1.2971186637878418,
      "learning_rate": 2.998391420911528e-05,
      "loss": 0.8374,
      "step": 15960
    },
    {
      "epoch": 2.5552,
      "grad_norm": 0.7126308083534241,
      "learning_rate": 2.987667560321716e-05,
      "loss": 0.8892,
      "step": 15970
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.730808436870575,
      "learning_rate": 2.976943699731904e-05,
      "loss": 0.8368,
      "step": 15980
    },
    {
      "epoch": 2.5584,
      "grad_norm": 0.6931825280189514,
      "learning_rate": 2.9662198391420915e-05,
      "loss": 0.7717,
      "step": 15990
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.9011216759681702,
      "learning_rate": 2.955495978552279e-05,
      "loss": 0.8557,
      "step": 16000
    },
    {
      "epoch": 2.5616,
      "grad_norm": 1.085681676864624,
      "learning_rate": 2.9458445040214477e-05,
      "loss": 0.7884,
      "step": 16010
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.6848822236061096,
      "learning_rate": 2.935120643431635e-05,
      "loss": 0.7799,
      "step": 16020
    },
    {
      "epoch": 2.5648,
      "grad_norm": 0.6781551837921143,
      "learning_rate": 2.9243967828418232e-05,
      "loss": 0.8274,
      "step": 16030
    },
    {
      "epoch": 2.5664,
      "grad_norm": 0.8820388317108154,
      "learning_rate": 2.913672922252011e-05,
      "loss": 0.8164,
      "step": 16040
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.973796010017395,
      "learning_rate": 2.9029490616621984e-05,
      "loss": 0.8575,
      "step": 16050
    },
    {
      "epoch": 2.5696,
      "grad_norm": 0.770839273929596,
      "learning_rate": 2.8922252010723862e-05,
      "loss": 0.8259,
      "step": 16060
    },
    {
      "epoch": 2.5712,
      "grad_norm": 0.7838459610939026,
      "learning_rate": 2.8815013404825743e-05,
      "loss": 0.8368,
      "step": 16070
    },
    {
      "epoch": 2.5728,
      "grad_norm": 0.7033392190933228,
      "learning_rate": 2.8707774798927617e-05,
      "loss": 0.7705,
      "step": 16080
    },
    {
      "epoch": 2.5744,
      "grad_norm": 0.737540602684021,
      "learning_rate": 2.860053619302949e-05,
      "loss": 0.79,
      "step": 16090
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.6892895698547363,
      "learning_rate": 2.849329758713137e-05,
      "loss": 0.8091,
      "step": 16100
    },
    {
      "epoch": 2.5776,
      "grad_norm": 1.4191592931747437,
      "learning_rate": 2.8386058981233243e-05,
      "loss": 0.7657,
      "step": 16110
    },
    {
      "epoch": 2.5792,
      "grad_norm": 1.0402634143829346,
      "learning_rate": 2.8278820375335124e-05,
      "loss": 0.8407,
      "step": 16120
    },
    {
      "epoch": 2.5808,
      "grad_norm": 0.6314122080802917,
      "learning_rate": 2.8171581769437e-05,
      "loss": 0.8228,
      "step": 16130
    },
    {
      "epoch": 2.5824,
      "grad_norm": 0.7641972899436951,
      "learning_rate": 2.8064343163538876e-05,
      "loss": 0.8713,
      "step": 16140
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.8609880805015564,
      "learning_rate": 2.795710455764075e-05,
      "loss": 0.8102,
      "step": 16150
    },
    {
      "epoch": 2.5856,
      "grad_norm": 0.9271296858787537,
      "learning_rate": 2.7849865951742625e-05,
      "loss": 0.8581,
      "step": 16160
    },
    {
      "epoch": 2.5872,
      "grad_norm": 0.7782979607582092,
      "learning_rate": 2.7742627345844506e-05,
      "loss": 0.8333,
      "step": 16170
    },
    {
      "epoch": 2.5888,
      "grad_norm": 0.8954875469207764,
      "learning_rate": 2.7635388739946384e-05,
      "loss": 0.8237,
      "step": 16180
    },
    {
      "epoch": 2.5904,
      "grad_norm": 1.2456793785095215,
      "learning_rate": 2.7528150134048258e-05,
      "loss": 0.7879,
      "step": 16190
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.8275437355041504,
      "learning_rate": 2.7420911528150136e-05,
      "loss": 0.8091,
      "step": 16200
    },
    {
      "epoch": 2.5936,
      "grad_norm": 0.9093427062034607,
      "learning_rate": 2.731367292225201e-05,
      "loss": 0.8319,
      "step": 16210
    },
    {
      "epoch": 2.5952,
      "grad_norm": 0.9760783314704895,
      "learning_rate": 2.720643431635389e-05,
      "loss": 0.8086,
      "step": 16220
    },
    {
      "epoch": 2.5968,
      "grad_norm": 0.9830209016799927,
      "learning_rate": 2.7099195710455765e-05,
      "loss": 0.8483,
      "step": 16230
    },
    {
      "epoch": 2.5984,
      "grad_norm": 0.8605015277862549,
      "learning_rate": 2.6991957104557643e-05,
      "loss": 0.8225,
      "step": 16240
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.9085740447044373,
      "learning_rate": 2.6884718498659517e-05,
      "loss": 0.7651,
      "step": 16250
    },
    {
      "epoch": 2.6016,
      "grad_norm": 1.6603707075119019,
      "learning_rate": 2.6777479892761398e-05,
      "loss": 0.8437,
      "step": 16260
    },
    {
      "epoch": 2.6032,
      "grad_norm": 1.1981886625289917,
      "learning_rate": 2.6670241286863272e-05,
      "loss": 0.8176,
      "step": 16270
    },
    {
      "epoch": 2.6048,
      "grad_norm": 0.6062068939208984,
      "learning_rate": 2.656300268096515e-05,
      "loss": 0.7993,
      "step": 16280
    },
    {
      "epoch": 2.6064,
      "grad_norm": 0.8882300853729248,
      "learning_rate": 2.6455764075067024e-05,
      "loss": 0.8098,
      "step": 16290
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.8188220262527466,
      "learning_rate": 2.63485254691689e-05,
      "loss": 0.8968,
      "step": 16300
    },
    {
      "epoch": 2.6096,
      "grad_norm": 0.9596399664878845,
      "learning_rate": 2.624128686327078e-05,
      "loss": 0.8062,
      "step": 16310
    },
    {
      "epoch": 2.6112,
      "grad_norm": 0.8358851671218872,
      "learning_rate": 2.6134048257372657e-05,
      "loss": 0.8609,
      "step": 16320
    },
    {
      "epoch": 2.6128,
      "grad_norm": 0.6175050139427185,
      "learning_rate": 2.602680965147453e-05,
      "loss": 0.8461,
      "step": 16330
    },
    {
      "epoch": 2.6144,
      "grad_norm": 0.9570050239562988,
      "learning_rate": 2.5919571045576406e-05,
      "loss": 0.7881,
      "step": 16340
    },
    {
      "epoch": 2.616,
      "grad_norm": 1.2605334520339966,
      "learning_rate": 2.5812332439678283e-05,
      "loss": 0.8096,
      "step": 16350
    },
    {
      "epoch": 2.6176,
      "grad_norm": 0.8495574593544006,
      "learning_rate": 2.5705093833780164e-05,
      "loss": 0.8945,
      "step": 16360
    },
    {
      "epoch": 2.6192,
      "grad_norm": 0.7745574712753296,
      "learning_rate": 2.559785522788204e-05,
      "loss": 0.8117,
      "step": 16370
    },
    {
      "epoch": 2.6208,
      "grad_norm": 0.9112818837165833,
      "learning_rate": 2.5490616621983916e-05,
      "loss": 0.7836,
      "step": 16380
    },
    {
      "epoch": 2.6224,
      "grad_norm": 0.7165102362632751,
      "learning_rate": 2.538337801608579e-05,
      "loss": 0.8313,
      "step": 16390
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.87981116771698,
      "learning_rate": 2.5276139410187672e-05,
      "loss": 0.8429,
      "step": 16400
    },
    {
      "epoch": 2.6256,
      "grad_norm": 0.8819306492805481,
      "learning_rate": 2.5168900804289546e-05,
      "loss": 0.8436,
      "step": 16410
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.7183989882469177,
      "learning_rate": 2.5061662198391424e-05,
      "loss": 0.7023,
      "step": 16420
    },
    {
      "epoch": 2.6288,
      "grad_norm": 0.7577312588691711,
      "learning_rate": 2.4954423592493298e-05,
      "loss": 0.813,
      "step": 16430
    },
    {
      "epoch": 2.6304,
      "grad_norm": 1.102446436882019,
      "learning_rate": 2.4847184986595176e-05,
      "loss": 0.7902,
      "step": 16440
    },
    {
      "epoch": 2.632,
      "grad_norm": 1.4269288778305054,
      "learning_rate": 2.4739946380697053e-05,
      "loss": 0.8541,
      "step": 16450
    },
    {
      "epoch": 2.6336,
      "grad_norm": 0.9335132837295532,
      "learning_rate": 2.463270777479893e-05,
      "loss": 0.8471,
      "step": 16460
    },
    {
      "epoch": 2.6352,
      "grad_norm": 0.6800650358200073,
      "learning_rate": 2.4525469168900805e-05,
      "loss": 0.8205,
      "step": 16470
    },
    {
      "epoch": 2.6368,
      "grad_norm": 1.1077395677566528,
      "learning_rate": 2.441823056300268e-05,
      "loss": 0.8456,
      "step": 16480
    },
    {
      "epoch": 2.6384,
      "grad_norm": 0.8233312964439392,
      "learning_rate": 2.431099195710456e-05,
      "loss": 0.8263,
      "step": 16490
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.9034938812255859,
      "learning_rate": 2.4203753351206435e-05,
      "loss": 0.8053,
      "step": 16500
    },
    {
      "epoch": 2.6416,
      "grad_norm": 1.060760259628296,
      "learning_rate": 2.4096514745308312e-05,
      "loss": 0.8423,
      "step": 16510
    },
    {
      "epoch": 2.6432,
      "grad_norm": 0.68904048204422,
      "learning_rate": 2.3989276139410187e-05,
      "loss": 0.8466,
      "step": 16520
    },
    {
      "epoch": 2.6448,
      "grad_norm": 1.3858150243759155,
      "learning_rate": 2.3882037533512068e-05,
      "loss": 0.8608,
      "step": 16530
    },
    {
      "epoch": 2.6464,
      "grad_norm": 1.435986042022705,
      "learning_rate": 2.3774798927613942e-05,
      "loss": 0.8762,
      "step": 16540
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.960837721824646,
      "learning_rate": 2.3667560321715816e-05,
      "loss": 0.8067,
      "step": 16550
    },
    {
      "epoch": 2.6496,
      "grad_norm": 1.1571155786514282,
      "learning_rate": 2.3560321715817697e-05,
      "loss": 0.844,
      "step": 16560
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 0.7013430595397949,
      "learning_rate": 2.345308310991957e-05,
      "loss": 0.8758,
      "step": 16570
    },
    {
      "epoch": 2.6528,
      "grad_norm": 1.4204137325286865,
      "learning_rate": 2.334584450402145e-05,
      "loss": 0.8296,
      "step": 16580
    },
    {
      "epoch": 2.6544,
      "grad_norm": 0.9698011875152588,
      "learning_rate": 2.3238605898123324e-05,
      "loss": 0.9176,
      "step": 16590
    },
    {
      "epoch": 2.656,
      "grad_norm": 1.163568139076233,
      "learning_rate": 2.3131367292225205e-05,
      "loss": 0.7545,
      "step": 16600
    },
    {
      "epoch": 2.6576,
      "grad_norm": 1.346069097518921,
      "learning_rate": 2.302412868632708e-05,
      "loss": 0.8591,
      "step": 16610
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 1.4314130544662476,
      "learning_rate": 2.2916890080428953e-05,
      "loss": 0.8428,
      "step": 16620
    },
    {
      "epoch": 2.6608,
      "grad_norm": 1.2325348854064941,
      "learning_rate": 2.2809651474530834e-05,
      "loss": 0.8147,
      "step": 16630
    },
    {
      "epoch": 2.6624,
      "grad_norm": 0.8553740382194519,
      "learning_rate": 2.270241286863271e-05,
      "loss": 0.826,
      "step": 16640
    },
    {
      "epoch": 2.664,
      "grad_norm": 1.1398677825927734,
      "learning_rate": 2.2595174262734586e-05,
      "loss": 0.894,
      "step": 16650
    },
    {
      "epoch": 2.6656,
      "grad_norm": 0.7326717376708984,
      "learning_rate": 2.248793565683646e-05,
      "loss": 0.872,
      "step": 16660
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 0.8263944983482361,
      "learning_rate": 2.238069705093834e-05,
      "loss": 0.8605,
      "step": 16670
    },
    {
      "epoch": 2.6688,
      "grad_norm": 1.7582865953445435,
      "learning_rate": 2.2273458445040216e-05,
      "loss": 0.8553,
      "step": 16680
    },
    {
      "epoch": 2.6704,
      "grad_norm": 0.7000352144241333,
      "learning_rate": 2.216621983914209e-05,
      "loss": 0.8458,
      "step": 16690
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.8261405825614929,
      "learning_rate": 2.205898123324397e-05,
      "loss": 0.8164,
      "step": 16700
    },
    {
      "epoch": 2.6736,
      "grad_norm": 0.7424439191818237,
      "learning_rate": 2.1951742627345845e-05,
      "loss": 0.8123,
      "step": 16710
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 0.8375053405761719,
      "learning_rate": 2.1844504021447723e-05,
      "loss": 0.7728,
      "step": 16720
    },
    {
      "epoch": 2.6768,
      "grad_norm": 1.004429817199707,
      "learning_rate": 2.1737265415549597e-05,
      "loss": 0.8902,
      "step": 16730
    },
    {
      "epoch": 2.6784,
      "grad_norm": 0.8888317346572876,
      "learning_rate": 2.1630026809651478e-05,
      "loss": 0.7771,
      "step": 16740
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.8369461894035339,
      "learning_rate": 2.1522788203753352e-05,
      "loss": 0.8205,
      "step": 16750
    },
    {
      "epoch": 2.6816,
      "grad_norm": 0.7905020117759705,
      "learning_rate": 2.1415549597855227e-05,
      "loss": 0.8493,
      "step": 16760
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 2.093501329421997,
      "learning_rate": 2.1308310991957108e-05,
      "loss": 0.8826,
      "step": 16770
    },
    {
      "epoch": 2.6848,
      "grad_norm": 0.8859971761703491,
      "learning_rate": 2.1201072386058982e-05,
      "loss": 0.8391,
      "step": 16780
    },
    {
      "epoch": 2.6864,
      "grad_norm": 0.8348125219345093,
      "learning_rate": 2.109383378016086e-05,
      "loss": 0.8277,
      "step": 16790
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.8064655065536499,
      "learning_rate": 2.0986595174262734e-05,
      "loss": 0.773,
      "step": 16800
    },
    {
      "epoch": 2.6896,
      "grad_norm": 0.7396878600120544,
      "learning_rate": 2.0879356568364615e-05,
      "loss": 0.8128,
      "step": 16810
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 0.68007493019104,
      "learning_rate": 2.077211796246649e-05,
      "loss": 0.7829,
      "step": 16820
    },
    {
      "epoch": 2.6928,
      "grad_norm": 0.9109345078468323,
      "learning_rate": 2.0664879356568364e-05,
      "loss": 0.8546,
      "step": 16830
    },
    {
      "epoch": 2.6944,
      "grad_norm": 0.8408949375152588,
      "learning_rate": 2.055764075067024e-05,
      "loss": 0.8164,
      "step": 16840
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 1.2668635845184326,
      "learning_rate": 2.045040214477212e-05,
      "loss": 0.8706,
      "step": 16850
    },
    {
      "epoch": 2.6976,
      "grad_norm": 1.1268516778945923,
      "learning_rate": 2.0343163538873997e-05,
      "loss": 0.8175,
      "step": 16860
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 0.7671315670013428,
      "learning_rate": 2.023592493297587e-05,
      "loss": 0.873,
      "step": 16870
    },
    {
      "epoch": 2.7008,
      "grad_norm": 0.764748752117157,
      "learning_rate": 2.012868632707775e-05,
      "loss": 0.8186,
      "step": 16880
    },
    {
      "epoch": 2.7024,
      "grad_norm": 0.9249810576438904,
      "learning_rate": 2.0021447721179626e-05,
      "loss": 0.7895,
      "step": 16890
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 1.2326526641845703,
      "learning_rate": 1.99142091152815e-05,
      "loss": 0.7913,
      "step": 16900
    },
    {
      "epoch": 2.7056,
      "grad_norm": 0.7828360795974731,
      "learning_rate": 1.9806970509383378e-05,
      "loss": 0.8805,
      "step": 16910
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 0.7169192433357239,
      "learning_rate": 1.9699731903485256e-05,
      "loss": 0.8323,
      "step": 16920
    },
    {
      "epoch": 2.7088,
      "grad_norm": 1.4766658544540405,
      "learning_rate": 1.9592493297587133e-05,
      "loss": 0.8819,
      "step": 16930
    },
    {
      "epoch": 2.7104,
      "grad_norm": 0.8157843351364136,
      "learning_rate": 1.9485254691689008e-05,
      "loss": 0.8647,
      "step": 16940
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 1.1965337991714478,
      "learning_rate": 1.9378016085790885e-05,
      "loss": 0.686,
      "step": 16950
    },
    {
      "epoch": 2.7136,
      "grad_norm": 0.9759600758552551,
      "learning_rate": 1.9270777479892763e-05,
      "loss": 0.7979,
      "step": 16960
    },
    {
      "epoch": 2.7152,
      "grad_norm": 0.7515912055969238,
      "learning_rate": 1.9163538873994637e-05,
      "loss": 0.8005,
      "step": 16970
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.9640582799911499,
      "learning_rate": 1.9056300268096515e-05,
      "loss": 0.8541,
      "step": 16980
    },
    {
      "epoch": 2.7184,
      "grad_norm": 0.8364019989967346,
      "learning_rate": 1.8949061662198393e-05,
      "loss": 0.9101,
      "step": 16990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.9151764512062073,
      "learning_rate": 1.884182305630027e-05,
      "loss": 0.8007,
      "step": 17000
    },
    {
      "epoch": 2.7216,
      "grad_norm": 0.8617339134216309,
      "learning_rate": 1.8734584450402144e-05,
      "loss": 0.8794,
      "step": 17010
    },
    {
      "epoch": 2.7232,
      "grad_norm": 1.048357605934143,
      "learning_rate": 1.8627345844504022e-05,
      "loss": 0.8265,
      "step": 17020
    },
    {
      "epoch": 2.7248,
      "grad_norm": 0.6902079582214355,
      "learning_rate": 1.85201072386059e-05,
      "loss": 0.8483,
      "step": 17030
    },
    {
      "epoch": 2.7264,
      "grad_norm": 1.1187083721160889,
      "learning_rate": 1.8412868632707774e-05,
      "loss": 0.83,
      "step": 17040
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.814994215965271,
      "learning_rate": 1.830563002680965e-05,
      "loss": 0.8417,
      "step": 17050
    },
    {
      "epoch": 2.7296,
      "grad_norm": 0.8379233479499817,
      "learning_rate": 1.819839142091153e-05,
      "loss": 0.8666,
      "step": 17060
    },
    {
      "epoch": 2.7312,
      "grad_norm": 0.6901618242263794,
      "learning_rate": 1.8091152815013407e-05,
      "loss": 0.7755,
      "step": 17070
    },
    {
      "epoch": 2.7328,
      "grad_norm": 0.8518340587615967,
      "learning_rate": 1.798391420911528e-05,
      "loss": 0.8057,
      "step": 17080
    },
    {
      "epoch": 2.7344,
      "grad_norm": 0.9427911639213562,
      "learning_rate": 1.787667560321716e-05,
      "loss": 0.8337,
      "step": 17090
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 1.6845515966415405,
      "learning_rate": 1.7769436997319037e-05,
      "loss": 0.8481,
      "step": 17100
    },
    {
      "epoch": 2.7376,
      "grad_norm": 1.2751034498214722,
      "learning_rate": 1.766219839142091e-05,
      "loss": 0.8037,
      "step": 17110
    },
    {
      "epoch": 2.7392,
      "grad_norm": 0.8067668676376343,
      "learning_rate": 1.755495978552279e-05,
      "loss": 0.8324,
      "step": 17120
    },
    {
      "epoch": 2.7408,
      "grad_norm": 1.0584521293640137,
      "learning_rate": 1.7447721179624666e-05,
      "loss": 0.8298,
      "step": 17130
    },
    {
      "epoch": 2.7424,
      "grad_norm": 0.8648099899291992,
      "learning_rate": 1.7340482573726544e-05,
      "loss": 0.7446,
      "step": 17140
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.756364107131958,
      "learning_rate": 1.7233243967828418e-05,
      "loss": 0.912,
      "step": 17150
    },
    {
      "epoch": 2.7456,
      "grad_norm": 0.9294622540473938,
      "learning_rate": 1.7126005361930296e-05,
      "loss": 0.7648,
      "step": 17160
    },
    {
      "epoch": 2.7472,
      "grad_norm": 0.8363403081893921,
      "learning_rate": 1.7018766756032173e-05,
      "loss": 0.8966,
      "step": 17170
    },
    {
      "epoch": 2.7488,
      "grad_norm": 0.7412691712379456,
      "learning_rate": 1.6911528150134048e-05,
      "loss": 0.8377,
      "step": 17180
    },
    {
      "epoch": 2.7504,
      "grad_norm": 0.8499806523323059,
      "learning_rate": 1.6804289544235925e-05,
      "loss": 0.8101,
      "step": 17190
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.7504655122756958,
      "learning_rate": 1.6697050938337803e-05,
      "loss": 0.8256,
      "step": 17200
    },
    {
      "epoch": 2.7536,
      "grad_norm": 0.8389214873313904,
      "learning_rate": 1.658981233243968e-05,
      "loss": 0.9042,
      "step": 17210
    },
    {
      "epoch": 2.7552,
      "grad_norm": 1.078488826751709,
      "learning_rate": 1.6482573726541555e-05,
      "loss": 0.8042,
      "step": 17220
    },
    {
      "epoch": 2.7568,
      "grad_norm": 0.8128851056098938,
      "learning_rate": 1.6375335120643433e-05,
      "loss": 0.8028,
      "step": 17230
    },
    {
      "epoch": 2.7584,
      "grad_norm": 0.8146982789039612,
      "learning_rate": 1.626809651474531e-05,
      "loss": 0.8138,
      "step": 17240
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.7330443859100342,
      "learning_rate": 1.6160857908847184e-05,
      "loss": 0.8545,
      "step": 17250
    },
    {
      "epoch": 2.7616,
      "grad_norm": 0.8433508276939392,
      "learning_rate": 1.6053619302949062e-05,
      "loss": 0.7945,
      "step": 17260
    },
    {
      "epoch": 2.7632,
      "grad_norm": 0.8440481424331665,
      "learning_rate": 1.594638069705094e-05,
      "loss": 0.8609,
      "step": 17270
    },
    {
      "epoch": 2.7648,
      "grad_norm": 0.7090440392494202,
      "learning_rate": 1.5839142091152817e-05,
      "loss": 0.7737,
      "step": 17280
    },
    {
      "epoch": 2.7664,
      "grad_norm": 0.8244121670722961,
      "learning_rate": 1.5731903485254692e-05,
      "loss": 0.7776,
      "step": 17290
    },
    {
      "epoch": 2.768,
      "grad_norm": 1.0381518602371216,
      "learning_rate": 1.562466487935657e-05,
      "loss": 0.8826,
      "step": 17300
    },
    {
      "epoch": 2.7696,
      "grad_norm": 1.0196261405944824,
      "learning_rate": 1.5517426273458447e-05,
      "loss": 0.7469,
      "step": 17310
    },
    {
      "epoch": 2.7712,
      "grad_norm": 0.8954402804374695,
      "learning_rate": 1.541018766756032e-05,
      "loss": 0.7928,
      "step": 17320
    },
    {
      "epoch": 2.7728,
      "grad_norm": 0.93378746509552,
      "learning_rate": 1.53029490616622e-05,
      "loss": 0.8803,
      "step": 17330
    },
    {
      "epoch": 2.7744,
      "grad_norm": 0.7937716245651245,
      "learning_rate": 1.5195710455764075e-05,
      "loss": 0.8895,
      "step": 17340
    },
    {
      "epoch": 2.776,
      "grad_norm": 1.1438087224960327,
      "learning_rate": 1.5088471849865951e-05,
      "loss": 0.8332,
      "step": 17350
    },
    {
      "epoch": 2.7776,
      "grad_norm": 0.7589042782783508,
      "learning_rate": 1.4981233243967829e-05,
      "loss": 0.8507,
      "step": 17360
    },
    {
      "epoch": 2.7792,
      "grad_norm": 0.8305280208587646,
      "learning_rate": 1.4873994638069705e-05,
      "loss": 0.8351,
      "step": 17370
    },
    {
      "epoch": 2.7808,
      "grad_norm": 1.5027908086776733,
      "learning_rate": 1.4766756032171584e-05,
      "loss": 0.8308,
      "step": 17380
    },
    {
      "epoch": 2.7824,
      "grad_norm": 0.7356462478637695,
      "learning_rate": 1.4659517426273458e-05,
      "loss": 0.8229,
      "step": 17390
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.8113952875137329,
      "learning_rate": 1.4552278820375337e-05,
      "loss": 0.852,
      "step": 17400
    },
    {
      "epoch": 2.7856,
      "grad_norm": 0.9403207898139954,
      "learning_rate": 1.4445040214477212e-05,
      "loss": 0.7433,
      "step": 17410
    },
    {
      "epoch": 2.7872,
      "grad_norm": 0.9378685355186462,
      "learning_rate": 1.4337801608579088e-05,
      "loss": 0.7914,
      "step": 17420
    },
    {
      "epoch": 2.7888,
      "grad_norm": 0.8957958817481995,
      "learning_rate": 1.4230563002680965e-05,
      "loss": 0.8245,
      "step": 17430
    },
    {
      "epoch": 2.7904,
      "grad_norm": 0.7709562182426453,
      "learning_rate": 1.4123324396782841e-05,
      "loss": 0.8828,
      "step": 17440
    },
    {
      "epoch": 2.792,
      "grad_norm": 1.3119629621505737,
      "learning_rate": 1.401608579088472e-05,
      "loss": 0.8513,
      "step": 17450
    },
    {
      "epoch": 2.7936,
      "grad_norm": 0.9185745716094971,
      "learning_rate": 1.3908847184986595e-05,
      "loss": 0.8033,
      "step": 17460
    },
    {
      "epoch": 2.7952,
      "grad_norm": 1.036068320274353,
      "learning_rate": 1.3801608579088474e-05,
      "loss": 0.8566,
      "step": 17470
    },
    {
      "epoch": 2.7968,
      "grad_norm": 0.7335308790206909,
      "learning_rate": 1.3694369973190349e-05,
      "loss": 0.8773,
      "step": 17480
    },
    {
      "epoch": 2.7984,
      "grad_norm": 0.955628514289856,
      "learning_rate": 1.3587131367292225e-05,
      "loss": 0.8728,
      "step": 17490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.8457293510437012,
      "learning_rate": 1.3479892761394102e-05,
      "loss": 0.8547,
      "step": 17500
    },
    {
      "epoch": 2.8016,
      "grad_norm": 0.9145719408988953,
      "learning_rate": 1.3372654155495978e-05,
      "loss": 0.7752,
      "step": 17510
    },
    {
      "epoch": 2.8032,
      "grad_norm": 0.9260897040367126,
      "learning_rate": 1.3265415549597856e-05,
      "loss": 0.8188,
      "step": 17520
    },
    {
      "epoch": 2.8048,
      "grad_norm": 0.9832004904747009,
      "learning_rate": 1.3158176943699732e-05,
      "loss": 0.8396,
      "step": 17530
    },
    {
      "epoch": 2.8064,
      "grad_norm": 1.2670010328292847,
      "learning_rate": 1.3050938337801611e-05,
      "loss": 0.8285,
      "step": 17540
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.8083884119987488,
      "learning_rate": 1.2943699731903485e-05,
      "loss": 0.8207,
      "step": 17550
    },
    {
      "epoch": 2.8096,
      "grad_norm": 0.8129324316978455,
      "learning_rate": 1.2836461126005361e-05,
      "loss": 0.8037,
      "step": 17560
    },
    {
      "epoch": 2.8112,
      "grad_norm": 0.6909502148628235,
      "learning_rate": 1.2729222520107239e-05,
      "loss": 0.7755,
      "step": 17570
    },
    {
      "epoch": 2.8128,
      "grad_norm": 0.8262291550636292,
      "learning_rate": 1.2621983914209115e-05,
      "loss": 0.8969,
      "step": 17580
    },
    {
      "epoch": 2.8144,
      "grad_norm": 1.1816511154174805,
      "learning_rate": 1.2514745308310993e-05,
      "loss": 0.8262,
      "step": 17590
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.9173991084098816,
      "learning_rate": 1.2407506702412869e-05,
      "loss": 0.8758,
      "step": 17600
    },
    {
      "epoch": 2.8176,
      "grad_norm": 0.7783465385437012,
      "learning_rate": 1.2300268096514746e-05,
      "loss": 0.8291,
      "step": 17610
    },
    {
      "epoch": 2.8192,
      "grad_norm": 1.0362188816070557,
      "learning_rate": 1.2193029490616622e-05,
      "loss": 0.8433,
      "step": 17620
    },
    {
      "epoch": 2.8208,
      "grad_norm": 1.3133193254470825,
      "learning_rate": 1.20857908847185e-05,
      "loss": 0.8744,
      "step": 17630
    },
    {
      "epoch": 2.8224,
      "grad_norm": 0.9480674862861633,
      "learning_rate": 1.1978552278820376e-05,
      "loss": 0.8577,
      "step": 17640
    },
    {
      "epoch": 2.824,
      "grad_norm": 1.0534026622772217,
      "learning_rate": 1.1871313672922253e-05,
      "loss": 0.8326,
      "step": 17650
    },
    {
      "epoch": 2.8256,
      "grad_norm": 0.9490155577659607,
      "learning_rate": 1.176407506702413e-05,
      "loss": 0.7981,
      "step": 17660
    },
    {
      "epoch": 2.8272,
      "grad_norm": 0.7851027846336365,
      "learning_rate": 1.1656836461126005e-05,
      "loss": 0.7915,
      "step": 17670
    },
    {
      "epoch": 2.8288,
      "grad_norm": 1.0494418144226074,
      "learning_rate": 1.1549597855227883e-05,
      "loss": 0.8909,
      "step": 17680
    },
    {
      "epoch": 2.8304,
      "grad_norm": 1.4053388833999634,
      "learning_rate": 1.1442359249329759e-05,
      "loss": 0.8764,
      "step": 17690
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.6847619414329529,
      "learning_rate": 1.1335120643431637e-05,
      "loss": 0.8218,
      "step": 17700
    },
    {
      "epoch": 2.8336,
      "grad_norm": 0.8614007830619812,
      "learning_rate": 1.1227882037533513e-05,
      "loss": 0.8291,
      "step": 17710
    },
    {
      "epoch": 2.8352,
      "grad_norm": 0.8373935222625732,
      "learning_rate": 1.112064343163539e-05,
      "loss": 0.8243,
      "step": 17720
    },
    {
      "epoch": 2.8368,
      "grad_norm": 0.7317901849746704,
      "learning_rate": 1.1013404825737266e-05,
      "loss": 0.8336,
      "step": 17730
    },
    {
      "epoch": 2.8384,
      "grad_norm": 0.8356083631515503,
      "learning_rate": 1.0906166219839142e-05,
      "loss": 0.8307,
      "step": 17740
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.266021966934204,
      "learning_rate": 1.079892761394102e-05,
      "loss": 0.8804,
      "step": 17750
    },
    {
      "epoch": 2.8416,
      "grad_norm": 1.128645658493042,
      "learning_rate": 1.0691689008042896e-05,
      "loss": 0.89,
      "step": 17760
    },
    {
      "epoch": 2.8432,
      "grad_norm": 1.4521437883377075,
      "learning_rate": 1.0584450402144774e-05,
      "loss": 0.8324,
      "step": 17770
    },
    {
      "epoch": 2.8448,
      "grad_norm": 0.6528659462928772,
      "learning_rate": 1.047721179624665e-05,
      "loss": 0.782,
      "step": 17780
    },
    {
      "epoch": 2.8464,
      "grad_norm": 1.4946686029434204,
      "learning_rate": 1.0369973190348525e-05,
      "loss": 0.8518,
      "step": 17790
    },
    {
      "epoch": 2.848,
      "grad_norm": 1.4825325012207031,
      "learning_rate": 1.0262734584450401e-05,
      "loss": 0.8691,
      "step": 17800
    },
    {
      "epoch": 2.8496,
      "grad_norm": 0.961430549621582,
      "learning_rate": 1.0155495978552279e-05,
      "loss": 0.8283,
      "step": 17810
    },
    {
      "epoch": 2.8512,
      "grad_norm": 1.2561789751052856,
      "learning_rate": 1.0048257372654157e-05,
      "loss": 0.8073,
      "step": 17820
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 1.0971417427062988,
      "learning_rate": 9.941018766756033e-06,
      "loss": 0.8049,
      "step": 17830
    },
    {
      "epoch": 2.8544,
      "grad_norm": 0.8219673037528992,
      "learning_rate": 9.83378016085791e-06,
      "loss": 0.8484,
      "step": 17840
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.7914801836013794,
      "learning_rate": 9.726541554959786e-06,
      "loss": 0.8435,
      "step": 17850
    },
    {
      "epoch": 2.8576,
      "grad_norm": 1.016729474067688,
      "learning_rate": 9.619302949061662e-06,
      "loss": 0.802,
      "step": 17860
    },
    {
      "epoch": 2.8592,
      "grad_norm": 0.9466817378997803,
      "learning_rate": 9.512064343163538e-06,
      "loss": 0.8936,
      "step": 17870
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 1.4144893884658813,
      "learning_rate": 9.404825737265416e-06,
      "loss": 0.817,
      "step": 17880
    },
    {
      "epoch": 2.8624,
      "grad_norm": 1.432218313217163,
      "learning_rate": 9.297587131367294e-06,
      "loss": 0.8974,
      "step": 17890
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.8843219876289368,
      "learning_rate": 9.19034852546917e-06,
      "loss": 0.8751,
      "step": 17900
    },
    {
      "epoch": 2.8656,
      "grad_norm": 0.9683184027671814,
      "learning_rate": 9.083109919571047e-06,
      "loss": 0.8796,
      "step": 17910
    },
    {
      "epoch": 2.8672,
      "grad_norm": 1.201447606086731,
      "learning_rate": 8.975871313672923e-06,
      "loss": 0.7914,
      "step": 17920
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 0.9439380764961243,
      "learning_rate": 8.868632707774799e-06,
      "loss": 0.8262,
      "step": 17930
    },
    {
      "epoch": 2.8704,
      "grad_norm": 0.8959201574325562,
      "learning_rate": 8.761394101876675e-06,
      "loss": 0.9243,
      "step": 17940
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.807997465133667,
      "learning_rate": 8.654155495978553e-06,
      "loss": 0.7623,
      "step": 17950
    },
    {
      "epoch": 2.8736,
      "grad_norm": 0.8817066550254822,
      "learning_rate": 8.546916890080429e-06,
      "loss": 0.83,
      "step": 17960
    },
    {
      "epoch": 2.8752,
      "grad_norm": 0.784819483757019,
      "learning_rate": 8.439678284182306e-06,
      "loss": 0.8592,
      "step": 17970
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.9270519614219666,
      "learning_rate": 8.332439678284184e-06,
      "loss": 0.7955,
      "step": 17980
    },
    {
      "epoch": 2.8784,
      "grad_norm": 0.6338165402412415,
      "learning_rate": 8.225201072386058e-06,
      "loss": 0.8402,
      "step": 17990
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.7671624422073364,
      "learning_rate": 8.117962466487936e-06,
      "loss": 0.8078,
      "step": 18000
    },
    {
      "epoch": 2.8816,
      "grad_norm": 0.8410115242004395,
      "learning_rate": 8.010723860589812e-06,
      "loss": 0.7487,
      "step": 18010
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.9668225049972534,
      "learning_rate": 7.90348525469169e-06,
      "loss": 0.7642,
      "step": 18020
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 0.7541015148162842,
      "learning_rate": 7.796246648793565e-06,
      "loss": 0.8566,
      "step": 18030
    },
    {
      "epoch": 2.8864,
      "grad_norm": 0.6172282695770264,
      "learning_rate": 7.689008042895443e-06,
      "loss": 0.7062,
      "step": 18040
    },
    {
      "epoch": 2.888,
      "grad_norm": 1.2860466241836548,
      "learning_rate": 7.58176943699732e-06,
      "loss": 0.7405,
      "step": 18050
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 0.7383111119270325,
      "learning_rate": 7.474530831099195e-06,
      "loss": 0.7924,
      "step": 18060
    },
    {
      "epoch": 2.8912,
      "grad_norm": 0.7856690287590027,
      "learning_rate": 7.367292225201073e-06,
      "loss": 0.8646,
      "step": 18070
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 0.5883120894432068,
      "learning_rate": 7.2600536193029495e-06,
      "loss": 0.8691,
      "step": 18080
    },
    {
      "epoch": 2.8944,
      "grad_norm": 0.8840715289115906,
      "learning_rate": 7.152815013404826e-06,
      "loss": 0.8523,
      "step": 18090
    },
    {
      "epoch": 2.896,
      "grad_norm": 1.841251015663147,
      "learning_rate": 7.045576407506703e-06,
      "loss": 0.8021,
      "step": 18100
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 0.7968294024467468,
      "learning_rate": 6.93833780160858e-06,
      "loss": 0.7227,
      "step": 18110
    },
    {
      "epoch": 2.8992,
      "grad_norm": 0.9428586959838867,
      "learning_rate": 6.831099195710457e-06,
      "loss": 0.8578,
      "step": 18120
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 0.878627598285675,
      "learning_rate": 6.723860589812332e-06,
      "loss": 0.8349,
      "step": 18130
    },
    {
      "epoch": 2.9024,
      "grad_norm": 0.8612506985664368,
      "learning_rate": 6.616621983914209e-06,
      "loss": 0.8376,
      "step": 18140
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.8707629442214966,
      "learning_rate": 6.509383378016086e-06,
      "loss": 0.8869,
      "step": 18150
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 0.8290725946426392,
      "learning_rate": 6.402144772117963e-06,
      "loss": 0.9086,
      "step": 18160
    },
    {
      "epoch": 2.9072,
      "grad_norm": 0.9786818623542786,
      "learning_rate": 6.29490616621984e-06,
      "loss": 0.79,
      "step": 18170
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 0.8314878344535828,
      "learning_rate": 6.187667560321716e-06,
      "loss": 0.7283,
      "step": 18180
    },
    {
      "epoch": 2.9104,
      "grad_norm": 0.7725786566734314,
      "learning_rate": 6.080428954423593e-06,
      "loss": 0.8531,
      "step": 18190
    },
    {
      "epoch": 2.912,
      "grad_norm": 1.2414849996566772,
      "learning_rate": 5.9731903485254696e-06,
      "loss": 0.8889,
      "step": 18200
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 0.9089403748512268,
      "learning_rate": 5.8659517426273455e-06,
      "loss": 0.8655,
      "step": 18210
    },
    {
      "epoch": 2.9152,
      "grad_norm": 0.9653939008712769,
      "learning_rate": 5.758713136729222e-06,
      "loss": 0.807,
      "step": 18220
    },
    {
      "epoch": 2.9168,
      "grad_norm": 0.7945005893707275,
      "learning_rate": 5.651474530831099e-06,
      "loss": 0.7819,
      "step": 18230
    },
    {
      "epoch": 2.9184,
      "grad_norm": 0.6963180303573608,
      "learning_rate": 5.544235924932977e-06,
      "loss": 0.8571,
      "step": 18240
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.7540424466133118,
      "learning_rate": 5.436997319034853e-06,
      "loss": 0.8722,
      "step": 18250
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 0.9552098512649536,
      "learning_rate": 5.32975871313673e-06,
      "loss": 0.824,
      "step": 18260
    },
    {
      "epoch": 2.9232,
      "grad_norm": 0.9371440410614014,
      "learning_rate": 5.222520107238606e-06,
      "loss": 0.7675,
      "step": 18270
    },
    {
      "epoch": 2.9248,
      "grad_norm": 1.2121847867965698,
      "learning_rate": 5.115281501340482e-06,
      "loss": 0.8768,
      "step": 18280
    },
    {
      "epoch": 2.9264,
      "grad_norm": 0.8869804739952087,
      "learning_rate": 5.008042895442359e-06,
      "loss": 0.7233,
      "step": 18290
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.7246225476264954,
      "learning_rate": 4.900804289544236e-06,
      "loss": 0.8006,
      "step": 18300
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 0.6799787282943726,
      "learning_rate": 4.793565683646113e-06,
      "loss": 0.7788,
      "step": 18310
    },
    {
      "epoch": 2.9312,
      "grad_norm": 0.7241663932800293,
      "learning_rate": 4.68632707774799e-06,
      "loss": 0.7059,
      "step": 18320
    },
    {
      "epoch": 2.9328,
      "grad_norm": 0.948592483997345,
      "learning_rate": 4.579088471849866e-06,
      "loss": 0.9099,
      "step": 18330
    },
    {
      "epoch": 2.9344,
      "grad_norm": 0.772689938545227,
      "learning_rate": 4.471849865951743e-06,
      "loss": 0.7746,
      "step": 18340
    },
    {
      "epoch": 2.936,
      "grad_norm": 1.0345478057861328,
      "learning_rate": 4.364611260053619e-06,
      "loss": 0.8163,
      "step": 18350
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 0.8014178276062012,
      "learning_rate": 4.257372654155496e-06,
      "loss": 0.7612,
      "step": 18360
    },
    {
      "epoch": 2.9392,
      "grad_norm": 0.8399192690849304,
      "learning_rate": 4.150134048257373e-06,
      "loss": 0.8057,
      "step": 18370
    },
    {
      "epoch": 2.9408,
      "grad_norm": 0.7560436129570007,
      "learning_rate": 4.04289544235925e-06,
      "loss": 0.7786,
      "step": 18380
    },
    {
      "epoch": 2.9424,
      "grad_norm": 0.9534783363342285,
      "learning_rate": 3.935656836461126e-06,
      "loss": 0.8495,
      "step": 18390
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.714599072933197,
      "learning_rate": 3.828418230563003e-06,
      "loss": 0.8034,
      "step": 18400
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 0.8860177397727966,
      "learning_rate": 3.72117962466488e-06,
      "loss": 0.8299,
      "step": 18410
    },
    {
      "epoch": 2.9472,
      "grad_norm": 1.1536259651184082,
      "learning_rate": 3.613941018766756e-06,
      "loss": 0.8514,
      "step": 18420
    },
    {
      "epoch": 2.9488,
      "grad_norm": 0.71417236328125,
      "learning_rate": 3.506702412868633e-06,
      "loss": 0.7217,
      "step": 18430
    },
    {
      "epoch": 2.9504,
      "grad_norm": 0.7401315569877625,
      "learning_rate": 3.3994638069705096e-06,
      "loss": 0.8486,
      "step": 18440
    },
    {
      "epoch": 2.952,
      "grad_norm": 1.3920503854751587,
      "learning_rate": 3.292225201072386e-06,
      "loss": 0.8046,
      "step": 18450
    },
    {
      "epoch": 2.9536,
      "grad_norm": 0.9306731224060059,
      "learning_rate": 3.184986595174263e-06,
      "loss": 0.8191,
      "step": 18460
    },
    {
      "epoch": 2.9552,
      "grad_norm": 0.8219221830368042,
      "learning_rate": 3.077747989276139e-06,
      "loss": 0.9236,
      "step": 18470
    },
    {
      "epoch": 2.9568,
      "grad_norm": 1.0326201915740967,
      "learning_rate": 2.9705093833780164e-06,
      "loss": 0.8713,
      "step": 18480
    },
    {
      "epoch": 2.9584,
      "grad_norm": 0.7419687509536743,
      "learning_rate": 2.863270777479893e-06,
      "loss": 0.8855,
      "step": 18490
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.2181212902069092,
      "learning_rate": 2.7560321715817696e-06,
      "loss": 0.8575,
      "step": 18500
    },
    {
      "epoch": 2.9616,
      "grad_norm": 0.9912129044532776,
      "learning_rate": 2.648793565683646e-06,
      "loss": 0.8684,
      "step": 18510
    },
    {
      "epoch": 2.9632,
      "grad_norm": 0.8786696791648865,
      "learning_rate": 2.541554959785523e-06,
      "loss": 0.7962,
      "step": 18520
    },
    {
      "epoch": 2.9648,
      "grad_norm": 1.458114743232727,
      "learning_rate": 2.4343163538873996e-06,
      "loss": 0.8424,
      "step": 18530
    },
    {
      "epoch": 2.9664,
      "grad_norm": 0.6819051504135132,
      "learning_rate": 2.327077747989276e-06,
      "loss": 0.7684,
      "step": 18540
    },
    {
      "epoch": 2.968,
      "grad_norm": 1.031994104385376,
      "learning_rate": 2.219839142091153e-06,
      "loss": 0.8815,
      "step": 18550
    },
    {
      "epoch": 2.9696,
      "grad_norm": 0.8645150661468506,
      "learning_rate": 2.1126005361930296e-06,
      "loss": 0.7643,
      "step": 18560
    },
    {
      "epoch": 2.9712,
      "grad_norm": 1.2809308767318726,
      "learning_rate": 2.0053619302949065e-06,
      "loss": 0.8186,
      "step": 18570
    },
    {
      "epoch": 2.9728,
      "grad_norm": 0.7137756943702698,
      "learning_rate": 1.898123324396783e-06,
      "loss": 0.8145,
      "step": 18580
    },
    {
      "epoch": 2.9744,
      "grad_norm": 0.712587296962738,
      "learning_rate": 1.7908847184986594e-06,
      "loss": 0.8236,
      "step": 18590
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.9496935606002808,
      "learning_rate": 1.6836461126005365e-06,
      "loss": 0.9094,
      "step": 18600
    },
    {
      "epoch": 2.9776,
      "grad_norm": 1.2480974197387695,
      "learning_rate": 1.5764075067024128e-06,
      "loss": 0.8831,
      "step": 18610
    },
    {
      "epoch": 2.9792,
      "grad_norm": 0.8297780752182007,
      "learning_rate": 1.4691689008042897e-06,
      "loss": 0.8071,
      "step": 18620
    },
    {
      "epoch": 2.9808,
      "grad_norm": 0.8384127020835876,
      "learning_rate": 1.3619302949061663e-06,
      "loss": 0.684,
      "step": 18630
    },
    {
      "epoch": 2.9824,
      "grad_norm": 0.9778443574905396,
      "learning_rate": 1.254691689008043e-06,
      "loss": 0.8688,
      "step": 18640
    },
    {
      "epoch": 2.984,
      "grad_norm": 1.0186635255813599,
      "learning_rate": 1.1474530831099197e-06,
      "loss": 0.8314,
      "step": 18650
    },
    {
      "epoch": 2.9856,
      "grad_norm": 0.79931640625,
      "learning_rate": 1.0402144772117963e-06,
      "loss": 0.8975,
      "step": 18660
    },
    {
      "epoch": 2.9872,
      "grad_norm": 0.9989880323410034,
      "learning_rate": 9.32975871313673e-07,
      "loss": 0.6905,
      "step": 18670
    },
    {
      "epoch": 2.9888,
      "grad_norm": 0.7821051478385925,
      "learning_rate": 8.257372654155497e-07,
      "loss": 0.79,
      "step": 18680
    },
    {
      "epoch": 2.9904,
      "grad_norm": 0.8837521076202393,
      "learning_rate": 7.184986595174264e-07,
      "loss": 0.8503,
      "step": 18690
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.9019056558609009,
      "learning_rate": 6.11260053619303e-07,
      "loss": 0.8355,
      "step": 18700
    },
    {
      "epoch": 2.9936,
      "grad_norm": 1.032905101776123,
      "learning_rate": 5.040214477211797e-07,
      "loss": 0.8394,
      "step": 18710
    },
    {
      "epoch": 2.9952,
      "grad_norm": 1.2687710523605347,
      "learning_rate": 3.9678284182305627e-07,
      "loss": 0.8504,
      "step": 18720
    },
    {
      "epoch": 2.9968,
      "grad_norm": 0.8432801961898804,
      "learning_rate": 2.8954423592493297e-07,
      "loss": 0.7711,
      "step": 18730
    },
    {
      "epoch": 2.9984,
      "grad_norm": 0.7947618365287781,
      "learning_rate": 1.8230563002680968e-07,
      "loss": 0.7943,
      "step": 18740
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.9294969439506531,
      "learning_rate": 7.506702412868633e-08,
      "loss": 0.7994,
      "step": 18750
    }
  ],
  "logging_steps": 10,
  "max_steps": 18750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0534961171375309e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
