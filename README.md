# DL Finetuning GPT

Un proyecto de fine-tuning del modelo TinyLlama especializado en contenido gaming utilizando LoRA (Low-Rank Adaptation).

## ğŸ“‹ DescripciÃ³n

Este repositorio contiene la implementaciÃ³n de un modelo de lenguaje optimizado para el dominio gaming, basado en TinyLlama-1.1B y entrenado con tÃ©cnicas de LoRA para eficiencia computacional.

## ğŸ® PresentaciÃ³n del Proyecto

Puedes ver la presentaciÃ³n completa del proyecto aquÃ­:
[Ver PresentaciÃ³n en Canva](https://www.canva.com/design/DAG5qjyqC4I/QeI0sOQymluWi2aIfQEB_g/view?utm_content=DAG5qjyqC4I&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h7f6edfbebf)

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ Docs/                            #Documentos de investigaciÃ³n 
â”œâ”€â”€ models/
â”‚   â””â”€â”€ tinyllama-gaming-1b-lora/    # Modelo fine-tuned con adaptadores LoRA
â”œâ”€â”€ notebooks/                        # Jupyter notebooks para experimentaciÃ³n
â”œâ”€â”€ utils/                           # Utilidades y funciones auxiliares
â”œâ”€â”€ .gitattributes                   # ConfiguraciÃ³n de atributos de Git
â”œâ”€â”€ .gitignore                       # Archivos ignorados por Git
â”œâ”€â”€ README.md                        # Este archivo
â”œâ”€â”€ app.py                          # AplicaciÃ³n principal
â””â”€â”€ requirements.txt                # Dependencias del proyecto
```

## ğŸš€ InstalaciÃ³n

1. Clona el repositorio:
```bash
git clone <url-del-repositorio>
cd <nombre-del-repositorio>
```

2. Instala las dependencias:
```bash
pip install -r requirements.txt
```

## ğŸ’» Uso

Ejecuta la aplicaciÃ³n principal:
```bash
python app.py
```

## ğŸ”§ CaracterÃ­sticas

- âœ… Fine-tuning con LoRA para eficiencia de memoria
- âœ… EspecializaciÃ³n en contenido gaming con reviews
- âœ… Modelo base TinyLlama-1.1B
- âœ… Interfaz interactiva mediante Gradio para despliegue en HF Spaces

## ğŸ“Š Notebooks

Los notebooks incluidos permiten:
- ExploraciÃ³n de datos
- Proceso de fine-tuning
- EvaluaciÃ³n del modelo
- ExperimentaciÃ³n con hiperparÃ¡metros

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- Python
- Transformers (Hugging Face)
- PEFT (Parameter-Efficient Fine-Tuning)
- LoRA (Low-Rank Adaptation)
- PyTorch

## ğŸ“ Requisitos

Ver `requirements.txt` para la lista completa de dependencias.

## ğŸ‘¥ Autores

- Juan Camilo NiÃ±o
- NicolÃ¡s Acevedo
- SimÃ³n Porras Villalobos

â­ Si este proyecto te resulta Ãºtil, considera darle una estrella en GitHub