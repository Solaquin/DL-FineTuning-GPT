{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, pipeline\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a451515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Configuración del modelo\n",
    "# -----------------------------\n",
    "\n",
    "# Posibles modelos\n",
    "#MODEL_NAME = \"meta-llama/Llama-3.1-8B\"  # si no cabe, cambio a 3B\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"models/llama2-gaming-3b-lora\"\n",
    "\n",
    "#MODEL_NAME = \"distilgpt2\"\n",
    "#OUTPUT_DIR = \"models/distilgpt2-gaming-lora\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Cargando modelo base...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datasets\n",
    "\n",
    "DATASET_PATH = \"datasets/train_chats.jsonl\"\n",
    "VAL_PATH = \"datasets/val_chats.jsonl\"\n",
    "\n",
    "\n",
    "train_ds = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "val_ds = load_dataset(\"json\", data_files=VAL_PATH, split=\"train\")\n",
    "\n",
    "print(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43975227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Activar QLoRA / LoRA ----------> NO ES NECESARIO PARA DISTILGPT2 - SOLO MODELOS GRANDES +3B\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat(example):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that recommends video games based on the user's tastes.\"},\n",
    "        {\"role\": \"user\", \"content\": example[\"User\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"Assistant\"]},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    tokenized = tokenizer(prompt, truncation=True)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_ds.map(\n",
    "    format_chat,\n",
    "    batched=False,\n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "val_tokenized = val_ds.map(\n",
    "    format_chat,\n",
    "    batched=False,\n",
    "    remove_columns=val_ds.column_names\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Submuestreo para entrenamiento rápido\n",
    "# -----------------------------\n",
    "\n",
    "# Barajamos y nos quedamos con una muestra pequeña\n",
    "max_train_samples = 50000   \n",
    "\n",
    "train_small = train_tokenized.shuffle(seed=42)\n",
    "\n",
    "if len(train_small) > max_train_samples:\n",
    "    train_small = train_small.select(range(max_train_samples))\n",
    "\n",
    "print(\"Tamaño train_small:\", len(train_small))\n",
    "\n",
    "train_small.set_format(\"torch\")\n",
    "val_tokenized.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def build_training_args_safe(output_dir=\"./outputs\", use_cuda=True):\n",
    "    # Diccionario con las opciones que normalmente usamos\n",
    "    base_kwargs = {\n",
    "        \"output_dir\": output_dir,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        # Force fp16 when CUDA is available to match model dtype if the model was loaded in float16.\n",
    "        # This avoids mismatches that can cause errors like \"Attempting to unscale FP16 gradients.\"\n",
    "        \"fp16\": bool(torch.cuda.is_available()),\n",
    "        \"logging_steps\": 10,\n",
    "        \"save_steps\": 200,\n",
    "        \"save_total_limit\": 2,\n",
    "        # \"evaluation_strategy\": \"no\",     \n",
    "        # \"evaluation_strategy\": \"steps\",  \n",
    "        # \"eval_steps\": 500,\n",
    "        \"report_to\": \"none\",\n",
    "        # \"load_best_model_at_end\": True,   \n",
    "        # \"metric_for_best_model\": \"loss\",\n",
    "        # \"greater_is_better\": False,\n",
    "    }\n",
    "\n",
    "    # Filtramos según la firma de TrainingArguments\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    valid_keys = set(sig.parameters.keys())\n",
    "\n",
    "    filtered = {k: v for k, v in base_kwargs.items() if k in valid_keys}\n",
    "\n",
    "    print(\"\\n[DEBUG] Claves aceptadas por TrainingArguments en este entorno:\")\n",
    "    for k in sorted(filtered.keys()):\n",
    "        print(\" -\", k)\n",
    "\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "# Detectar si CUDA disponible\n",
    "use_cuda = torch.cuda.is_available() if 'torch' in globals() else False\n",
    "\n",
    "# Construir training_args de forma segura\n",
    "training_args = build_training_args_safe(output_dir=OUTPUT_DIR, use_cuda=use_cuda)\n",
    "\n",
    "train_dataset = train_small\n",
    "eval_dataset = val_tokenized \n",
    "\n",
    "# Crear Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Trainer creado. Listo para entrenar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63df8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Entrenamiento\n",
    "# -----------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b82110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Guardar modelo\n",
    "# -----------------------------\n",
    "print(\"\\nGuardando modelo...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR + \"/final\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"/final\")\n",
    "\n",
    "print(\"Fine-Tuning completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69dc211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/FYG-model-final\\\\tokenizer_config.json',\n",
       " '../models/FYG-model-final\\\\special_tokens_map.json',\n",
       " '../models/FYG-model-final\\\\chat_template.jinja',\n",
       " '../models/FYG-model-final\\\\tokenizer.model',\n",
       " '../models/FYG-model-final\\\\added_tokens.json',\n",
       " '../models/FYG-model-final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MODEL_DIR = \"../models/tinyllama-gaming-1b-lora/final\"\n",
    "FINAL_SAVE_DIR = \"../models/FYG-model-final\"\n",
    "\n",
    "# Cargar el modelo base + adaptador\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(model, MODEL_DIR)\n",
    "\n",
    "# Fusionar los pesos de LoRA al modelo base\n",
    "model = lora_model.merge_and_unload()  # Esto aplica LoRA al modelo base y elimina los pesos de adapter\n",
    "\n",
    "# Guardar el modelo completo listo para Hugging Face\n",
    "model.save_pretrained(FINAL_SAVE_DIR)\n",
    "tokenizer.save_pretrained(FINAL_SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644182a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pruebas modelo fine-tuneado\n",
    "\n",
    "MODEL_DIR = \"models/llama2-gaming-3b-lora/final\"  # ruta a tu modelo guardado\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "example = {\n",
    "    \"User\": \"I'm looking for a good RPG game.\",\n",
    "    \"Assistant\": \"You might enjoy The Witcher 3 or Skyrim.\"\n",
    "}\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant that recommends video games based on the user's tastes.\"},\n",
    "    {\"role\": \"user\", \"content\": example[\"User\"]},\n",
    "    {\"role\": \"assistant\", \"content\": example[\"Assistant\"]},\n",
    "]\n",
    "\n",
    "# Convertir la conversación en un solo prompt\n",
    "prompt = \"\"\n",
    "for turn in conversation:\n",
    "    if turn[\"role\"] == \"system\":\n",
    "        prompt += f\"[SYSTEM]: {turn['content']}\\n\"\n",
    "    elif turn[\"role\"] == \"user\":\n",
    "        prompt += f\"[USER]: {turn['content']}\\n\"\n",
    "    elif turn[\"role\"] == \"assistant\":\n",
    "        prompt += f\"[ASSISTANT]: {turn['content']}\\n\"\n",
    "\n",
    "# Agregar la última pregunta del usuario para generar respuesta\n",
    "prompt += \"[USER]: Can you recommend me an action game?\\n[ASSISTANT]:\"\n",
    "\n",
    "# Generación\n",
    "output = pipe(prompt, max_new_tokens=150)\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
